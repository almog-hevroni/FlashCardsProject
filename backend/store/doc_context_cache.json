{"320849bc3626": {"context": "Page 6\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2191 Fig. 4. Difﬁculty-controllable answer-extraction model using BERT. We can ﬁne-tune the answer-extraction model by using a collection of triplets (r, a, b), which can be obtained from the extended SQuAD dataset explained in Section VI-A. This ﬁne- tuning is performed by minimizing cross-entropy loss between the true positions of the start and end of an answer text and the predicted probabilities for these positions. C. Difﬁculty-Controllable Answer-Aware QG Model We use T5 for difﬁculty-controllable answer-aware QG, where T5 is a pretrained transformer model, the details of which are summarized in Table II. We use T5 because it has been widely used before in various text generation tasks [54], [55], [56], [57] including QG tasks [2], [58], [59] and has achieved higher accuracy in QG compared to models, such as BART and GPT-2 [58]. Conventional answer-aware QG models [60] based on pre- trained language models are implemented by designing the model’s input as r1, . . . , [A], a1, . . . , aK, [A], . . . , rI (3) where [A] is a special token representing an answer’s start and end positions within a reading passage. The model’s target, which is a question text, is designed as [G], q1, . . . , qJ, [E] (4) where [G] and [E] are also special tokens representing the beginning of a question text and the end\n\n\nPage 5\nBERT output vector for the ith word in a passage r, we add two dense layers, denoted as f (s)(vi) and f (e)(vi), for each vi, where each dense layer transforms vi into a scalar value . The dense-layer outputs are then transformed through softmax acti- vations as P (s) = softmax(f (s)(v1), . . . , f (s)(vI)) and P (e) = softmax(f (e)(v1), . . . , f (e)(vI)), whose ith elements P (s) i and P (e) i represent the probability values for whether the ith word is at the start and end positions, respectively. Thus, by extracting the word sequence within the start and end positions, which take the maximum probabilities, we can extract an answer text from a given reading passage. We control the difﬁculty of the answer extraction by inputting a difﬁculty value with the reading passage. Speciﬁcally, the input for our model is deﬁned as b, [SEP], r1, r2, r3, . . . , rI (2) where [SEP] is the special token used to separate the difﬁculty value and the reading passage. This conﬁguration allows the modeltoconsiderthedifﬁcultyvaluewhenextractingtheanswer text from the reading passage. Fig. 4 shows an outline of the answer-extraction model. 3No changes were made to original BERT and T5 models in terms of architecture and training parameters.\n\n\nPage 10\ntable, more than 90% of the questions were rated as either ﬂuent or acceptable, and approximately 90% appropriately reﬂected the content of the reading passage. Moreover, in over 60% of cases, question and answer pairs were generated that were answerable, and nearly 90% were partially appropriate when accepting excessive or insufﬁcient results. These ﬁndings were conﬁrmed by the practicality ratings, which also found that nearly 90% of the questions were evaluated positively. 6The crowdworkers were recruited via the crowdsourcing platform Crowd- Works (crowdworks.jp). The only requirement was that they have a TOEIC score of 900 or above. No additional information about the raters’ attributes was collected. A TOEIC score of 900, which corresponds to an effective operational proﬁciency in the English language, can be compared with other international English scoring systems as follows: TOEFL > 79, International English Language Testing System (IELTS) > 6.5, Cambridge Exam ≥First Certiﬁcate in English (FCE), and Common European Framework of Reference for Languages (CEFR) ≥B2. Next, we calculated the Spearman rank-order correlation co- efﬁcient between the human evaluations of difﬁculty and the difﬁculty levels speciﬁed for QG, obtaining a low correlation value of 0.15. To assess the reason for this outcome, we de- termined the agreement ratio between the two raters for each\n\n\nPage 6\nof a question text, respectively. To implement difﬁculty-control for the answer-aware QG model, we concatenate a target difﬁculty value to the aforemen- tioned conventional input form using b, [Q], r1, . . . , [A], a1, . . . , aK, [A], . . . , rI (5) where [Q] is the special token used to separate the difﬁculty value and the given reading passage. Given this input, the model generates a question text based on a reading passage, an answer, and a target difﬁculty value. Fig. 5 presents an outline of our QG model. We can ﬁne-tune the answer-aware QG model by using a dataset consisting of quadruplets (r, q, a, b), explained in Fig. 5. Difﬁculty-controllable answer-aware QG model using T5. Section VI-A. Speciﬁcally, we prepare the input data following (5) format and output data following (4) format, and train T5 by maximizing the log-likelihood for target data. D. Estimating the Learner’s Ability and Adaptively Generating Questions A previous study on adaptive learning [44] reported that posing questions that learners can answer correctly with a 50% probability is pedagogically effective. As discussed in Sec- tion V-A, in the Rasch model, the probability of obtaining a correct answer is 0.5 when the ability and difﬁculty levels are equal. Therefore, if a learner’s ability level is known, using the proposed method to generate a question whose difﬁculty level is equal to or near the\n\n\nPage 9\n2194 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 Fig. 12. Logistic regression curves for the three different difﬁculties shown in Figs. 9–11. Fig. 13. Average word count of answers by difﬁculty level. that specifying higher difﬁculty values makes the generated questions more difﬁcult across a wide range of ability levels. Next, the average word count of the answers for each difﬁculty level is displayed in Fig. 13. The x-axis represents the speciﬁed levels of difﬁculty, while the y-axis represents the average word count of the answers. The ﬁgure shows an increasing trend in the average word count for higher levels of difﬁculty. Since longer answers generally make the question difﬁcult, the proposed method could extract answers while reﬂecting the difﬁculty levels. Table III shows examples of the generated questions and answers for different difﬁculty levels from the same reading pas- sage. It shows that giving a lower difﬁculty value induces short answer text, whereas giving a higher difﬁculty value induces longer and more complex answers. Finally, we investigated how the input difﬁculty affects the types of generated questions. To do so, we examined the pro- portion of leading interrogative words, “What,” “Who,” “When,” “Why,” “Where,” “Which,” and “How” in the generated ques- tions. The results are shown in Fig. 14. The x-axis represents the speciﬁed levels of\n\n\nPage 7\nlarge dataset without difﬁculty considerations can enhance their performance in answer extraction and QG. 7) Fine-tuning was then performed on D(train) b to develop answer-extraction and QG models that consider question difﬁculty. This ﬁne-tuning employed the model parame- ters estimated in step (6) as initial values. 8) To assess the proﬁciency of the developed models in con- trolling the difﬁculty, we generated question–answer pairs with various difﬁculties and evaluated them. Speciﬁcally, we ﬁrst input each reading passage in D(eval) b and each of the 61 difﬁculty values, from −3.0 to 3.0 in increments of 0.1, into the proposed answer-extraction model and gen- erated 61 answers for each reading passage. Then, given each triplet consisting of a reading passage, difﬁculty value, and generated answer, we generated questions using the proposed QG model. The generated sets of questions and answers were subjected to both machine-based and human evaluations. We now summarize the basic statistics of the datasets D(train) b and D(eval) b , which we developed in the aforementioned pro- cedure (5) to train and evaluate our difﬁculty-controllable QG method. First, the number of reading passages in D(train) b and D(eval) b was 1860and207, respectively. Next, theaveragenumber of questions per reading passage in D(train) b and D(eval) b was 5.21 and 4.28. Furthermore, Fig. 7 is a\n\n\nPage 12\nseparation,” in Proc. AAAI Conf. Artif. Intell., 2019, pp. 6602–6609. [43] X. Sun, J. Liu, Y. Lyu, W. He, Y. Ma, and S. Wang, “Answer-focused and position-aware neural question generation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2018, pp. 3930–3939. [44] M. Ueno and Y. Miyazawa, “IRT-based adaptive hints to scaffold learning inprogramming,”IEEETrans.Learn.Technol.,vol.11,no.4,pp. 415–428, Oct.–Dec. 2018.\n\n\nPage 7\nanswer. 3) Utilizing the collected binary response data, we employed theRaschmodel,expressedin(1),toestimatethedifﬁculty level for each question in D(eval) and the ability for each of the QA systems. 4The rationale behind our use of 60 QA systems is based on a previous report [61], which stated that the minimum requirement of respondents for estimating difﬁculty based on the Rasch model is 30. However, a larger number of respondents generally contributes to improving the stability of difﬁculty estimation. Thus, considering the tradeoff between computational cost and estimation stability, this experiment used 60 QA systems. 5[Online]. Available: https://huggingface.co/ Fig. 7. Number of questions corresponding to the difﬁculty values in D(train) b and D(eval) b . 4) We then integrated the estimated question difﬁculty with D(eval) to produce a new dataset, Db. The real-valued difﬁ- culty estimates were rounded to the second decimal place to facilitate processing by the language models BERT and T5. 5) The created dataset, Db, was further divided into two partitions that were 90% and 10%, denoted as D(train) b and D(eval) b , respectively. 6) We ﬁrst ﬁne-tuned both the answer-extraction model and the QG model using the original SQuAD training dataset, D(train), without consideringthedifﬁcultylevels. Although this step is not mandatory, we expect that pretraining these models on a\n\n\nPage 10\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2195 TABLE IV HUMAN EVALUATION 900 or higher.6 The raters were asked to assess the assigned question–answer pairs based on the following evaluation crite- ria. The raters were crowdworkers having a TOEIC score of 900 or higher . 1) Fluency: Evaluate the grammatical correctness and ﬂu- ency of the questions. Ratings were done on a three-point scale: 3. appropriate, 2. acceptable, and 1. inappropriate. 2) Content Relevance: Assess whether the generated ques- tions were relevant to the reading passage. Ratings were done on a two-point scale: 2. appropriate and 1. inappro- priate. 3) Answerability: Evaluate whether the extracted answer was indeed the correct answer for the generated question. Ratings were done on a nominal scale with four categories: a. appropriate, b. insufﬁcient, c. excessive, and d. inappro- priate. 4) Practicality: Assess whether the question and/or answer could become feasible with slight modiﬁcations. Ratings were done on a three-point scale: 3. already feasible, 2. need minor correction, and 1. not feasible. 5) Question Difﬁculty: Evaluate the difﬁculty level of the generated question. Ratings were done on a ﬁve-point scale, ranging from 1 (easiest) to 5 (most difﬁcult). Results for ﬂuency, content relevance, answerability, and practicality are presented in Table IV. From this\n\n\nPage 10\nevaluation criterion. The results showed that the agreement rates for ﬂuency, content relevance, answerability, and practicality were 0.63, 0.85, 0.53, and 0.60, respectively, while that for difﬁcultywas 0.22. Fromtheseresults, weconcludethat it is hard to ensure reliable subjective evaluations for question difﬁculty, while the evaluations for other criteria seem acceptable. Thus, to achieve a more reliable evaluation of question difﬁ- culty, we conducted anot", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Adaptive Question-Answer Generation (QG):** The document discusses a methodology for generating questions and answers that adapt to different difficulty levels using models like BERT and T5.\n2. **Difficulty Control:** The focus is on controlling the difficulty of the generated questions and answers based on learner ability and specified difficulty values.\n3. **Model Architecture:** The architecture of the answer-extraction model and the QG model is outlined, detailing how inputs are structured to incorporate difficulty levels.\n\n**Goals:**\n1. **Fine-tuning Models:** The aim is to fine-tune the answer-extraction and QG models using datasets that include difficulty levels, enhancing their performance in generating appropriate questions and answers.\n2. **Evaluating Effectiveness:** The document aims to evaluate the effectiveness of the generated question-answer pairs in terms of fluency, content relevance, answerability, practicality, and difficulty.\n3. **Adaptive Learning:** The goal is to develop a system that can adaptively generate questions that match the learner's ability, thereby optimizing the learning experience.\n\n**Insights:**\n1. **Model Performance:** The T5 model is noted for its superior performance in Q", "updated_at": 1765699328.369592}, "ca12a935c93b": {"context": "Page 1\nTABLE IV Human Evaluation 900 or higher.6 The raters were asked to assess the assigned question–answer pairs based on the following evaluation crite- ria. The raters were crowdworkers having a TOEIC score of 900 or higher . Fluency: Evaluate the grammatical correctness and flu- ency of the questions. Ratings were done on a three-point scale: 3. appropriate, 2. acceptable, and 1. inappropriate. Content Relevance: Assess whether the generated ques- tions were relevant to the reading passage. Ratings were done on a two-point scale: 2. appropriate and 1. inappro- priate. Answerability: Evaluate whether the extracted answer was indeed the correct answer for the generated question. Ratings were done on a nominal scale with four categories: a. appropriate, b. insufficient, c. excessive, and d. inappro- priate. Practicality: Assess whether the question and/or answer could become feasible with slight modifications. Ratings were done on a three-point scale: 3. already feasible, 2. need minor correction, and 1. not feasible. Question Difficulty: Evaluate the difficulty level of the generated question. Ratings were done on a five-point scale, ranging from 1 (easiest) to 5 (most difficult).\n\n\nPage 1\nThe obtained answers were graded by one of the authors to remove the effects of superficial fluctuation in human answers. Based on the correct and incorrect response data, we calculated the correct rates for each question as an index representing its difficulty. The correlation between correct rates and the specified diffi- culty levels was −0.67, where a test for noncorrelation revealed that the p-value was less than .01, confirming a statistically significant correlation at the 1% level. These results suggest that the proposed method can generate questions in alignment with the difficulty levels perceived by human respondents. Evaluation of the Accuracy of Learners’ Ability Estimation In this section, we evaluate the efficacy of the method of adaptive QG proposed in Section V-B. We simulated the adap- tive QG process for three respondents with different abili- ties. For the respondents, we used three QA models with the minimum and maximum θ, specifically, −3.658 and 2.766, as well as that which was closest to the average, θ = 1.244. Using the three QA models, we examined the following two QG processes and analyzed the trajectory of their estimated abilities. Adaptive Generation (Proposed): Initially, ten questions with random difficulty are generated and administered. Then, 40 questions are administered, following the adap- tive procedures detailed in Section VII-B, while\n\n\nPage 1\nEarly QG methods were based on rule-based or template- based approaches, which generated questions by converting declarative texts into interrogative questions, using handcrafted rules or templates [1], [8], [9], [10], [11]. However, preparing well-designed rules and templates for specific applications is time consuming and labor-intensive [1], [12]. To address this limitation, end-to-end QG methods based on deep neural net- works have been used [1], [13], [14], [15], [16], [17], [18], [19], [20]. Initial neural QG methods were designed as sequence- to-sequence (seq2seq) models based on recurrent neural net- works (RNNs) and attention mechanisms [14], [21], whereas more recent methods have employed pretrained transformer models [2], [5], [13], [15], [22], [23], such as bidirectional encoder representations from transformers (BERT) [24], gen- erative pretrained transformer 2 (GPT-2) [25], bidirectional and autoregressive transformers (BART) [26], and text-to-text trans- fer transformer (T5) [27]. These approaches have successfully generated fluent questions that are relevant to the given reading passages.\n\n\nPage 1\nreal data, we confirmed that the proposed method can generate question–answer pairs with difficulty levels that align with the learners’ abilities while efficiently estimating their abilities.\nIndex Terms—Adaptive learning, adaptive testing, automated question generation (QG) for reading comprehension, deep neural networks, item response theory (IRT), natural language processing. Introduction\nUTOMATIC question generation (QG) for reading com- prehension is the task of creating questions related to given reading passages without human intervention, where read- ing passages are arbitrary natural language texts comprising a sequence of words, a sentence, a paragraph, or an entire document. The field of natural language processing has pro- duced numerous methods for QG [1], which are being utilized\n\n\nPage 1\nA notable application of QG in education is reading tutors [1], [3], [4], [5], which provide reading-comprehension questions for diverse reading materials. Offering questions helps direct learn- ers’ attention to the content and helps them identify misconcep- tions, thereby improving their reading-comprehension skills [3]. To enhance the efficiency of such learning, offering questions with difficulty levels tailored to each learner’s reading ability is beneficial. For this reason, several difficulty-controllable QG methods have recently been proposed.\nDifficulty-controllable QG for reading comprehension is a rel- atively new area of research, and thus, the literature is scant [22], [28]. One method for realizing difficulty-controllable QG is to use an RNN-based seq2seq model in which the hidden states from the encoder are adjusted to accept the difficulty levels categorized as either easy or hard [22]. Another approach is a multihop method [28], which defines the question difficulty according to the number of inference steps required to answer the question and generates questions by controlling the number of these inference steps. However, both methods face the following limitations that make it difficult to generate questions suitable for the learner’s abilities.\n\n\nPage 1\n© 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Fig. 1.  Conventional QG task: answer-aware QG. The relationship between the difficulty of the questions and the learner’s ability is ignored, making it difficult to determine the appropriate difficulty for each learner.\nThe methods are answer-aware, which means a reading passage and an answer text must be input to generate questions, as shown in Fig. 1. For this reason, they can- not generate question–answer pairs. Generating answers along with questions is essential to realize automatic assessment of learners’ responses to generated questions. Furthermore, controlling the difficulty of the generated answers is also crucial because both questions and answers generally affect the overall difficulty.\n\n\nPage 1\nUsing this procedure, we can effectively estimate a learner’s ability while administering questions at appropriate levels of difficulty. Remember that when the ability and difficulty levels are equal, the Rasch model produces a 0.5 probability that a correct answer will be obtained, and Fischer information is maximized.\nFig. 7.  Number of questions corresponding to the difficulty values in D(train) Fig. 6.  Flow of the experimental procedures. Experiments In this section, we demonstrate that our proposed method can generate questions and answers corresponding to the target IRT- based difficulty values. Experimental Procedures We conducted the following experiment using the original SQuAD data, which was originally split into D(train) and D(eval), where the sample size of D(train) was 87 599 and that of D(eval) was 10 570. Fig. 6 illustrates the flow of the following experi- mental procedures. Using the original SQuAD training data D(train), we con- structed 60 different QA systems.4\nSpecifically, 12 QA systems (BERT-base, BERT- large, RoBERTa-base, RoBERTa-large, DeBERTa-base, DeBERTa-large, DeBERTa-v3-base, DeBERTa-v3-large, ALBERT-base-v1, ALBERT-base-v2, ALBERT-large-v2, and DistilBERT-base), which are available on hugging- face,5 were trained with 600, 1200, 1800, 2400, and 3000\n\n\nPage 1\nA representative neural QG method for reading comprehen- sion is an RNN-based seq2seq model [14]. In this method, a reading passage and an answer are fed into an RNN encoder, and the output feature vector is given to an RNN decoder to generate a question text. In addition, Zhou et al. [32] proposed using an RNN-based QG model that can consider both the words’ sequence and their part-of-speech (POS) tags.\nIn recent years, pretrained transformer-based models, which have outperformed RNN-based seq2seq models on many natural language processing tasks (e.g., [25], [27], [33], [34], [35], [36], [37], [38], [39], and [40]), have been used for automated QG tasks (e.g., [1], [13], [14], [41], [42], and [43]). Some examples include a QG method proposed by Chan and Fan [13] that uses BERT and a method proposed by Lee and Lee [23] that uses T5. Difficulty-Controllable Neural QG for Reading Comprehension When utilizing QG methods as a learning aid to foster reading comprehension skills, it is critical to be able to generate ques- tions with arbitrary difficulty levels [44]. Accordingly, several recent studies have proposed difficulty-controllable QG meth- ods [22], [28].\n\n\nPage 1\nAdaptive Question–Answer Generation With Difficulty Control Using Item Response Theory and Pretrained Transformer Models Yuto Tomikawa  , Ayaka Suzuki, and Masaki Uto\n\n\nPage 1\nThe parameters of the IRT model are typically estimated in two stages: item calibration and ability estimation. Item calibra- tion involves estimating the item parameters from the response data. Specifically, marginal maximum likelihood estimation is commonly employed for item calibration [50]. Once the item parameters have been calibrated, the ability estimation phase estimates the examinee’s ability θ, commonly based on the expected a posteriori estimation [51], [52].\nComputerized Adaptive Testing (CAT) exp(θm − bn)  pnm = 1 + exp(θ − b ) (1) As explained earlier, the IRT is often used as a basis for CAT, which adaptively administers items appropriate for each exam- where bn denotes the difficulty of the nth item, and θm denotes the latent ability of the mth examinee. For a detailed explanation of this model, Fig. 3 illustrates item response curves (IRCs) for the Rasch model, which are plots of 1In this context, “latent” indicates that the corresponding factors are not directly observable but lie behind the observed data. 2“Discriminative power” means the degree to which an item can distinguish between examinees with different levels of the underlying ability.\n\n\nPage 1\nWe collected ten subjects, different from those in the previous experiment, and asked them to answer 20 of those 30 questions. The set of questions administered to each subject was different, and each question was administered an equal number of times. Given that the experiment required participants with diverse language abilities, this time we recruited crowdworkers possessing English skills equivalent to a TOEIC score of 600 or higher.\n\n\nPage 1\nAs discussed in the aforementioned sections, our first research objective is to develop a method that generates reading compre- hension questions along with their corresponding answers, based on the given reading passages and specified difficulty levels, as illustrated in Fig. 2. Our second objective is to develop an adaptive QG framework that generates questions with difficulty levels appropriate for learners while efficiently estimating their abilities each time a question is presented.\nThe detailed task definition for the first objective is as follows. Let a given reading passage be a word sequence r = {ri | i ∈ {1,...,I}}, where ri represents the ith word in the passage, and I is the passage text length. Similarly, let a question text q and an answer text a be word sequences q = {qj | j ∈ {1,...,J}} and a = {ak | k ∈ {1,...,K}}, respectively, where qj is the jth word in the question text, ak is the kth word in the answer\ntext,", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Automated Question Generation (QG):** Focuses on creating questions related to reading passages using natural language processing (NLP) techniques.\n2. **Evaluation Criteria for QG:** Discusses various criteria for assessing the quality of generated questions, including fluency, content relevance, answerability, practicality, and question difficulty.\n3. **Adaptive QG Methods:** Explores methods that adjust question difficulty based on learners' abilities, utilizing Item Response Theory (IRT) and deep learning models.\n4. **Neural QG Techniques:** Highlights the transition from rule-based methods to advanced neural network approaches, particularly using transformer models like BERT and T5.\n\n**Goals:**\n1. **Develop Effective QG Frameworks:** Create systems that generate reading comprehension questions and corresponding answers tailored to different difficulty levels.\n2. **Adaptive Learning:** Implement adaptive QG processes that can dynamically adjust the difficulty of questions based on the learner's estimated abilities.\n3. **Enhance Reading Comprehension:** Utilize QG to improve learners' engagement and understanding of reading materials by providing appropriately challenging questions.\n\n**Insights:**\n1. **Statistical Correlation:** The study", "updated_at": 1765699334.835139}, "3af775a53ec1": {"context": "Page 3\nFigure 2: Book2Dial Framework for Generating Dialogues from Textbooks: Our approach uses two models – a Student model and a Teacher model. The Student model plays the role of a student, formulating questions from a limited context (document formatting). In contrast, the Teacher model assumes the role of a teacher, providing answers and guidance by referencing the (sub-)section in the textbook. This framework can be adapted to various instantiations of the two roles with varying formatting information, such as multi-turn QA-QG models (Kim et al., 2022), Dialogue Inpainting (Dai et al., 2022), and a prompting approach utilizing role-playing LLMs. grounded dialogue datasets. 2.2 Educational Dialogue Datasets The development of educational chatbots is highly reliant on high-quality data. Yet such data is hard to obtain. Therefore, previous works such as Math- Dial (Macina et al., 2023a) collect conversational data by pairing real teachers with an LLM that simulates students. Other datasets are commonly created by roleplaying both teacher and student, such as CIMA (Stasaski et al., 2020) or by tran- scribing classroom interactions (Suresh et al., 2022; Demszky and Hill, 2023) or logging online conver- sations (Caines et al., 2020). However, all of these methods are challenging to scale, and using non- experts often leads to data quality issues (Macina et al., 2023a). Thus, in this\n\n\nPage 12\nGangwoo Kim, Sungdong Kim, Kang Min Yoo, and Jaewoo Kang. 2022. Generating information-seeking conversations from unlabeled documents. In Pro- ceedings of the 2022 Conference on Empirical Meth- ods in Natural Language Processing, pages 2362– 2378. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2023. SODA: Million-scale dialogue dis- tillation with social commonsense contextualization. In Proceedings of the 2023 Conference on Empiri- cal Methods in Natural Language Processing, pages 12930–12949, Singapore. Association for Computa- tional Linguistics. Mohammad Amin Kuhail, Nazik Alturki, Salwa Alram- lawi, and Kholood Alhejori. 2023. Interacting with educational chatbots: A systematic review. Educa- tion and Information Technologies, 28(1):973–1018. Ghader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and Salam Al-Emari. 2020. A systematic review of auto- matic question generation for educational purposes. International Journal of Artificial Intelligence in Ed- ucation, 30:121–204. Mark R. Lepper and Maria Woolverton. 2002. Chapter 7 - the wisdom of practice: Lessons learned from the study of highly effective tutors. In Joshua Aron- son, editor, Improving Academic Achievement, Educa- tional Psychology, pages 135–158. Academic Press, San Diego. I Loshchilov and F Hutter.\n\n\nPage 23\nimages such as paintings and photographs. Student: How do monocular cues like linear perspec- tive, interposition, and relative size and closeness of images contribute to our perception of depth in the con- text of the section “5-3-vision”? Teacher: Linear perspective and interposition, as binoc- ular cues, contribute to our perception of depth by pro- viding additional visual information through the use of both eyes. Linear perspective refers to the perception of depth when we see two parallel lines appearing to con- verge, while interposition refers to the partial overlap of objects. These cues, along with binocular disparity and other monocular cues, help us establish a sense of depth and understand the spatial relationships between objects in our three-dimensional world and in 2-D images such as paintings and photographs. Table 17: Example of Dialogue with Insufficient Follow-up Ability: This table illustrates a dialogue sequence where the first QA pair addresses monocular cues, but the following pair shifts focus to binocular cues without maintaining topical continuity. The dialogue then reverts to monocular cues in the third pair, demonstrating an overall lack of follow-up ability in the dialogue. For each teacher’s response, the student also does not give any feedback and just jumps to the next question, which also does not align with natural conversations. 9729\n\n\nPage 5\nreconstruction task treats a conversation as a sequence of utterances {ui}2T i=1, To train it, a ran- domly chosen utterance ui is masked to create a 9711\n\n\nPage 5\nWhile non-answerable questions could also rep- resent the curiosity of a student (Scialom and Sta- iano, 2020), the answerability of questions given the context is generally important for a more useful dialog. We use the QA model1 to judge whether each question is answerable given the textbook con- tent. We refer to this metric as Answerability. This approach is akin to the method employed in (Kim et al., 2022). More details are in Table 5. 3.2.6 Factual Consistency of the Answer Factual Consistency measures whether the answer correctly responds to the student’s question. This criterion is crucial in education because it is impor- tant for students to learn accurate information (Met- zger et al., 2003). Existing metrics like Q2 (Hon- ovich et al., 2021) use a QA model to assess an- swer correctness, while RQUGE (Mohammadshahi et al., 2023) uses a QA model to evaluate the quality of the candidate question. In our scenario, we need to measure whether the answer contains correct information and accurately answers the question. Therefore, we build on the idea of Q2 and introduce a new metric referred as QFactScore: α · sim(QA(qt, S), at) + β · sim(qt, at) (1) It calculates the cosine similarity of embeddings between the predicted and original answers for each QA pair and also evaluates the similarity between the question and the original answer. This metric has been validated for\n\n\nPage 20\nlearning rate of 6.25e- 5 and employ linear learning rate decay without warmup. For model optimization, we utilize check- points from the transformers library (Wolf et al., 2020). The negative log-likelihood of the ground- truth response is minimized using the AdamW opti- mizer, as detailed in (Loshchilov and Hutter, 2019). We assess model performance using the sacrebleu implementation of the BLEU metric, following (Post, 2018). We use one V100 GPU to train the model; the FLAN-T5-LARGE model has 0.8 Bil- lion parameters and takes 7 hours to train. The result in Table 2 is the average of 3 runs. G.1 Examples of Results We demonstrate an example of a comparison be- tween the ground truth, the prediction from the model without fine-tuning, and the prediction from the model with fine-tuning in Table 15. These dia- logues are based on the MCTest split of the CoQA dataset and the predictions are generated based on the same context information as shown in Table 14. From Table 15, we find that in the third answer, the prediction example without fine-tuning is overly verbose, suggesting a poor grasp of the question, ’What was the first thing she did that morning?’ which focuses on the person’s initial action. In con- trast, the model without fine-tuning simply repli- cates the entire context sentence. On the other hand, the prediction with fine-tuning accurately mirrors the ground\n\n\nPage 2\nStudent: What is the color of Mars? Teacher: Mars has moons. Formatting (C) Subsection Title: Planet Key Concepts: Sun, Earth, Mars Learning Objectives: Learn about Planets Summary: The Sun is the center of the solar system, Earth is ... Answer Relevance Informativeness Coherence Factual Consistency Textbook source text (S) The Sun as the center of the solar system. Earth, the third planet from the Sun, with one moon. Mars, known for its red color, having two moons, Phobos and Deimos. Student: How many moons does it have? Teacher: I don't know how many moons Mars has. Student: What is interesting about this passage? Teacher: Sun is the center of solar system. Student: How many moons does Earth have? Teacher: Earth has moons, it has two moons. NA Specificity Answerability Groundedness Student: Mars is red. Teacher: Mars is red. Figure 1: Example of a synthetic teacher-student interaction based on a textbook, along with various criteria for evaluating the quality of the interaction. The criteria include Answer Relevance of the answer to the question, Coherence of the question-answer interaction with respect to the dialogue history, Informativeness of the overall interaction, Groundedness to the textbook, Answerability of the question from the textbook, Factual Consistency of the answer with respect to the question, and Specificity of the question. More details in Section 3.2.\n\n\nPage 6\nSection 5.4. 5.1 Automatic Evaluation In this section, we discuss statistics and metrics for the generated datasets. We present aggregated statistics of the datasets across four textbook do- mains in Tables 1 and 2, also noting comparisons with existing datasets: MathDial (Macina et al., 2023a) – math tutoring conversations between hu- man teachers and student LLMs, fact-based human- human conversations of QuAC (Choi et al., 2018), and NCTE transcripts of math classrooms (Dem- szky and Hill, 2023). Domain-specific results in the dataset are detailed in Tables 10 and 11. To adjust for varying dialogue lengths, we limit the number of turns to T = 12 for each model, as in (Kim et al., 2022). 5.1.1 Statistical Analysis In dialogue, different types of questions empha- size various aspects. We hypothesize that “what” and “which” questions focus on factual information. In contrast, other question types, such as “why” and “how,” tend to reflect more complex inquiries, which are also important in educational contexts. In Table 1, we present the percentages of student questions including words what, which, why, and how4. Furthermore, the average token count for questions and answers across each dataset is also shown. The key findings are as follows: 3https://openstax.org/ 4This ratio excludes ’how much’ and ’how many’ questions because they pertain to factual information. 9712\n\n\nPage 23\nStudent: What are monocular cues and how do they contribute to our perception of depth in the context of the section “5-3-vision”? Teacher: Monocular cues are visual cues that only re- quire one eye and contribute to our perception of depth. In the context of the section “5-3-vision monocular cues such as linear perspective, interposition, and relative size and closeness of images help us perceive depth in 2-D images, such as paintings and photographs. These cues allow us to understand the spatial relationships between objects in a scene and create a sense of depth, despite the visual stimulus being 2-D. Student: How do binocular cues like linear perspective and interposition contribute to our perception of depth in the context of the section “5-3-vision”? Teacher: Binocular cues like linear perspective and in- terposition contribute to our perception of depth in the context of the section “5-3-vision” by providing addi- tional visual information to our brain through the use of both eyes. Linear perspective refers to the perception of depth when we see two parallel lines appearing to con- verge, while interposition refers to the partial overlap of objects. These binocular cues, along with other cues like binocular disparity, help us establish a sense of depth and understand the spatial relationships between objects in a scene, both in our three-dimensional world and in 2-D\n\n\nPage 24\nthe first law of thermodynamics rather than the zeroth? The explanation is that this law was discovered af- ter the first and second laws of thermodynam- ics but is so fundamental that scientists decided it should logically come first. As an example of the zeroth law in action, consider newborn babies in neonatal intensive-care units in hos- pitals. Prematurely born or sick newborns are placed in special incubators. These babies have very little covering while in the incuba- tors, so to an observer, they look as though they may not be warm enough. However, in- side the incubator, t", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Book2Dial Framework**: \n   - Utilizes a Student model for question formulation and a Teacher model for providing answers based on textbook content.\n   - Adaptable to various educational dialogue formats.\n\n2. **Educational Dialogue Datasets**:\n   - Importance of high-quality data for developing educational chatbots.\n   - Challenges in data collection methods, including teacher-student pairing and classroom transcriptions.\n\n3. **Dialogue Quality Metrics**:\n   - Evaluation criteria for dialogues, including Answer Relevance, Coherence, Informativeness, Groundedness, Answerability, Factual Consistency, and Specificity.\n\n4. **Automatic Evaluation**:\n   - Statistical analysis of generated datasets across different educational domains.\n   - Examination of question types and their implications for educational dialogues.\n\n#### Goals:\n- To create a robust framework for generating educational dialogues that enhance learning through interactive question-answer formats.\n- To address the challenges of data quality and scalability in creating educational dialogue datasets.\n- To establish metrics for evaluating the effectiveness and accuracy of educational dialogues.\n\n#### Insights:\n- The Student and Teacher models can be tailored for various educational contexts, improving the adaptability of chatbots.\n- High-quality", "updated_at": 1765699340.884916}, "6571396b8547": {"context": "Page 4\nbut lie behind the observed data. 2“Discriminative power” means the degree to which an item can distinguish between examinees with different levels of the underlying ability. Fig. 3. IRCs for a Rasch model with different item difﬁculty values. the probability p. In this ﬁgure, the horizontal axis represents the ability θ; the vertical axis represents the probability p; and the three solid curves represent the IRCs for three items with different difﬁculty levels, b = −1.0, 0.0, and 1.0. These IRCs demonstrate that examinees with higher θ have an increased probability of responding correctly to each item, indicating that it reﬂects a reasonable relationship between the examinee’s ability and the probability of a correct response. In addition, it is evident that the IRC shifts to the right as the bvalue increases, indicating that higher ability θ is necessary to correctly answer items with higher b values. This means that the parameter b reﬂects the difﬁculty of each item. Moreover, this model enables the calculation of the correct answer probability from the ability and difﬁculty values, which helps to determine a question with an appropriate difﬁculty level for an examinee with a given ability. For example, when we aim to select a question that an examinee with ability θwill answer correctly with 0.5 probability, offering a question with b ≒ θ is appropriate because the\n\n\nPage 4\nRESPONSE THEORY (IRT) The IRT [29] is a statistical framework that uses probabilistic models, called IRT models, to estimate two latent factors:1 exam- inee’s ability and item characteristics such as item difﬁculty and discriminative power,2 where the examinee and the item corre- spond to the learner and the question, respectively, in our study. These latent factors are estimated from the response data, which generally consist of the examinee’s binary correct/incorrect re- sponses to the items. The IRT has been widely used in various educational and psychological tests because of its numerous ben- eﬁts, such as offering accurate estimates of examinee’s ability and item characteristics, unifying measurement scales among different tests, and facilitating CAT applications [45], [46], [47], [48]. This study uses the Rasch model [49], the most traditional and well-known IRT model, to quantify question difﬁculty. A. Rasch Model The Rasch model deﬁnes the probability that the mth exami- nee correctly answers the nth item as pnm = exp(θm −bn) 1 + exp(θm −bn) (1) where bn denotes the difﬁculty of the nth item, and θm denotes the latent ability of the mth examinee. For a detailed explanation of this model, Fig. 3 illustrates item response curves (IRCs) for the Rasch model, which are plots of 1In this context, “latent” indicates that the corresponding factors are not directly observable\n\n\nPage 9\n2194 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 Fig. 12. Logistic regression curves for the three different difﬁculties shown in Figs. 9–11. Fig. 13. Average word count of answers by difﬁculty level. that specifying higher difﬁculty values makes the generated questions more difﬁcult across a wide range of ability levels. Next, the average word count of the answers for each difﬁculty level is displayed in Fig. 13.T h e x-axis represents the speciﬁed levels of difﬁculty, while the y-axis represents the average word count of the answers. The ﬁgure shows an increasing trend in the average word count for higher levels of difﬁculty. Since longer answers generally make the question difﬁcult, the proposed method could extract answers while reﬂecting the difﬁculty levels. Table III shows examples of the generated questions and answers for different difﬁculty levels from the same reading pas- sage. It shows that giving a lower difﬁculty value induces short answer text, whereas giving a higher difﬁculty value induces longer and more complex answers. Finally, we investigated how the input difﬁculty affects the types of generated questions. To do so, we examined the pro- portion of leading interrogative words, “What,” “Who,” “When,” “Why,” “Where,” “Which,” and “How” in the generated ques- tions. The results are shown in Fig. 14.T h e x-axis represents the speciﬁed levels of\n\n\nPage 13\nNatural Lang. Process., 2021, pp. 692–701. [61] J. M. Linacre, “Sample size and item calibration stability,” Rasch Meas. Trans., vol. 7, 1994, Art. no. 328. Yuto Tomikawareceived the B.S. degree in engineer- ing from the University of Electro-Communications, Chofu, Japan, in 2023, where he is currently working toward the M.S. degree in engineering. His research interests include educational and psy- chological measurement, machine learning, and nat- ural language processing. Ayaka Suzuki received the B.S. degree in engineer- ing from the University of Electro-Communications, Chofu, Japan, in 2022, where she is currently working toward the M.S. degree in engineering. Her research interests include educational and psy- chological measurement, machine learning, and nat- ural language processing. Masaki Uto received the Ph.D. degree in engineer- ing from the University of Electro-Communications (UEC), Chofu, Japan, in 2013. He has been an Associate Professor with the Grad- uate School of Informatics and Engineering, UEC, since 2020. His research interests include educational and psychological measurement, Bayesian statistics, machine learning, and natural language processing. Dr. Uto was the recipient of the Best Paper Runner- Up Award at the 2020 International Conference on Artiﬁcial Intelligence in Education.\n\n\nPage 11\n2196 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 Fig. 15. Trajectory in ability estimates for a QA system with ability −3.658 (Minimum θ). Fig. 16. Trajectory in ability estimates for a QA system with ability 1.244 (average θ). Fig. 17. Trajectory in ability estimates for a QA system with ability 2.766 (Maximum θ). 2) Random Generation: In total, 50 questions with random difﬁculty are generated and administered. The trajectories in estimated θ values obtained using these methods are presented in Figs. 15–17. The horizontal axis rep- resents the number of questions administered, and the vertical axis denotes the value of θ. The blue line signiﬁes the true value, whereas the orange and green lines represent the trajectory in estimated abilities due to adaptive and random question posing, respectively. Note that these graphs are constructed by averaging the results from 500 iterations of the experiment. The results show that the proposed adaptive strategy estimated the respon- dents’ true ability more efﬁciently than the random strategy. For further analysis, we conducted a performance evalua- tion targeting all the QA systems. Speciﬁcally, we ran the the aforementioned experiment 500 times for each of the 60 QA models and calculated the mean absolute error (MAE) between the true ability values and the estimated abilities after posing 50 questions, as given by MAE =\n\n\nPage 3\ndifﬁculty-controllable QG task based on a standard benchmark dataset, such as SQuAD. In the method, each question in the dataset is answered by two question–answering (QA) systems. Then, the binary difﬁculty label is estimated for each question. Speciﬁcally, the label “easy” is assigned to a question when both QA systems correctly answer it, while the label “hard” is assigned when both incorrectly answer it. Questions for which only one QA system produces a correct answer are excluded from the dataset. An RNN-based seq2seq QG model is then trained using this training dataset, which comprises a reading passage, a question text, a corre- sponding answer, and a difﬁculty label. The model is designed to take the difﬁculty label as part of the input, enabling the generation of questions tailored to speciﬁc difﬁculty levels. Another difﬁculty-controllable QG method, proposed by Cheng et al. [28], uses the number of inferential steps required\n\n\nPage 8\nanalysis is given by Fig. 9, which shows the correct answer ratio of the 60 QA systems for the generated question–answer pairs given b =1 .0.T h e x-axis represents the ability θ,t h ey-axis represents the correct answer ratio, each plot indicates an individual QA system, and the orange line repre- sents the logistic regression curve ﬁtted to the 60 data points by the least-squares method. As described before, the Rasch model gives a probability of 0.5 that a correct answer will be given when θ = b. To emphasize this point, the green dashed line represents θ =1 .0 and a correct answer ratio of 0.5. The ﬁgure shows that the QA systems with an ability level around θ =1 .0 demonstrate a correct answer ratio of approximately 0.5, suggesting that the questions are generated as expected. A similar trend was observed for other difﬁculty levels, as exempliﬁed in Figs. 10 Fig. 9. Average correct answer ratio of the QA systems for generated questions given difﬁculty b =1 .0. Each dot plot represents an individual QA system. Fig. 10. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b =2 .0. Each dot plot represents an individual QA system. Fig. 11. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b =0 .0. Each dot plot represents an individual QA system. and 11, which show the results for b =2 .0 and b =0 .0,\n\n\nPage 4\nprobability p becomes 0.5 when b = θ, as shown in the ﬁgure. The parameters of the IRT model are typically estimated in two stages: item calibrationand ability estimation. Item calibra- tion involves estimating the item parameters from the response data. Speciﬁcally, marginal maximum likelihood estimation is commonly employed for item calibration [50]. Once the item parameters have been calibrated, the ability estimation phase estimates the examinee’s ability θ, commonly based on the expected a posteriori estimation [51], [52]. B. Computerized Adaptive Testing (CAT) As explained earlier, the IRT is often used as a basis for CAT, which adaptively administers items appropriate for each exam- inee while sequentially estimating the examinee’s ability from their response history. Speciﬁcally, CAT based on the Rasch model begins by initializing the ability of a target examinee, then it generally selects an item with a difﬁculty level that maximizes Fisher information [31]. The ability estimate is then updated with a correct or incorrect response to the offered item. By repeating these procedures, CAT offers optimal items while efﬁciently estimating an examinee’s ability.\n\n\nPage 2\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2187 Fig. 1. Conventional QG task: answer-aware QG. 1) The relationship between the difﬁculty of the questions and the learner’s ability is ignored, making it difﬁcult to determine the appropriate difﬁculty for each learner. 2) The methods are answer-aware, which means a reading passage and an answer text must be input to generate questions, as shown in Fig. 1. For this reason, they can- not generate question–answer pairs. Generating answers along with questions is essential to realize automatic assessment of learners’ responses to generated questions. Furthermore, controlling the difﬁculty of the generated answers is also crucial because both questions and answers generally affect the overall difﬁculty. To address these issues, we introduce a novel method for gen- erating question–answer pairs while considering the difﬁculty associated with the learners’ ability. A unique feature of our method is the use of item response theory (IRT) [29] to quantify the question difﬁculty. The IRT is based on statistical models that deﬁne the relationship between question difﬁculty and learner ability, thereby facilitating the selection of an appropriate difﬁculty level for each learner. For this reason, our method is designed to generate question–answer pairs while controlling IRT-based difﬁculty. For our QG\n\n\nPage 12\nlanguage understand- ing,” in Proc. Neural Inf. Process. Syst.", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Item Response Theory (IRT)**: A statistical framework used to estimate examinee ability and item characteristics, particularly item difficulty and discriminative power.\n2. **Rasch Model**: A specific IRT model that quantifies question difficulty and defines the probability of a correct response based on examinee ability and item difficulty.\n3. **Computerized Adaptive Testing (CAT)**: An application of IRT that adapts question difficulty based on the examinee's estimated ability, optimizing the assessment process.\n\n#### Goals:\n1. **To Develop a Difficulty-Controlled Question Generation (QG) Method**: This method aims to generate question-answer pairs that align with the learner's ability level, enhancing the assessment's relevance and effectiveness.\n2. **To Analyze the Relationship Between Question Difficulty and Learner Ability**: Understanding how different difficulty levels affect response probabilities and learner performance.\n3. **To Improve Adaptive Learning Systems**: By utilizing IRT, the study seeks to create a more efficient and personalized learning experience through tailored question difficulty.\n\n#### Insights:\n1. **Discriminative Power**: The ability of items to differentiate between examinees of varying abilities is crucial for effective", "updated_at": 1765795236.1866431}, "2d898d7d8164": {"context": "Page 4\ngames [20], which could support learning eﬃciency and help complement learning diﬃculties as well. 3. Research The authors of this paper conducted a study among all three student cohorts of bachelor Informatics students at Subotica Tech—College of Applied Sciences. The research included 182 student participants who were taking an introductory course called ‘Algorithms and data structures’. The course is composed of lectures, practices, and laboratory practices. The course runs in the spring semester over 15 weeks in the ﬁrst year of study. It consists of one 90 min lecture per week as well as a 45 min practice and another 90 min lab practice. The aim of the course is to introduce the students to the basic concepts of algorithms and data structures in C/C++ programming language. The course covers the basic principles of programming, variables, control statements ( if, while, do-while, breakand continue statements), arrays, functions, pointers, and some basic algorithms (sort and search). Based on historical data, the grade average in this given course is 7.02 (5 is the minimum grade and means fail, and the maximum grade is 10), while the pass rate is about 50%. During their studies, informatics students learn new developing methods and developing environments as well as new programming languages. The novel techniques help them solve more complex information and communications\n\n\nPage 10\ndata  for the whole scale, and this result partially supports the H2 hypothesis. Table 7. The top five highest score questions. Year 1 Year 2 Year 3 One-Way ANOVA Avg. Sd. Avg. Sd. Avg. Sd. F p Q3 1.31 0.5 1.36 0.4 1.48 0.5 1.52 0.02 Q30 1.12 0.3 1.17 0.3 1.30 0.4 3.18 0.04 Q37 1.22 0.4 1.24 0.4 1.00 0.0 2.92 0.05 Q39 1.12 0.3 1.33 0.4 1.20 0.4 4.49 0.01 Q42 1.00 0.00 1.10 0.2 1.24 0.6 6.1 0.003 The five worst-performing questions were Questions 6, 13, 14, 20, and 25 on the average of the responses for students from all three years of study. For these questions, most of the estimations fell into the medium or hard category (Figure 3). Base d  o n  t h e  r e s u l t s ,  a  r e v i e w  o f  q u e s t i o n  Q 6  i s definitely recommended, as the majority of students rated it as an easy question; however, they received one of the lowest scores (the lowest rate of correct answers) for this question. Figure 2. Estimation of the top ﬁve highest score questions (percentage of correct answers). The descriptive results were analyzed with a one-way ANOVA test. Three out of the top ﬁve highest score questions (Q30, Q39, Q42) show a signiﬁcant diﬀerence between students’ estimation (Table 7). Based on the results, we can state that the students from the third year estimated the questions as signiﬁcantly harder than the Year 1 and Year 2 students. The students from Year 2 estimated\n\n\nPage 3\nto their problem description and presentation, since novices were easily confused by nested loops and recursion, which diﬀerentiated between intermediate programmers’ and novice programmers’ errors.\n\n\nPage 3\ncorrect completion of the code out of a small set of possibilities. An alternative explanation is that many students have a weak grasp of the basic programming principles and were missing the ability to systematically carry out routine programming tasks, such as tracing through code [13]. This working group established that many students lacked knowledge and skills that are a precursor to problem-solving. These missing elements were more associated with the students’ ability to read code than to write it. Many showed weakness in systematically analyzing a short piece of code. The working group did not argue that all students who manifested weakness in problem-solving were doing so due to reading-related factors. They accepted that a student who scored high on the type of tests used in this study, but was unable to write a novel code of similar complexity, was most likely suﬀering from a weakness in problem-solving. The working group merely stated that any research project aiming to study problem-solving skills in novice programmers had to include a mechanism to screen for subjects weak in the precursor, code reading-related skills. Kopec et al. [ 14] analyzed programmers’ examination errors but focused on intermediate programmers (i.e., with some programming experience and understanding of basic programming concepts). They concluded that educators had to pay careful attention\n\n\nPage 10\nAppl. Sci.2020, 10, 7044 10 of 21 The responses were ranked on a three-point Likert scale, where the easy questions were rated as 1, medium questions were rated as 2, and the hard questions were rated as 3. Although the Likert scale is strictly a ranking scale, it is generally accepted that in order to take advantage of the possibilities oﬀered by complex statistical procedures, it is treated as an interval scale. According to Selltiz et al. [21], the scores can be added and averaged. The following analysis presents the degree of estimated di ﬃculty for the top ﬁve correctly answered questions and top ﬁve questions with the lowest correct-answer rate, for all students from all years of study. The results show estimations on the full scale. As already indicated, the students could rate every single question as easy, medium, or hard. In terms of the average of the responses for students from all three years of study, the top ﬁve correctly answered questions were Questions 3, 30, 37, 39, and 42. For these speciﬁc questions, most of the estimations fell into the easy category, which is in accordance with the number of correct answers to those questions (Figure 2). Appl. Sci. 2020, 10, x FOR PEER REVIEW 10 of 21 scale is strictly a ranking scale, it is generally accepted that in order to take advantage of the possibilities offered by complex statistical procedures, it is treated as\n\n\nPage 5\ncategories were the following: 1. Statements. Through these questions, the authors aimed to explore the students’ knowledge of various basic concepts of the C programming language. For example, students had to calculate an arithmetic problem with diﬀerent operators, determine the correct names of variables, ﬁnd syntax errors, etc. The “statement” category contained 12 questions. 2. Conditional statements . This category consisted of questions that used the if, if-else, if-else-if : (ternary operator), and switch structures. The conditions that the students had to examine/conclude ranged from simple to complex. Altogether, there were six conditional statements in this category. 3. Iterations. With the help of these iteration questions, the authors studied how the students would solve questions with for, while, and do-while cycles. The variation in task diﬃculty was achieved by adjusting the conditions of the cycle: they ranged from simple to complex. There were ten iteration-type questions in this category.\n\n\nPage 12\nlowest-score (Q25) question. When estimating the degree of di fficulty of the full scale (the whole test), the majority of students identified it as being of medium difficulty (Table 9). Table 9. Estimating the degree of difficulty for the whole test. n % easy 30 16 medium 147 81 hard 5 3 Total 182 100 Figure 5. Estimation of diﬃculty for the lowest-score (Q25) question. When estimating the degree of diﬃculty of the full scale (the whole test), the majority of students identiﬁed it as being of medium diﬃculty (Table 9).\n\n\nPage 21\n2016, 92–93, 142–160. [CrossRef] © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n\n\nPage 18\nquestion) in CAT, the system uses IRT and requires a database in which all the levels of diﬃculty for all questions (item) are precisely determined (calibrated). Figure 13 shows the probability of a correct response depending on the skill level. The left-hand curve indicates an easy item, the middle-dotted line refers to a medium item, while the broken line on the right side signiﬁes an item whose diﬃculty level is hard. The horizontal axis shows skill levels between −4 and 4.\n\n\nPage 13\nAppl. Sci.2020, 10, 7044 13 of 21 Table 9. Estimating the degree of diﬃculty for the whole test. n % easy 30 16 medium 147 81 hard 5 3 Total 182 100 The diﬀerences in students’ estimation for each year of study are illustrated in Figure 6. Appl. Sci. 2020, 10, x FOR PEER REVIEW 13 of 21 The differences in students’ estimation for each year of study are illustrated in Figure 6. Figure 6. Estimating the difficulty of questions by year of study. The results were tested with the chi square probe (Table 10). Table 10. Correlations between year of study and estimated difficulty of the questions. Easy Medium Hard Total Year 1 12 66 0 78 Year 2 5 53 0 58 Year 3  7 36 3 46 Total 24 155 3 182 The data are consistent with the values shown in the descriptive statistics: the majority of students from all three years of the study estimated the test with medium difficulty value. The results show a correlation between the two variables (performance and task difficulty estimation) (χ2 = 10.73 p = 0.03). When students from a higher year of the study estimated a question as medium or hard, then, they achieved a lower score there. This result supports hypothesis H2. Based on year of study, the au thors separately examined the five best- and worst-performing questions and the estimation associated with them. For these questions, they also compared the estimated value of the questions given by\n\n\nPage 20\nAppl. Sci.2020, 10, 7044 20 of 21 Author Contributions: Conceptualization, A.K.; methodology, R.P . and S.M. ˇC.; software, R.P . and S.M. ˇC.; validation, L.M., R.P . and S.M.ˇC.; formal analysis, A.K. and J.K.; investigation, R.P .; resources, S.M.ˇC.; data curation, S.M. ˇC.; writing—R.P . and A.K.; writing—review and editing, S.M, ˇC and J.K.; visualization, L.M.; supervision, P .ˇC.; project administration, A.K.; funding acquisition, A.K. All authors have read and agreed to the published version of the manuscript. Funding: The project is sponsored by EFOP-3.6.2-16-2017-00018 “Produce together with the nature” project. Conﬂicts of Interest: The authors declare no conﬂict of interest. References 1. Reckase, M.D.; Ju, U.; Kim, S. How Adaptive Is an Adaptive Test: Are All Adaptive Tests Adaptive? J. Comput. Adapt. Test.2019, 7, 1–14. [CrossRef] 2. Luecht, R.M.; Nungester, R.J. Some Practical Examples of Computer-Adaptive Sequential Testing. J. Educ. Meas. 2005, 35, 229–249. [CrossRef] 3. Lister, R.; Clear, T.; Simon, B.; Bouvier, D.J.; Carter, P .; Eckerdal, A.; Jacková, J.; Lopez, M.; McCartney, R.; Robbins, P .; et al. Naturally occurring data as research instrument: Analyzing examination responses to study the novice programmer. ACM SIGCSE Bull.2010, 41, 156–173. [CrossRef] 4. Spiro, R.J.; Jehng, J.C. Cognitive ﬂexibility and hypertext: Theory and technology for the\n\n\nPage 15\nAppl. Sci.2020, 10, 7044 15 of 21 in the descriptive statistics; the Mann–Whitney test does not show a statistically signiﬁcant diﬀerence. This result supports the H3 hypothesis. The questions with lowest scores showing in Figure 10. Appl. Sci. 2020, 10, x FOR PEER REVIEW 15 of 21 the descriptive statistics; the Mann–Whitney test does not show a statistically significant difference. This result supports the H3 hypothesis. The questions with lowest scores showing in Figure 10. Figure 9. Comparing estimation difficulty of the five  highest-scoring ques tions defined by the students in Year 2 and the instructors. Figure 10. Comparing estimation difficulty of the five  lowest-scoring questions defined by the students in Year 2 and the instructors. With the students in Year 3, regarding the questions that received the most points, there was a difference in the estimation for the students an d the teachers for Question 23 (Figure 11). The students rated the question as easy, whereas the teacher ", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Course Structure**: The study focuses on an introductory course in 'Algorithms and Data Structures' for bachelor Informatics students, detailing its components, including lectures, practices, and laboratory sessions.\n2. **Student Performance**: Analysis of student performance metrics, including average grades and pass rates, highlighting challenges faced by students in programming concepts.\n3. **Assessment of Understanding**: Evaluation of student responses to questions categorized by programming concepts (statements, conditional statements, iterations) to gauge understanding and difficulty perceptions.\n\n#### Goals:\n1. **Identify Learning Difficulties**: To explore the learning efficiency of students and identify common difficulties in understanding programming concepts.\n2. **Evaluate Question Difficulty**: To assess how students perceive the difficulty of various questions and correlate this with their performance.\n3. **Enhance Teaching Methods**: To provide insights that can help educators improve instructional strategies based on student performance and feedback.\n\n#### Insights:\n1. **Weakness in Basic Principles**: Many students struggle with foundational programming principles, particularly in reading and analyzing code, which affects their problem-solving abilities.\n2. **Difficulty Estimation**: Students generally rated the overall test as medium difficulty,", "updated_at": 1765795241.8173857}, "8c06e23aa576": {"context": "Page 1\nJWT (JSON Web Token) – הוא Token שמכיל בתוכו (ב-payload) מידע מזוהה על המשתמש (כגון userId, name). – הוא חתום דיגיטלית בעזרת מחרוזת סודית (JWT_SECRET), כך שאפשר לאמת שלא נשמט או שונה. מבנה ה-JWT Header: מבנה ואלגוריתם החתימה (alg: HS256, typ: JWT) Payload: אובייקט JSON עם הנתונים שלך (userId, name, ועוד אם תרצי) Signature: HMAC-SHA256 של Base64(Header) + “.” + Base64(Payload) + ה-secret למה צריך אותו? אחרי שהמשתמש הוכח (login) הוא מקבל את ה-token ומאחסן אותו בצד הלקוח (למשל ב-localStorage או cookie). בקריאות הבאות ל־API, מצרפים אותו בכותרת Authorization: Bearer <token>. בצד השרת, ב־middleware לאימות (שנלמד ב-2.6), מבצעים jwt.verify(token, JWT_SECRET) כדי: לוודא שהחתימה תקינה (לא זויפה) לחלץ את ה-payload (למשל userId) ולהניח אותו ב־req.user expiresIn: \"1h\" – קובע שהטוקן יהיה תקף לשעה אחת בלבד; אחרי זה המשתמש ייאלץ להתחבר מחדש. באופן כללי – התהליך הוא: Register → hash+salt → שמירה ב־DB Login → compare → sign JWT → החזרת token Protected Routes → middleware jwt.verify → אם תקין, ממשיכים לבקשה כך מתקבלת מערכת Authentication סטנדרטית, מאובטחת ומבוססת טוקנים ללא מצרך ב־session state בצד השרת. דומה למשתמש קיים כדי לקבל תוקן: { \"email\": \"vered@example.com\", \"password\": \"Secret123\" } its time to travel\n", "summary": "### Summary of Primary Topics, Goals, and Insights:\n\n**Primary Topics:**\n1. **JWT (JSON Web Token) Structure:**\n   - Components: Header, Payload, Signature.\n   - Signing Algorithm: HMAC-SHA256.\n   - Payload includes user information (e.g., userId, name).\n\n2. **Authentication Process:**\n   - User registration and login flow.\n   - Token issuance upon successful login.\n   - Token storage on the client side (localStorage or cookies).\n\n3. **Token Verification:**\n   - Use of middleware for token validation on the server.\n   - Process of verifying the token and extracting user information.\n\n4. **Token Expiration:**\n   - Tokens are set to expire after a specified duration (e.g., 1 hour).\n\n**Goals:**\n- To establish a secure authentication system using JWTs.\n- To eliminate the need for session state on the server by using stateless tokens.\n\n**Insights:**\n- JWTs provide a compact and self-contained way to transmit user information securely.\n- The authentication flow is streamlined, allowing for easy validation and user management.\n- Understanding the structure and verification process of JWTs is crucial for implementing secure APIs.", "updated_at": 1765795246.9716983}, "7c7dcd7dc1f4": {"context": "Page 8\nrespec- tively. Furthermore, Fig. 12 shows the three logistic regression curves shown in Figs. 9–11.T h e x-axis represents the ability θ, and the y-axis represents the correct answer ratio. The blue, or- ange, and green lines represent the curves for the difﬁculty levels (b) = 0.0, 1.0, and 2.0, respectively. This ﬁgure demonstrates\n\n\nPage 6\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2191 Fig. 4. Difﬁculty-controllable answer-extraction model using BERT. We can ﬁne-tune the answer-extraction model by using a collection of triplets (r,a,b), which can be obtained from the extended SQuAD dataset explained in Section VI-A. This ﬁne- tuning is performed by minimizing cross-entropy loss between the true positions of the start and end of an answer text and the predicted probabilities for these positions. C. Difﬁculty-Controllable Answer-Aware QG Model We use T5 for difﬁculty-controllable answer-aware QG, where T5 is a pretrained transformer model, the details of which are summarized in Table II. We use T5 because it has been widely used before in various text generation tasks [54], [55], [56], [57] including QG tasks [2], [58], [59] and has achieved higher accuracy in QG compared to models, such as BART and GPT-2 [58]. Conventional answer-aware QG models [60] based on pre- trained language models are implemented by designing the model’s input as r1,..., [A],a1,...,a K,[A],...,r I (3) where [A] is a special token representing an answer’s start and end positions within a reading passage. The model’s target, which is a question text, is designed as [G],q1,...,q J ,[E] (4) where [G] and [E] are also special tokens representing the beginning of a question text and the end of a question text,\n\n\nPage 2\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2187 Fig. 1. Conventional QG task: answer-aware QG. 1) The relationship between the difﬁculty of the questions and the learner’s ability is ignored, making it difﬁcult to determine the appropriate difﬁculty for each learner. 2) The methods are answer-aware, which means a reading passage and an answer text must be input to generate questions, as shown in Fig. 1. For this reason, they can- not generate question–answer pairs. Generating answers along with questions is essential to realize automatic assessment of learners’ responses to generated questions. Furthermore, controlling the difﬁculty of the generated answers is also crucial because both questions and answers generally affect the overall difﬁculty. To address these issues, we introduce a novel method for gen- erating question–answer pairs while considering the difﬁculty associated with the learners’ ability. A unique feature of our method is the use of item response theory (IRT) [29] to quantify the question difﬁculty. The IRT is based on statistical models that deﬁne the relationship between question difﬁculty and learner ability, thereby facilitating the selection of an appropriate difﬁculty level for each learner. For this reason, our method is designed to generate question–answer pairs while controlling IRT-based difﬁculty. For our QG\n\n\nPage 1\nlearner’s abilities. © 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\n\nPage 7\nFurthermore, Fig. 7 is a histogram of the difﬁculty\n\n\nPage 8\nanalysis is given by Fig. 9, which shows the correct answer ratio of the 60 QA systems for the generated question–answer pairs given b =1 .0.T h e x-axis represents the ability θ,t h ey-axis represents the correct answer ratio, each plot indicates an individual QA system, and the orange line repre- sents the logistic regression curve ﬁtted to the 60 data points by the least-squares method. As described before, the Rasch model gives a probability of 0.5 that a correct answer will be given when θ = b. To emphasize this point, the green dashed line represents θ =1 .0 and a correct answer ratio of 0.5. The ﬁgure shows that the QA systems with an ability level around θ =1 .0 demonstrate a correct answer ratio of approximately 0.5, suggesting that the questions are generated as expected. A similar trend was observed for other difﬁculty levels, as exempliﬁed in Figs. 10 Fig. 9. Average correct answer ratio of the QA systems for generated questions given difﬁculty b =1 .0. Each dot plot represents an individual QA system. Fig. 10. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b =2 .0. Each dot plot represents an individual QA system. Fig. 11. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b =0 .0. Each dot plot represents an individual QA system. and 11, which show the results for b =2 .0 and b =0 .0,\n\n\nPage 6\nfor enhancing reading comprehensive skills. However, in typical educational settings, a learner’s ability is often unknown a priori. To address this, we are proposing an efﬁcient method for estimating a learner’s ability. We utilize the CAT framework, as mentioned in Section V-B, while generating and administering questions at appropriate levels of difﬁculty. Speciﬁcally, we propose the following procedure for ability estimation and QG. 1) Randomly generate and administer a few questions to the learner to obtain initial response data. 2) Utilize the obtained response data along with the Rasch model to estimate and update the learner’s ability level. 3) Generate and administer a question at the estimated ability level using the proposed QG method, thereby obtaining new response data. 4) Repeat steps (2) and (3). Using this procedure, we can effectively estimate a learner’s ability while administering questions at appropriate levels of difﬁculty. Remember that when the ability and difﬁculty levels are equal, the Rasch model produces a 0.5 probability that a correct answer will be obtained, and Fischer information is maximized.\n\n\nPage 12\nlanguage understand- ing,” in Proc. Neural Inf. Process. Syst., 2019. [35] P. He, X. Liu, J. Gao, and W. Chen, “DeBERTa: Decoding-enhanced BERT with disentangled attention,” in Proc. Int. Conf. Learn. Representations, 2021. [36] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence embeddings using Siamese BERT-networks,” in Proc. Conf Empirical Methods Nat- ural Lang. Process./9th Int. Joint Conf Natural Lang. Process., 2019, pp. 3982–3992. [37] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher, “CTRL: A conditional transformer language model for controllable gen- eration,” 2019, arXiv:1909.05858. [38] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT: A lite BERT for self-supervised learning of language repre- sentations,” in Proc. Int. Conf. Learn. Represent., 2020. [39] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a dis- tilled version of BERT: Smaller, faster, cheaper and lighter,” 2019, arXiv:1910.01108. [40] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” in Proc. Int. Conf. Learn. Represent., 2020. [41] S. Subramanian, T. Wang, X. Yuan, S. Zhang, A. Trischler, and Y . Bengio, “Neural models for key phrase extraction and question generation,” in Proc. Workshop Mach. Reading Question Answering, 2018, pp. 78–88. [42] Y . Kim, H. Lee, J.\n\n\nPage 5\ndifﬁculty levels for their ability. Because our difﬁculty-controllable QG method requires a dataset with IRT-based difﬁculty values for model training, we ﬁrst propose a method for constructing it in the next section. A. Creating a Dataset With IRT-Based Question Difﬁculty While several popular datasets have been developed for gen- eral reading comprehension QG tasks [1], the most popular is SQuAD [30], which consists of over 100 000 question–answer pairs from Wikipedia articles posed by crowdworkers. Speciﬁ- cally, SQuAD is a collection of triplets (r,q,a), where each an- swer a is a text fragment from a corresponding reading passage r and each reading passage r corresponds to a paragraph of a Wikipedia article. However, to construct a difﬁculty-controllable QG method, we require a dataset consisting of quadruplets (r,q,a,b). Thus, we ﬁrst propose a method for extending the SQuAD dataset by appending the IRT-based difﬁculty values for each question–answer pair. The details for doing so are as follows. 1) Collecting response data for each question–answer pair: We collect answers from multiple respondents to each question in the SQuAD dataset and grade those answers as correct or incorrect based on exact matching with the corresponding true answers. Ideally, we should gather responses from a population of target learners, but this is highly expensive and time consuming. Thus,\n\n\nPage 5\n2190 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 TABLE II CONFIGURATION OF BERT AND T5 USED AS THE BASIS MODELS IN THIS STUDY3 The reason for using the Fisher information for item selection is that the variance of the maximum likelihood estimate of ability converges to the inverse of Fisher information, meaning that an item with the maximum Fisher information is the most effective for accurately estimating the examinee’s ability. In the Rasch model, Fisher information is maximized when the item difﬁculty parameter bequals the ability value θ. Repeating the ability estimation and the Fisher-information-based item selection enables us to achieve a highly accurate ability estimate with fewer items administered [31]. VI. P ROPOSED METHOD Based on conventional QG methods and IRT approach, our difﬁculty-controllable QG method is carried out by performing the following two tasks in sequence. 1) Difﬁculty-controllable answer extraction, which extracts an answer text from a given reading passage while considering a target IRT-based difﬁculty value. 2) Difﬁculty-controllable answer-aware QG, which gener- ates a question given a reading passage, an answer text, and a target IRT-based difﬁculty value. Furthermore, within the framework of CAT, we propose a methodology for sequentially estimating a learner’s ability and adaptively generating questions with appropriate\n\n\nPage 10\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2195 TABLE IV HUMAN EVA L UAT I O N 900 or higher. 6 The raters were asked to assess the assigned question–answer pairs based on the following evaluation crite- ria. The raters were crowdworkers having a TOEIC score of 900 or higher . 1) Fluency: Evaluate the grammatical correctness and ﬂu- ency of the questions. Ratings were done on a three-point scale: 3. appropriate, 2. acceptable, and 1. inappropriate. 2) Content Relevance: Assess whether the generated ques- tions were relevant to the reading passage. Ratings were done on a two-point scale: 2. appropriate and 1. inappro- priate. 3) Answerability: Evaluate whether the extracted answer was indeed the correct answer for the generated question. Ratings were done on a nominal scale with four categories: a. appropriate, b. insufﬁcient, c. excessive, and d. inappro- priate. 4) Practicality: Assess whether the question and/or answer could become feasible with slight modiﬁcations. Ratings were done on a three-point scale: 3. already feasible, 2. need minor correction, and 1. not feasible. 5) Question Difﬁculty: Evaluate the difﬁculty level of the generated question. Ratings were done on a ﬁve-point scale, ranging from 1 (easiest) to 5 (most difﬁcult). Results for ﬂuency, content relevance, answerability, and practicality are presented in Table IV. From\n\n\nPage 13\nNatural Lang. Process., 2021, pp. 692–701. [61] J. M. Linacre, “Sample size and item calibration stability,” Rasch Meas. Trans., vol. 7, 1994, Art. no. 328. Yuto Tomikawareceived the B.S. degree in engineer- ing from the University of Electro-Communications, Chofu, Japan, in 2023, where he is currently working toward the M.S. degree in engineering. His research interests include educational and p", "summary": "### Study Guide: Adaptive Question-Answer Generation with Difficulty Control\n\n#### Primary Topics:\n1. **Adaptive Question Generation (QG)**: Focuses on generating questions that are tailored to the learner's ability level.\n2. **Difficulty Control**: The method incorporates difficulty levels into the question-answer pairs, allowing for a more personalized learning experience.\n3. **Item Response Theory (IRT)**: Utilized to quantify question difficulty and relate it to learner ability, facilitating appropriate difficulty selection.\n4. **Model Frameworks**: The use of BERT and T5 models for answer extraction and question generation, respectively.\n\n#### Goals:\n- To develop a QG system that generates question-answer pairs while considering the difficulty level appropriate for each learner.\n- To estimate a learner's ability dynamically using a Computerized Adaptive Testing (CAT) framework.\n- To enhance reading comprehension skills through tailored question generation.\n\n#### Insights:\n- The proposed method involves a sequential process: \n  1. Initial questions are administered to gather response data.\n  2. The Rasch model is applied to estimate the learner's ability based on their responses.\n  3. Questions are generated at the estimated ability level, and the process is repeated to refine ability estimation.\n- The", "updated_at": 1765795718.7273958}, "e79987bf2e83": {"context": "Page 19\no n s  w h e n  g i v i n g  t h e correct answer depends on knowledge in some other topic, but then such items should be excluded from the database of questions. A s  f o r  f u t u r e  p l a n s ,  t h e  n e x t  p h a s e  o f  t h i s  p a r t i c u l a r  r e s e a r c h  w i l l  i n c l u d e  a  m o r e  d e t a i l e d analysis of the results. Comparing the results by the categories (sub-scale) can offer a better understanding of the efficiency of the implemented teaching methodology. The results of this comparison can aid teachers in pinpointing the specif ic parts of the material in the CS1 course that need to be taught or explained in a different way. Figure 13. Probability for the correct answer. The results of this research can help instructors in creating items for such a database. The following passage oﬀers some ways to apply the results of the research to CAT. Based on the score (correct answers) results (Figure 1), groups of diﬃculties can be formed. For example, questions for which less than 20% of the students supplied a correct answer would be classiﬁed as hard. Questions with scores between 21% and 40% would be medium, while the rest would be categorized as easy. This is a categorization of the items and not the calibration. However, this categorization can be used to build di ﬀerent variations of tests with approximately the same diﬃculty by selecting\n\n\nPage 7\nAppl. Sci.2020, 10, 7044 7 of 21 Appl. Sci. 2020, 10, x FOR PEER REVIEW 7 of 21 } a. 14, 24 b. 11, 21 c. 10, 20 d. Nothing, there is an error in the code e. 12, 23 4.3. Estimation of the Question Difficulty Students’ subjective estimation of question diff iculty is very useful feedback. Based on the students’ answers, teachers gain information abou t which part of the study material the students understand the least. Furthermore, the teachers can rethink and revise their own estimation of the questions’ difficulty. For example, a given question thought to be ‘easy’ by the teacher may be perceived as an unsolvable problem by the studen ts themselves. This feed back can be used by teachers when preparing questions for the exam in order to only include questions with approximately the same difficulty. As mentioned above, a subjective estimation of question difficulty can also serve in computerized adaptive testing that adapts to the examinee’s level of ability. This is advantageous from the examinee’s perspective, as the difficulty of the exam seems to tailor itself to their level of ability. For instance , if an examinee performs well on an item of intermediate difficulty, they will then be presented with more difficult questions. Or, if they performed poorly, they will be presented with a simpler question. The evaluation of task difficulty was conducted on two levels, by the\n\n\nPage 3\nEducation) working group [12] assessed the programming ability of 216 post-CS1 students from eight tertiary institutions in various countries. The “McCracken group” used a common set of programming problems. The majority of students performed much worse than their teachers had expected. The average score was 22.89 out of 110 points. While such a report by an author at a single institution might be dismissed as a consequence of poor teaching at that particular institution, dismissing a multinational study is not done so lightly. Given the scale and the multinational nature of the collaboration, these results were widely viewed as signiﬁcant and compelling. The McCracken study did not isolate the causes of the problem. A popular explanation for the students’ poor performance was that they lacked the ability to problem-solve. In fact, students lack the ability to take a problem description, decompose it into sub-problems, implement the necessary steps, and then reassemble the pieces to create a complete solution. Based on the McCracken group research, an ITiCSE 2004 working group (the “Leeds Group”) tested students from seven countries in two ways. First, students were tested on their ability to predict the outcome of executing a short piece of code. Next, the students were given the desired function of a short piece of near-complete code and tested on their ability to select the\n\n\nPage 7\nparticipant teachers and students. The teachers involved in this study gave their subjective estimation both of the difficulty (how challenging a given task was) and complexity  (how complicated the question was, how many steps it involved) of each question, resulting in two separate marks for every question. The underlying reason was to increase the objectivity and accuracy of difficulty estimation. This step was followed by a detailed professional discussion of the given marks, which were then transformed into the difficulty values of “easy”, “medium” and “hard”. While the teachers performed this double estimation, primarily to make the overall estimation more objective, such double estimation was not required from the students, because the authors believed it would prove too distracting for students, given that they had to pay close attention to actually solving the question and then evaluating its difficulty. In the test itself, the students were able to give a subjective estimation of the question difficulty by selecting one of the three offered options: “easy”, “medium” and “hard”, as seen above, next to the sample questions. 4.4. Participants As stated before, the total number was 182 bachelor students of informatics. Table 1. shows the structure of the participating students by year of study. Table 1. The number of students per year of study. n % First year 78 42.9\n\n\nPage 1\napplied sciences Article Case Study: Students’ Code-Tracing Skills and Calibration of Questions for Computer Adaptive Tests Robert Pinter 1, *, Sanja Maravi´ cˇCisar 1 , Attila Kovari 2 , Lenke Major 3, Petar ˇCisar 4 and Jozsef Katona 5 1 Department of Informatics, Subotica Tech—College of Applied Science, 24000 Subotica, Serbia; sanjam@vts.su.ac.rs 2 Department of Natural Sciences and Environmental Protection, Institute of Engineering, University of Dunaujvaros, 2400 Dunaújváros, Hungary; kovari@uniduna.hu 3 University of Novi Sad Hungarian Language Teacher Training Faculty, 24000 Subotica, Serbia; major.lenke@magister.uns.ac.rs 4 Department of Informatics, University of Criminal Investigation and Police Studies, 11080 Belgrade-Zemun, Serbia; petar.cisar@kpu.edu.rs 5 CogInfoCom based LearnAbility Reseacrh Team, University of Dunaujvaros, 2400 Dunaújváros, Hungary; katonaj@uniduna.hu * Correspondence: pinter.robert@vts.su.ac.rs Received: 31 August 2020; Accepted: 8 October 2020; Published: 11 October 2020 /gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001 /gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046 Featured Application: Authors are encouraged to provide a concise description of the speciﬁc application or a potential application of the work. This section is not mandatory. Abstract: Computer adaptive testing (CAT)\n\n\nPage 20\nnonlinear and multidimensional traversal of complex subject matter. In Cognition, Education, and Multimedia: Exploring Ideas in High Technology; Nix, D., Spiro, R.J., Eds.; Lawrence Erlbaum Associates, Inc.: Mahwah, NJ, USA, 1990; pp. 163–205. 5. Maravi´ cˇCisar, S.; Radosav, D.; Pinter, R.; ˇCisar, P . Eﬀectiveness of Program Visualization in Learning Java: A Case Study with Jeliot 3. Int. J. Comput. Commun. Control2011, 6, 668–680. [CrossRef] 6. Lopez, M.; Whalley, J.; Robbins, P .; Lister, R. Relationships between reading, tracing and writing skills in introductory programming. In Proceedings of the Fourth International Workshop on Computing Education Research (ICER ‘08), Sydney, Australia, 6–7 September 2008; pp. 101–112. 7. Kumar, A. Solving Code-tracing Problems and its E ﬀect on Code-writing Skills Pertaining to Program Semantics. In Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education (ITiCSE’15), Vilnius, Lithuania, 4–8 July 2015; pp. 314–319. 8. Cunningham, K.; Blanchard, S.; Ericson, B.; Guzdial, M. Using tracing and sketching to solve programming problems: Replicating and extending an analysis of what students draw. In Proceedings of the 2017 ACM Conference on International Computing Education Research, Tacoma, WA, USA, 18–20 August 2017; pp. 164–172. 9. Nelson, G.L.; Xie, B.; Ko, A.J. Comprehension First: Evaluating a\n\n\nPage 15\nAppl. Sci.2020, 10, 7044 15 of 21 in the descriptive statistics; the Mann–Whitney test does not show a statistically signiﬁcant diﬀerence. This result supports the H3 hypothesis. The questions with lowest scores showing in Figure 10. Appl. Sci. 2020, 10, x FOR PEER REVIEW 15 of 21 the descriptive statistics; the Mann–Whitney test does not show a statistically significant difference. This result supports the H3 hypothesis. The questions with lowest scores showing in Figure 10. Figure 9. Comparing estimation difficulty of the five  highest-scoring ques tions defined by the students in Year 2 and the instructors. Figure 10. Comparing estimation difficulty of the five  lowest-scoring questions defined by the students in Year 2 and the instructors. With the students in Year 3, regarding the questions that received the most points, there was a difference in the estimation for the students an d the teachers for Question 23 (Figure 11). The students rated the question as easy, whereas the teacher saw it as medium. The other questions’ estimation did not show much diversion: the majority of students as well as the teachers found the questions easy. Figure 9. Comparing estimation diﬃculty of the ﬁve highest-scoring questions deﬁned by the students in Year 2 and the instructors. Appl. Sci. 2020, 10, x FOR PEER REVIEW 15 of 21 the descriptive statistics; the Mann–Whitney test does not\n\n\nPage 1\nenables an individualization of tests and better accuracy of knowledge level determination. In CAT, all test participants receive a uniquely tailored set of questions. The number and the diﬃculty of the next question depend on whether the respondent’s previous answer was correct or incorrect. In order for CAT to work properly, it needs questions with suitably deﬁned levels of diﬃculty. In this work, the authors compare the results of questions’ diﬃculty determination given by experts (teachers) and students. Bachelor students of informatics in their ﬁrst, second, and third year of studies at Subotica Tech—College of Applied Sciences had to answer 44 programming questions in a test and estimate the diﬃculty for each of those questions. Analyzing the correct answers shows that the basic programming knowledge, taught in the ﬁrst year of study, evolves very slowly among senior students. The comparison of estimations on questions diﬃculty highlights that the senior students have a better understanding of basic programming tasks; thus, their estimation of diﬃculty approximates to that given by the experts. Keywords: computer adaptive testing; code tracing; basic programming skills 1. Introduction Modern technologies o ﬀer numerous possibilities for improving knowledge assessment and the process of education. In higher education, testing is one of the most commonly used methods of\n\n\nPage 14\nthe students and the instructor were no longer so consistent (Figure 8). For Questions 6 an d 16, the majority of students and the instructor w e r e  o f  t h e  s a m e  o p i n i o n .  Q u e s t i o n  2 5  w a s  m a r k e d  a s  m e d i u m  b y  t h e  s t u d e n t s  a n d  h a r d  b y  t h e instructor. Question 43 was rated as hard by the students as opposed to medium by the instructor. Statistically, these differences are not significan t. However, for Question 14, where students estimated the question as medium and the instructor estimated the question as easy, there was a significant difference in estimation based on the Mann–Whitney test (Z = −1.97 p = 0.04). This result supports hypothesis H3: it indicates that", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Computer Adaptive Testing (CAT):** The document discusses the implementation and benefits of CAT in educational settings, particularly in assessing programming skills.\n2. **Question Difficulty Estimation:** It emphasizes the importance of accurately categorizing question difficulty based on both student performance and teacher assessments.\n3. **Student Performance Analysis:** The analysis of student responses to programming questions to identify areas of difficulty and improve teaching methodologies.\n\n**Goals:**\n1. **Enhance Assessment Accuracy:** To improve the accuracy of knowledge assessments through tailored testing that adjusts to individual student performance.\n2. **Identify Learning Gaps:** To use student feedback on question difficulty to identify specific areas in the curriculum that may require different teaching approaches.\n3. **Facilitate Better Teaching Strategies:** To aid instructors in refining their question design and teaching methods based on empirical data from student performance.\n\n**Insights:**\n1. **Discrepancy in Difficulty Perception:** There is often a mismatch between how teachers and students perceive question difficulty, which can inform adjustments in teaching and assessment strategies.\n2. **Categorization of Questions:** Questions can be categorized into difficulty levels (easy, medium, hard) based on student performance", "updated_at": 1765795724.014123}, "d6f4e8d7406f": {"context": "Page 1\nJWT (JSON Web Token) – הוא Token שמכיל בתוכו (ב-payload) מידע מזוהה על המשתמש (כגון userId, name). – הוא חתום דיגיטלית בעזרת מחרוזת סודית (JWT_SECRET), כך שאפשר לאמת שלא נשמט או שונה. מבנה ה-JWT Header: מבנה ואלגוריתם החתימה (alg: HS256, typ: JWT) Payload: אובייקט JSON עם הנתונים שלך (userId, name, ועוד אם תרצי) Signature: HMAC-SHA256 של Base64(Header) + “.” + Base64(Payload) + ה-secret למה צריך אותו? אחרי שהמשתמש הוכח (login) הוא מקבל את ה-token ומאחסן אותו בצד הלקוח (למשל ב-localStorage או cookie). בקריאות הבאות ל־API, מצרפים אותו בכותרת Authorization: Bearer <token>. בצד השרת, ב־middleware לאימות (שנלמד ב-2.6), מבצעים jwt.verify(token, JWT_SECRET) כדי: לוודא שהחתימה תקינה (לא זויפה) לחלץ את ה-payload (למשל userId) ולהניח אותו ב־req.user expiresIn: \"1h\" – קובע שהטוקן יהיה תקף לשעה אחת בלבד; אחרי זה המשתמש ייאלץ להתחבר מחדש. באופן כללי – התהליך הוא: Register → hash+salt → שמירה ב־DB Login → compare → sign JWT → החזרת token Protected Routes → middleware jwt.verify → אם תקין, ממשיכים לבקשה כך מתקבלת מערכת Authentication סטנדרטית, מאובטחת ומבוססת טוקנים ללא מצרך ב־session state בצד השרת. דומה למשתמש קיים כדי לקבל תוקן: { \"email\": \"vered@example.com\", \"password\": \"Secret123\" } its time to travel\n", "summary": "### Summary of Primary Topics, Goals, and Insights:\n\n1. **JWT Overview**:\n   - Definition: JSON Web Token (JWT) is a token that contains identifiable user information (e.g., userId, name) in its payload.\n   - Structure: Composed of three parts - Header, Payload, and Signature.\n\n2. **JWT Structure**:\n   - **Header**: Contains the signing algorithm (e.g., HS256) and token type (JWT).\n   - **Payload**: A JSON object with user data (e.g., userId, name).\n   - **Signature**: Created using HMAC-SHA256 to ensure integrity and authenticity.\n\n3. **Purpose of JWT**:\n   - Used for user authentication after login.\n   - The token is stored client-side (e.g., in localStorage or cookies) and sent in the Authorization header for subsequent API requests.\n\n4. **Authentication Process**:\n   - User registration involves hashing and salting passwords before storing them in the database.\n   - On login, credentials are compared, and a JWT is signed and returned to the user.\n   - Protected routes utilize middleware to verify the token and extract user information.\n\n5. **Token Expiration**:", "updated_at": 1765795728.1588178}, "758c82c61bcc": {"context": "Page 12\ntranslation, and comprehension,” in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020, pp. 7871–7880. [27] C. Raffel and et al., “Exploring the limits of transfer learning with a uniﬁed text-to-text transformer,”J .M a c h .L e a r n .R e s ., vol. 21, no. 140, pp. 1–67, Jan. 2020. [28] Y . Cheng et al., “Guiding the growth: Difﬁculty-controllable question generation through step-by-step rewriting,” in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics/11th Int. Joint Conf. Natural Lang. Process., 2021, pp. 5968–5978. [29] F. M. Lord, Applications of Item Response Theory to Practical Testing Problems. Evanston, IL, USA: Routledge, 1980. [30] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000 questions for machine comprehension of text,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2016, pp. 2383–2392. [31] W. J. van der Linden and C. A. Glas, Elements of Adaptive Testing.N e w York, NY , USA: Springer, 2010. [32] Q. Zhou, N. Yang, F. Wei, C. Tan, H. Bao, and M. Zhou, “Neural question generation from text: A preliminary study,” in Proc. Nat. CCF Conf. Natural Lang. Process. Chin. Comput., 2018, pp. 662–671. [33] A. Vaswani et al., “Attention is all you need,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2017. [34] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le, “Xlnet: Generalized autoregressive pretraining for\n\n\nPage 1\nfer transformer (T5) [27]. These approaches have successfully generated ﬂuent questions that are relevant to the given reading passages. A notable application of QG in education is reading tutors [1], [3], [4], [5], which provide reading-comprehension questions for diverse reading materials. Offering questions helps direct learn- ers’ attention to the content and helps them identify misconcep- tions, thereby improving their reading-comprehension skills [3]. To enhance the efﬁciency of such learning, offering questions with difﬁculty levels tailored to each learner’s reading ability is beneﬁcial. For this reason, several difﬁculty-controllable QG methods have recently been proposed. Difﬁculty-controllable QG for reading comprehension is a rel- atively new area of research, and thus, the literature is scant [22], [28]. One method for realizing difﬁculty-controllable QG is to use an RNN-based seq2seq model in which the hidden states from the encoder are adjusted to accept the difﬁculty levels categorized as either easy or hard [22]. Another approach is a multihop method [28], which deﬁnes the question difﬁculty according to the number of inference steps required to answer the question and generates questions by controlling the number of these inference steps. However, both methods face the following limitations that make it difﬁcult to generate questions suitable for the\n\n\nPage 5\nwe substitute actual learner responses with automated QA systems, in the same way that several previous difﬁculty-controllable QG studies have done [12], [22]. 2) Difﬁculty estimation using IRT: Using the collected re- sponse data, we estimate 3 the question difﬁculty based on the Rasch model following the item calibration procedure introduced in Section V. 3) Creating a dataset with difﬁculty estimates:We construct a dataset consisting of quadruplets (r,q,a,b) by appending the estimated difﬁculty values b into the triplets (r,q,a) of the SQuAD dataset. Using this dataset, the proposed method trains following two models: 1) a difﬁculty-controllable answer-extraction model; 2) a difﬁculty–controllable answer-aware QG model. B. Difﬁculty-Controllable Answer-Extraction Model We use BERT [24] for difﬁculty-controllable answer extrac- tion. BERT is a pretrained transformer model, the details of which are summarized in Table II. BERT can be adapted for various downstream tasks by ﬁne-tuning it with task-speciﬁc su- pervised datasets and incorporating task-speciﬁc output layers. We employed BERT for the answer-extraction task because of its extensive prior usage in various text-extraction applications[53]. To perform answer extraction using BERT, we add output layers that predict the start and end positions of the answer text within a given reading passage. Speciﬁcally, letting vi be\n\n\nPage 11\nparameter θ of learners is unknown, adaptive generation enables more efﬁcient estimation of θ compared to random question posing. This results in the effective generation of questions tailored to individual abilities. Finally, we summarize the conclusions based on our two research questions. Our ﬁrst research question was, “Is it possi- ble to generate question–answer pairs by specifying arbitrary IRT-based difﬁculty values?” We conclude that our method makes this possible because our experiments demonstrated that the speciﬁed IRT-based difﬁculty values correlate well with the correct answer rates of both QA systems and human respon- dents. Our second research question was, “Can the adaptive QG strategy, based on the CAT framework, enhance the selection of more suitable difﬁculty levels for each learner through efﬁcient ability estimation?” We also answer this question afﬁrmatively, given that our experiments in Section VIII demonstrated that the proposed method increases ability estimation accuracy more quickly than the random method as the number of questions increases, indicating that the proposed method enables more ef- ﬁcient ability estimation. The accurate ability estimation allows for the selection of difﬁculty levels that more closely align with each learner’s true ability, leading to more appropriate difﬁculty selection. One limitation of this study is that we used\n\n\nPage 2\nillustrated in Fig. 2. Our second objective is to develop an adaptive QG framework that generates questions with difﬁculty levels appropriate for learners while efﬁciently estimating their abilities each time a question is presented. The detailed task deﬁnition for the ﬁrst objective is as follows. Let a given reading passage be a word sequence r = {ri | i ∈ {1,...,I }}, where ri represents the ith word in the passage, and I is the passage text length. Similarly, let a question text q and an answer text a be word sequences q = {qj | j ∈{ 1,...,J }} and a = {ak | k ∈{ 1,...,K }}, respectively, where qj is the jth word in the question text, ak is the kth word in the answer text, Jis the question text length, and Kis the answer text length. Note that the answer text a must be a subset of the word sequence in the reading passage r, namely, a ⊂ r, which means that our answer-generation task can be seen as a text span extraction from a reading passage. This implies that our answer-generation task is a text span extraction from the reading passage, as in typical answer-aware QG tasks [30]. Using this notation, our task is to\n\n\nPage 4\nbut lie behind the observed data. 2“Discriminative power” means the degree to which an item can distinguish between examinees with different levels of the underlying ability. Fig. 3. IRCs for a Rasch model with different item difﬁculty values. the probability p. In this ﬁgure, the horizontal axis represents the ability θ; the vertical axis represents the probability p; and the three solid curves represent the IRCs for three items with different difﬁculty levels, b = −1.0, 0.0, and 1.0. These IRCs demonstrate that examinees with higher θ have an increased probability of responding correctly to each item, indicating that it reﬂects a reasonable relationship between the examinee’s ability and the probability of a correct response. In addition, it is evident that the IRC shifts to the right as the bvalue increases, indicating that higher ability θ is necessary to correctly answer items with higher b values. This means that the parameter b reﬂects the difﬁculty of each item. Moreover, this model enables the calculation of the correct answer probability from the ability and difﬁculty values, which helps to determine a question with an appropriate difﬁculty level for an examinee with a given ability. For example, when we aim to select a question that an examinee with ability θwill answer correctly with 0.5 probability, offering a question with b ≒ θ is appropriate because the\n\n\nPage 1\nlearner’s abilities. © 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\n\nPage 10\nBased on the correct and incorrect response data, we calculated the correct rates for each question as an index representing its difﬁculty. The correlation between correct rates and the speciﬁed difﬁ- culty levels was −0.67, where a test for noncorrelation revealed that the p-value was less than .01, conﬁrming a statistically signiﬁcant correlation at the 1% level. These results suggest that the proposed method can generate questions in alignment with the difﬁculty levels perceived by human respondents. VIII. E VA L UAT I O N O F T H EACCURACY OF LEARNERS’ABILITY ESTIMATION In this section, we evaluate the efﬁcacy of the method of adaptive QG proposed in Section V-B. We simulated the adap- tive QG process for three respondents with different abili- ties. For the respondents, we used three QA models with the minimum and maximum θ, speciﬁcally, −3.658 and 2.766, as well as that which was closest to the average, θ = 1.244. Using the three QA models, we examined the following two QG processes and analyzed the trajectory of their estimated abilities. 1) Adaptive Generation (Proposed):Initially, ten questions with random difﬁculty are generated and administered. Then, 40 questions are administered, following the adap- tive procedures detailed in Section VII-B, while updating the ability estimates.\n\n\nPage 9\nWe then divided them into ﬁve sets, each containing 20 question–answer pairs, and assigned two raters to each set. The raters were crowdworkers having Test of English for International Communication (TOEIC) scores of\n\n\nPage 4\nRESPONSE THEORY (IRT) The IRT [29] is a statistical framework that uses probabilistic models, called IRT models, to estimate two latent factors:1 exam- inee’s ability and item characteristics such as item difﬁculty and discriminative power,2 where the examinee and the item corre- spond to the learner and the question, respectively, in our study. These latent factors are estimated from the response data, which generally consist of the examinee’s binary correct/incorrect re- sponses to the items. The IRT has been widely used in various educational and psychological tests because of its numerous ben- eﬁts, such as offering accurate estimates of examinee’s ability and item characteristics, unifying measurement scales among different tests, and facilitating CAT applications [45], [46], [47], [48]. This study uses the Rasch model [49], the most traditional and well-known IRT model, to quantify question difﬁculty. A. Rasch Model The Rasch model deﬁnes the probability that the mth exami- nee correctly answers the nth item as pnm = exp(θm −bn) 1 + exp(θm −bn) (1) where bn denotes the difﬁculty of the nth item, and θm denotes the latent ability of the mth examinee. For a detailed explanation of this model, Fig. 3 illustrates item response curves (IRCs) for the Rasch model, which are plots of 1In this context, “latent” indicates that the corresponding factors are not directly observable\n\n\nPage 12\nShin, and K. Jung, “Improving neural question gener- ation using answer separation,” in Proc. AAAI Conf. Artif. Intell., 2019, pp. 6602–6609. [43] X. Sun, J. Liu, Y . Lyu, W. He, Y . Ma, and S. Wang, “Answer-focused and position-aware neural question generation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2018, pp. 3930–3939. [44] M. Ueno and Y . Miyazawa, “IRT-based adaptive hints to scaffold learning in programming,”IEEE Trans. Learn. Technol., vol. 11, no. 4, pp. 415–428, Oct.–Dec. 2018.\n\n\nPage 1\nadvance, we propose an adaptive QG framework that efﬁciently estimates the learners’ abilities while generating and presenting questions with difﬁcult", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Question Generation (QG):** Focuses on generating reading comprehension questions tailored to learners' abilities.\n2. **Difficulty-Controllable QG:** Discusses methods to adjust question difficulty based on learners' reading levels.\n3. **Item Response Theory (IRT):** Utilizes IRT, particularly the Rasch model, to estimate question difficulty and learner ability.\n4. **Adaptive Learning Framework:** Proposes an adaptive QG framework that dynamically adjusts question difficulty based on real-time learner performance.\n\n**Goals:**\n1. **Enhance Reading Comprehension:** Improve learners' comprehension skills through targeted questioning.\n2. **Tailor Difficulty Levels:** Develop methods to generate questions that match individual learners' abilities, facilitating personalized learning experiences.\n3. **Efficient Ability Estimation:** Create a system that accurately estimates learners' abilities to select appropriately challenging questions.\n\n**Insights:**\n1. **Correlation of Difficulty and Performance:** The study found a significant correlation between specified question difficulty levels and learner performance, validating the effectiveness of the proposed methods.\n2. **Adaptive QG Efficiency:** The adaptive QG strategy based on the CAT framework improves the accuracy of ability estimation", "updated_at": 1765801157.2596219}, "678ec988fa95": {"context": "Page 11\nAppl. Sci.2020, 10, 7044 11 of 21 into the medium or hard category (Figure 3). Based on the results, a review of question Q6 is deﬁnitely recommended, as the majority of students rated it as an easy question; however, they received one of the lowest scores (the lowest rate of correct answers) for this question. Appl. Sci. 2020, 10, x FOR PEER REVIEW 11 of 21 Figure 3. Estimation of the top five lowest-score questions (percentage of correct answers). Four from the top five lowest-score questions (Q6, Q14, Q20, Q25) show a significant difference between students’ estimation (Table 8). The Q6 question was estimated as significantly harder by the Year 3 students than the Year 1 and Year 2 students. In the case of Q14, opposite estimation was done: this question was “harder” for the Year 1 students. The result for Q14 supports the H2 hypothesis. The estimations for Q20 and Q25 questions differ from the previous three. Table 8. The top five lowest-score questions. Year 1 Year 2 Year 3 One-Way ANOVA Avg. Sd. Avg. Sd. Avg. Sd. F p Q6 1.47 0.6 1.60 0.6 2.37 0.7 23.2 0.001 Q13 2.48 0.7 2.44 0.7 2.48 0.6 0.02 0.9 Q14 2.12 0.5 1.96 0.6 1.70 0.8 5.93 0.003 Q20 2.21 0.6 2.62 0.5 2.11 0.6 6.86 0.001 Q25 2.22 0.6 2.63 0.4 2.59 0.7 7.23 0.001 These results do not support hypothesis H2 and show only that the students’ estimation was close to the real difficulty of the questions: wh en a\n\n\nPage 3\nAppl. Sci.2020, 10, 7044 3 of 21 • By tracing, developers can identify bottlenecks and focus on improving performance. • When a problem does occur, tracing allows the user to see how it came to be: which function, duration of a function, which parameters passed, and how deep into the function the user could delve. 2. Related Works Problems that arise in novices when learning programming is a ﬁeld that has been the subject of numerous studies, as have the ways of adopting those new concepts [10]. Xie et al. [11] proposed a theory that identiﬁed four distinct skills that novices learned incrementally. These skills were tracing, writing syntax, comprehending templates, and writing code with templates. They assumed that the explicit instruction of these skills decreased cognitive demand. The authors conducted an exploratory mixed-methods study and compared students’ exercise completion rates, error rates, ability to explain code, and engagement when learning to program. They compared the learning material that reﬂected this theory to more traditional material that did not distinguish between the skills. The ﬁndings of their study were as follows: teaching skills incrementally resulted in an improved completion rate on practice exercises and decreased error rate and improved understanding on the post-test. The report of a 2001 ITiCSE (Innovation and Technology in Computer Science\n\n\nPage 5\nthe large number of questions, even for students with an average programming language knowledge and tracing skills, completing this test correctly was not an easy task. The underlying reason for compiling a challenging test was to highlight the diﬀerences in individual skills. As a result of such a test design, there were students who did not answer all the questions either in the tracing part or the diﬃculty estimation of questions part. 4.1. Test Design The test was composed of 44 tasks or questions. Out of these, 24 were multiple-choice questions with four possible answers, while 22 questions required the students to write down the output of the program or program snippet. The test questions were categorized based on two criteria: the subject matter of the question and the subjective estimation of the task’s diﬃculty. Students ﬁlling in the test essentially had to do two things for each question: ﬁrst, to solve the question as required by the task, then, to give their subjective estimate regarding the diﬃculty of the given question. In short, the authors asked them to do the task and then to indicate how easy or diﬃcult (or medium) they found the task. 4.2. The Categories of the Questions. Apart from skills tracing, the authors were also keen to explore students’ programming knowledge; therefore, the test contained six types of tasks (i.e., six categories). The task\n\n\nPage 13\nstudents with those defined by the instructors. In addition to Questions 42 and 25 already analyzed above, several questions received similar estimations from the students of all three years of study. Question 37 received a high score from the students in their first and second year, and Q39 re ceived a high score from the students in their second and third year. Students in Year 1 and Year 2 achieved low scores with Q14. Questions 5 and 16 were those where students in Year 1 and Year 3 gave fewer correct answers. In the estimation of the difficulty of the five highest-scoring questions, the opinions voiced by students in Year 1 and the instructor are very similar (Figure 7). Estimations were also compared with a nonparametric statistical analysis, where the result of the Mann–Whitney test also supported the agreement of the estimations. Figure 6. Estimating the diﬃculty of questions by year of study. The results were tested with the chi square probe (Table 10). Table 10. Correlations between year of study and estimated diﬃculty of the questions. Easy Medium Hard Total Year 1 12 66 0 78 Year 2 5 53 0 58 Year 3 7 36 3 46 Total 24 155 3 182 The data are consistent with the values shown in the descriptive statistics: the majority of students from all three years of the study estimated the test with medium diﬃculty value. The results show a correlation between the two variables\n\n\nPage 16\nthe ﬁve lowest-scoring questions deﬁned by the students in Year 3 and the instructors. 4.6.2. Relationship between Student and Teacher Estimations Apart from the students, all questions used in this research were also evaluated by the teachers. The teachers estimated the average diﬃculty of the questions’ full scale as “medium”. The results for the full scale are derived from the average score of the answers to each question. The diﬀerence between the teachers’ and the students’ evaluation was examined in a one-sample t-test, where the teachers’ evaluation was viewed as an external standard average (Table 11).\n\n\nPage 1\nenables an individualization of tests and better accuracy of knowledge level determination. In CAT, all test participants receive a uniquely tailored set of questions. The number and the diﬃculty of the next question depend on whether the respondent’s previous answer was correct or incorrect. In order for CAT to work properly, it needs questions with suitably deﬁned levels of diﬃculty. In this work, the authors compare the results of questions’ diﬃculty determination given by experts (teachers) and students. Bachelor students of informatics in their ﬁrst, second, and third year of studies at Subotica Tech—College of Applied Sciences had to answer 44 programming questions in a test and estimate the diﬃculty for each of those questions. Analyzing the correct answers shows that the basic programming knowledge, taught in the ﬁrst year of study, evolves very slowly among senior students. The comparison of estimations on questions diﬃculty highlights that the senior students have a better understanding of basic programming tasks; thus, their estimation of diﬃculty approximates to that given by the experts. Keywords: computer adaptive testing; code tracing; basic programming skills 1. Introduction Modern technologies o ﬀer numerous possibilities for improving knowledge assessment and the process of education. In higher education, testing is one of the most commonly used methods of\n\n\nPage 18\nevaluate the diﬃculty of the task at hand. The results of the conducted research conﬁrm this assumption, and by this, the second hypothesis is also conﬁrmed. The same trend can be observed regarding hypothesis H3: the results of estimating the task diﬃculty given by the third-year students approximate the evaluation given by the teachers. Since the teachers deﬁned the level of every task by taking into account both task complexity and diﬃculty, it can be stated that objective estimation skills can be acquired with more and more years of education. This is why senior students were able to objectively evaluate the complexity and diﬃculty of the task. The results of comparing the answers on questions on a full scale can be used in the so-called computerized adaptive testing (CAT). These tests adapt to the given student’s level of ability. CAT selects questions so as to maximize the exam accuracy based on what is known about the student from previous questions [23]. For the student, this means that he or she will always be given a “tailored” test suited to their level of ability. For instance, if the student does well on a medium question (i.e., medium in diﬃculty), they will then receive a more diﬃcult next question. However, a wrong answer to a given question means the student performed poorly and will be given simpler questions. For tailoring the optimal item (subsequent\n\n\nPage 17\nAppl. Sci.2020, 10, 7044 17 of 21 Table 11. Comparing teacher and student estimation of test diﬃculty. Average of Marks by Students SD Average of Marks by Teachers One-Sample t-test t p First year 1.85 0.3 2 −3.74 0.001 Second year 1.91 0.2 −2.31 0.02 Third year 1.91 0.5 −1.27 0.2 Based on the obtained results, students in their ﬁrst and second year estimated the questions as signiﬁcantly easier than the medium diﬃculty level speciﬁed by the teachers. The third-year students’ estimation was the same as that of the teachers. The results support the H3 hypothesis, namely, there is no diﬀerence between the students’ and the teachers’ estimation in the third year of study on question diﬃculty. However, the discrepancy in the estimation of students in their ﬁrst and second year suggests that the students perceived the tasks as easier than the teachers did. 5. Discussion Three hypotheses were formulated in this research. The ﬁrst was to prove that students were perfecting their knowledge and skills during their studies. The assumption was that if a given material was taught in the ﬁrst year and subsequently used in the later years of education, that knowledge or skill would be improved. As a result, the students in the later years of study would solve the basic programming tasks more easily than their peers in the ﬁrst year. The hypothesis was also supported by the assumption that\n\n\nPage 20\n[CrossRef] 15. Lampert, B.; Pongracz, A.; Sipos, J.; Vehrer, A.; Horvath, I. MaxWhere VR-learning improves eﬀectiveness over clasiccal tools of e-learning. Acta Polytech. Hung.2018, 15, 125–147. 16. Budai, T.; Kuczmann, M. Towards a modern, integrated virtual laboratory system.Acta Polytech. Hung.2018, 15, 191–204.\n\n\nPage 1\napplied sciences Article Case Study: Students’ Code-Tracing Skills and Calibration of Questions for Computer Adaptive Tests Robert Pinter 1, *, Sanja Maravi´ cˇCisar 1 , Attila Kovari 2 , Lenke Major 3, Petar ˇCisar 4 and Jozsef Katona 5 1 Department of Informatics, Subotica Tech—College of Applied Science, 24000 Subotica, Serbia; sanjam@vts.su.ac.rs 2 Department of Natural Sciences and Environmental Protection, Institute of Engineering, University of Dunaujvaros, 2400 Dunaújváros, Hungary; kovari@uniduna.hu 3 University of Novi Sad Hungarian Language Teacher Training Faculty, 24000 Subotica, Serbia; major.lenke@magister.uns.ac.rs 4 Department of Informatics, University of Criminal Investigation and Police Studies, 11080 Belgrade-Zemun, Serbia; petar.cisar@kpu.edu.rs 5 CogInfoCom based LearnAbility Reseacrh Team, University of Dunaujvaros, 2400 Dunaújváros, Hungary; katonaj@uniduna.hu * Correspondence: pinter.robert@vts.su.ac.rs Received: 31 August 2020; Accepted: 8 October 2020; Published: 11 October 2020 /gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001 /gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046 Featured Application: Autho", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Assessment of Programming Skills**: The document discusses the evaluation of programming skills among students through a test comprising multiple-choice and coding questions.\n2. **Difficulty Estimation**: It explores how students estimate the difficulty of test questions and compares these estimations with those of instructors.\n3. **Student Progression**: The study examines the progression of programming knowledge from first to third year, focusing on how students' skills and estimations evolve.\n\n#### Goals:\n1. **Evaluate Knowledge Levels**: To assess the programming knowledge and tracing skills of students at different academic levels.\n2. **Identify Misalignments**: To identify discrepancies between student and instructor perceptions of question difficulty.\n3. **Support Adaptive Testing**: To provide insights that could enhance Computer Adaptive Testing (CAT) by tailoring questions to individual student abilities.\n\n#### Insights:\n1. **Skill Development**: The results indicate that students improve their programming skills over time, with third-year students aligning their difficulty estimations more closely with those of instructors.\n2. **Question Difficulty**: Certain questions (e.g., Q6, Q14) were identified as problematic, with students misestimating their difficulty,", "updated_at": 1765801162.8538022}, "eb97151a7441": {"context": "Page 1\nJWT (JSON Web Token) – הוא Token שמכיל בתוכו (ב-payload) מידע מזוהה על המשתמש (כגון userId, name). – הוא חתום דיגיטלית בעזרת מחרוזת סודית (JWT_SECRET), כך שאפשר לאמת שלא נשמט או שונה. מבנה ה-JWT Header: מבנה ואלגוריתם החתימה (alg: HS256, typ: JWT) Payload: אובייקט JSON עם הנתונים שלך (userId, name, ועוד אם תרצי) Signature: HMAC-SHA256 של Base64(Header) + “.” + Base64(Payload) + ה-secret למה צריך אותו? אחרי שהמשתמש הוכח (login) הוא מקבל את ה-token ומאחסן אותו בצד הלקוח (למשל ב-localStorage או cookie). בקריאות הבאות ל־API, מצרפים אותו בכותרת Authorization: Bearer <token>. בצד השרת, ב־middleware לאימות (שנלמד ב-2.6), מבצעים jwt.verify(token, JWT_SECRET) כדי: לוודא שהחתימה תקינה (לא זויפה) לחלץ את ה-payload (למשל userId) ולהניח אותו ב־req.user expiresIn: \"1h\" – קובע שהטוקן יהיה תקף לשעה אחת בלבד; אחרי זה המשתמש ייאלץ להתחבר מחדש. באופן כללי – התהליך הוא: Register → hash+salt → שמירה ב־DB Login → compare → sign JWT → החזרת token Protected Routes → middleware jwt.verify → אם תקין, ממשיכים לבקשה כך מתקבלת מערכת Authentication סטנדרטית, מאובטחת ומבוססת טוקנים ללא מצרך ב־session state בצד השרת. דומה למשתמש קיים כדי לקבל תוקן: { \"email\": \"vered@example.com\", \"password\": \"Secret123\" } its time to travel\n", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **JWT (JSON Web Token) Structure:**\n   - **Header:** Contains the signing algorithm (e.g., HS256) and token type (JWT).\n   - **Payload:** Holds user information (e.g., userId, name).\n   - **Signature:** Ensures integrity using HMAC-SHA256 with a secret key.\n\n2. **Authentication Process:**\n   - User registration and login flow.\n   - Token issuance upon successful login.\n   - Token storage on the client-side (e.g., localStorage, cookies).\n\n3. **Token Usage:**\n   - Inclusion of the token in API requests via the Authorization header.\n   - Server-side verification of the token using `jwt.verify`.\n\n4. **Token Expiration:**\n   - Tokens are set to expire (e.g., after 1 hour), requiring re-authentication.\n\n5. **Middleware Role:**\n   - Middleware for verifying tokens and extracting user information for protected routes.\n\n**Goals:**\n- To establish a secure authentication system using JWTs without relying on server-side session states.\n- To ensure that user data is transmitted securely and can be verified for authenticity.\n\n**Insights", "updated_at": 1765801170.4683797}, "681fbb2e6614": {"context": "Page 9\nWe then divided them into ﬁve sets, each containing 20 question–answer pairs, and assigned two raters to each set. The raters were crowdworkers having Test of English for International Communication (TOEIC) scores of\n\n\nPage 7\nthese models on a large dataset without difﬁculty considerations can enhance their performance in answer extraction and QG. 7) Fine-tuning was then performed on D(train) b to develop answer-extraction and QG models that consider question difﬁculty. This ﬁne-tuning employed the model parame- ters estimated in step (6) as initial values. 8) To assess the proﬁciency of the developed models in con- trolling the difﬁculty, we generated question–answer pairs with various difﬁculties and evaluated them. Speciﬁcally, we ﬁrst input each reading passage in D(eval) b and each of the 61 difﬁculty values, from −3.0 to 3.0 in increments of 0.1, into the proposed answer-extraction model and gen- erated 61 answers for each reading passage. Then, given each triplet consisting of a reading passage, difﬁculty value, and generated answer, we generated questions using the proposed QG model. The generated sets of questions and answers were subjected to both machine-based and human evaluations. We now summarize the basic statistics of the datasets D(train) b and D(eval) b , which we developed in the aforementioned pro- cedure (5) to train and evaluate our difﬁculty-controllable QG method. First, the number of reading passages in D(train) b and D(eval) b was 1860 and 207, respectively. Next, the average number of questions per reading passage in D(train) b and D(eval) b was 5.21 and 4.28.\n\n\nPage 1\nSchool of Informatics and Engi- neering, University of Electro-Communications, Chofu 182-8585, Japan (e-mail: tomikawa@ai.lab.uec.ac.jp; suzuki_ayaka@ai.lab.uec.ac.jp; uto@ai. lab.uec.ac.jp). Digital Object Identiﬁer 10.1109/TLT.2024.3491801 in various educational applications, including intelligent tutor- ing systems, writing-assistance tools, and knowledge-evaluation platforms [1], [2], [3], [4], [5], [6], [7]. Early QG methods were based on rule-based or template- based approaches, which generated questions by converting declarative texts into interrogative questions, using handcrafted rules or templates [1], [8], [9], [10], [11]. However, preparing well-designed rules and templates for speciﬁc applications is time consuming and labor-intensive [1], [12]. To address this limitation, end-to-end QG methods based on deep neural net- works have been used [1], [13], [14], [15], [16], [17], [18], [19], [20]. Initial neural QG methods were designed as sequence- to-sequence (seq2seq) models based on recurrent neural net- works (RNNs) and attention mechanisms [14], [21], whereas more recent methods have employed pretrained transformer models [2], [5], [13], [15], [22], [23], such as bidirectional encoder representations from transformers (BERT) [24], gen- erative pretrained transformer 2 (GPT-2) [25], bidirectional and autoregressive transformers (BART) [26], and text-to-text trans-\n\n\nPage 6\nfor enhancing reading comprehensive skills. However, in typical educational settings, a learner’s ability is often unknown a priori. To address this, we are proposing an efﬁcient method for estimating a learner’s ability. We utilize the CAT framework, as mentioned in Section V-B, while generating and administering questions at appropriate levels of difﬁculty. Speciﬁcally, we propose the following procedure for ability estimation and QG. 1) Randomly generate and administer a few questions to the learner to obtain initial response data. 2) Utilize the obtained response data along with the Rasch model to estimate and update the learner’s ability level. 3) Generate and administer a question at the estimated ability level using the proposed QG method, thereby obtaining new response data. 4) Repeat steps (2) and (3). Using this procedure, we can effectively estimate a learner’s ability while administering questions at appropriate levels of difﬁculty. Remember that when the ability and difﬁculty levels are equal, the Rasch model produces a 0.5 probability that a correct answer will be obtained, and Fischer information is maximized.\n\n\nPage 6\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2191 Fig. 4. Difﬁculty-controllable answer-extraction model using BERT. We can ﬁne-tune the answer-extraction model by using a collection of triplets (r,a,b), which can be obtained from the extended SQuAD dataset explained in Section VI-A. This ﬁne- tuning is performed by minimizing cross-entropy loss between the true positions of the start and end of an answer text and the predicted probabilities for these positions. C. Difﬁculty-Controllable Answer-Aware QG Model We use T5 for difﬁculty-controllable answer-aware QG, where T5 is a pretrained transformer model, the details of which are summarized in Table II. We use T5 because it has been widely used before in various text generation tasks [54], [55], [56], [57] including QG tasks [2], [58], [59] and has achieved higher accuracy in QG compared to models, such as BART and GPT-2 [58]. Conventional answer-aware QG models [60] based on pre- trained language models are implemented by designing the model’s input as r1,..., [A],a1,...,a K,[A],...,r I (3) where [A] is a special token representing an answer’s start and end positions within a reading passage. The model’s target, which is a question text, is designed as [G],q1,...,q J ,[E] (4) where [G] and [E] are also special tokens representing the beginning of a question text and the end of a question text,\n\n\nPage 3\nneural QG method for reading comprehen- sion is an RNN-based seq2seq model [14]. In this method, a reading passage and an answer are fed into an RNN encoder, and the output feature vector is given to an RNN decoder to generate a question text. In addition, Zhou et al. [32] proposed using an RNN-based QG model that can consider both the words’ sequence and their part-of-speech (POS) tags. In recent years, pretrained transformer-based models, which have outperformed RNN-based seq2seq models on many natural language processing tasks (e.g., [25], [27], [33], [34], [35], [36], [37], [38], [39], and [40]), have been used for automated QG tasks (e.g., [1], [13], [14], [41], [42], and [43]). Some examples include a QG method proposed by Chan and Fan [13] that uses BERT and a method proposed by Lee and Lee [23] that uses T5. B. Difﬁculty-Controllable Neural QG for Reading Comprehension When utilizing QG methods as a learning aid to foster reading comprehension skills, it is critical to be able to generate ques- tions with arbitrary difﬁculty levels [44]. Accordingly, several recent studies have proposed difﬁculty-controllable QG meth- ods [22], [28]. For example, Gao et al. [22] proposed an RNN-based seq2seq model that generates reading comprehension questions for dif- ﬁculty levels categorized as either “easy” or “hard.” They also proposed to construct training data for the\n\n\nPage 2\nmethod, we ﬁrst propose a method to create a train- ing dataset composed of quadruplets (reading passage, question text, answer text, and IRT-based difﬁculty) by extending the SQuAD [30] dataset, the most utilized benchmark dataset for the reading comprehension QG task. Subsequently, we propose a difﬁculty-controllable generation method for question–answer pairs that can be trained using this extended dataset. Our genera- tion method consists of two pretrained transformer-based mod- els that are modiﬁed to incorporate IRT-based difﬁculty values as inputs: a difﬁculty-controllable answer-extraction model using BERT, and a difﬁculty-controllable answer-aware QG model using T5. Moreover, to generate questions with appropriate difﬁculty levels tailored to each learner, we need to know the learner’s ability in advance, a requirement that is not satisﬁed in prac- tice. To overcome this limitation, we propose leveraging the framework of computerized adaptive testing (CAT) [31], a well- known test administration method. This method repeats a cycle of sequentially presenting questions of a difﬁculty level suited to a learner’s ability and estimating their ability from their responses. We expect this method to enable efﬁcient ability esti- mation while tuning the difﬁculty levels of provided questions. To our knowledge, this is the ﬁrst QG method that enables difﬁculty control aimed at\n\n\nPage 12\nlanguage understand- ing,” in Proc. Neural Inf. Process. Syst., 2019. [35] P. He, X. Liu, J. Gao, and W. Chen, “DeBERTa: Decoding-enhanced BERT with disentangled attention,” in Proc. Int. Conf. Learn. Representations, 2021. [36] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence embeddings using Siamese BERT-networks,” in Proc. Conf Empirical Methods Nat- ural Lang. Process./9th Int. Joint Conf Natural Lang. Process., 2019, pp. 3982–3992. [37] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher, “CTRL: A conditional transformer language model for controllable gen- eration,” 2019, arXiv:1909.05858. [38] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT: A lite BERT for self-supervised learning of language repre- sentations,” in Proc. Int. Conf. Learn. Represent., 2020. [39] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a dis- tilled version of BERT: Smaller, faster, cheaper and lighter,” 2019, arXiv:1910.01108. [40] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” in Proc. Int. Conf. Learn. Represent., 2020. [41] S. Subramanian, T. Wang, X. Yuan, S. Zhang, A. Trischler, and Y . Bengio, “Neural models for key phrase extraction and question generation,” in Proc. Workshop Mach. Reading Question Answering, 2018, pp. 78–88. [42] Y . Kim, H. Lee, J.\n\n\nPage 12\nEng., 2014, vol. 282, pp. 325–338. [5] M. Rathod, T. Tu, and K. Stasaski, “Educational multi-question generation for reading comprehension,” in Proc. 17th Workshop Innov. Use NLP Build. Educ. Appl., 2022, pp. 216–223. [6] M. Liu and R. A. Calvo, “Using information extraction to generate trigger questions for academic writing support,” in Proc. Int. Conf. Intell. Tutoring Syst., 2012, pp. 358–367. [7] M. Liu, R. A. Calvo, and V . Rus, “G-Asks: An intelligent automatic ques- tion generation system for academic writing support,”Dialogue Discourse, vol. 3, no. 2, pp. 101–124, Mar. 2012. [8] J. Mostow and W. Chen, “Generating instruction automatically for the reading strategy of self-questioning,” in Proc. Int. Conf. Artif. Intell. Educ., 2009, pp. 456–472. [9] H. Kunichika, T. Katayama, T. Hirashima, and A. Takeuchi, “Automated question generation methods for intelligent English learning systems and its evaluation,” in Proc. Int. Conf. Consum. Electron., 2004. [10] Y . Huang and L. He, “Automatic generation of short answer questions for reading comprehension assessment,” Natural Lang. Eng., vol. 22, no. 3, pp. 457–489, May 2016. [11] M. Heilman and N. A. Smith, “Good question! Statistical ranking for question generation,” in Proc. Annu. Conf. North Amer . Chapter Assoc. Comput. Linguistics, 2010, pp. 609–617. [12] F. Chen, J. Xie, Y . Cai, T. Wang, and Q. Li,\n\n\nPage 12\nfor automatic question gen- eration,” inProc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process., 2019, pp. 3314–3323. [20] X. Ma, Q. Zhu, Y . Zhou, and X. Li, “Improving question generation with sentence-level semantic matching and answer position inferring,” in Proc. AAAI conf. Artif. Intell., 2020, vol. 34, pp. 8464–8471. [21] L. Song, Z. Wang, and W. Hamza, “A uniﬁed query-based gener- ative model for question generation and question answering,” 2017, arXiv:1709.01058. [22] Y . Gao, L. Bing, W. Chen, M. Lyu, and I. King, “Difﬁculty controllable generation of reading comprehension questions,” in Proc. 28th Int. Joint Conf. Artif. Intell., 2019, pp. 4968–4974. [23] S. Lee and M. Lee, “Type-dependent prompt CycleQAG: Cycle consis- tency for mult", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Question Generation (QG) Methods**: The document discusses various approaches to QG, including traditional rule-based methods and modern deep learning techniques, particularly those utilizing transformer models like BERT and T5.\n2. **Difficulty Control in QG**: A significant focus is on generating questions that can be tailored to different difficulty levels, which is crucial for effective learning and assessment.\n3. **Adaptive Learning Framework**: The use of Computerized Adaptive Testing (CAT) to estimate learner ability and adjust question difficulty accordingly is explored.\n\n#### Goals:\n1. **Enhance QG Models**: The aim is to improve QG models to produce questions that are not only relevant but also appropriate for the learner's current ability level.\n2. **Establish a Difficulty-Controllable QG System**: The goal is to create a system that can generate question-answer pairs with varying levels of difficulty based on the learner's responses.\n3. **Facilitate Reading Comprehension**: By generating questions that match a learner's ability, the system seeks to enhance reading comprehension skills effectively.\n\n#### Insights:\n1. **Importance of Difficulty Levels**: The ability", "updated_at": 1765802474.8701103}, "0e62b369b6d4": {"context": "Page 8\nAppl. Sci.2020, 10, 7044 8 of 21 Table 2. Average of points. Year of Study Average Point Standard Deviation (SD) 1st 25.73 6.1 2nd 21.21 7.2 3rd 17.39 6.0 The authors also examined the results achieved by the students on a question-by-question basis. The percentage of correct answers for each question is illustrated in Figure 1. The questioned revealed for which items the students’ correct answer reached a level of 51% (the lower limit of the points to “just pass”). Those questions where the value did not surpass the 51% limit are marked (e.g., Q2, Q6, Q11, etc.) Appl. Sci. 2020, 10, x FOR PEER REVIEW 8 of 21 Table 2. Average of points. Year of Study Average Point Standard Deviation (SD) 1st 25.73 6.1 2nd 21.21 7.2 3rd 17.39 6.0 The authors also examined the results achieved by the students on a question-by-question basis. The percentage of correct answers for each questi on is illustrated in Figure 1. The questioned revealed for which items the students’ correct answer reached a level of 51% (the lower limit of the points to “just pass”). Those qu estions where the value did not su rpass the 51% limit are marked (e.g., Q2, Q6, Q11, etc.) Figure 1. Percentage of correct answers for each question. Figure 1. Percentage of correct answers for each question.\n\n\nPage 6\nreturn 0; } void swap(int& x, int& y) { x+=2; y+=3;\n\n\nPage 20\n[CrossRef] 15. Lampert, B.; Pongracz, A.; Sipos, J.; Vehrer, A.; Horvath, I. MaxWhere VR-learning improves eﬀectiveness over clasiccal tools of e-learning. Acta Polytech. Hung.2018, 15, 125–147. 16. Budai, T.; Kuczmann, M. Towards a modern, integrated virtual laboratory system.Acta Polytech. Hung.2018, 15, 191–204.\n\n\nPage 12\nlowest-score (Q25) question. When estimating the degree of di fficulty of the full scale (the whole test), the majority of students identified it as being of medium difficulty (Table 9). Table 9. Estimating the degree of difficulty for the whole test. n % easy 30 16 medium 147 81 hard 5 3 Total 182 100 Figure 5. Estimation of diﬃculty for the lowest-score (Q25) question. When estimating the degree of diﬃculty of the full scale (the whole test), the majority of students identiﬁed it as being of medium diﬃculty (Table 9).\n\n\nPage 16\nthe instructor were the same for Questions 25 and 26, as both parties saw their difficulty as hard. Question 6 was considered hard by the students, as opposed to easy by the teacher. Question 7 was considered easy by the majority of students and of medium difficulty by the teacher. Question 16 was estimated as hard by the students and of medium difficulty by the teacher. In all cases, the Mann–Whitney test performed with these result s showed no relevant difference between the estimation of the students from the third year and the teacher. This result also supports hypothesis H3 for the students of Year 3. Figure 12. Comparing estimation difficulty of the five  lowest-scoring questions defined by the students in Year 3 and the instructors. 4.6.2. Relationship between Student and Teacher Estimations Apart from the students, all questions used in this research were also evaluated by the teachers. The teachers estimated the average difficulty of the questions’ full scale as “medium”. The results for the full scale are derived from the average score of the answers to each question. The difference between the teachers’ and the students’ evaluation was examined in a one-sample t-test, where the teachers’ evaluation was viewed as an external standard average (Table 11). Table 11. Comparing teacher and student estimation of test difficulty. Figure 12. Comparing estimation diﬃculty of\n\n\nPage 5\nthe large number of questions, even for students with an average programming language knowledge and tracing skills, completing this test correctly was not an easy task. The underlying reason for compiling a challenging test was to highlight the diﬀerences in individual skills. As a result of such a test design, there were students who did not answer all the questions either in the tracing part or the diﬃculty estimation of questions part. 4.1. Test Design The test was composed of 44 tasks or questions. Out of these, 24 were multiple-choice questions with four possible answers, while 22 questions required the students to write down the output of the program or program snippet. The test questions were categorized based on two criteria: the subject matter of the question and the subjective estimation of the task’s diﬃculty. Students ﬁlling in the test essentially had to do two things for each question: ﬁrst, to solve the question as required by the task, then, to give their subjective estimate regarding the diﬃculty of the given question. In short, the authors asked them to do the task and then to indicate how easy or diﬃcult (or medium) they found the task. 4.2. The Categories of the Questions. Apart from skills tracing, the authors were also keen to explore students’ programming knowledge; therefore, the test contained six types of tasks (i.e., six categories). The task\n\n\nPage 3\nAppl. Sci.2020, 10, 7044 3 of 21 • By tracing, developers can identify bottlenecks and focus on improving performance. • When a problem does occur, tracing allows the user to see how it came to be: which function, duration of a function, which parameters passed, and how deep into the function the user could delve. 2. Related Works Problems that arise in novices when learning programming is a ﬁeld that has been the subject of numerous studies, as have the ways of adopting those new concepts [10]. Xie et al. [11] proposed a theory that identiﬁed four distinct skills that novices learned incrementally. These skills were tracing, writing syntax, comprehending templates, and writing code with templates. They assumed that the explicit instruction of these skills decreased cognitive demand. The authors conducted an exploratory mixed-methods study and compared students’ exercise completion rates, error rates, ability to explain code, and engagement when learning to program. They compared the learning material that reﬂected this theory to more traditional material that did not distinguish between the skills. The ﬁndings of their study were as follows: teaching skills incrementally resulted in an improved completion rate on practice exercises and decreased error rate and improved understanding on the post-test. The report of a 2001 ITiCSE (Innovation and Technology in Computer Science\n\n\nPage 12\nAppl. Sci.2020, 10, 7044 12 of 21 and Year 2. Without exception, all students from Year 1 estimated its diﬃculty as easy, only 12% of students from Year 2, and 10% from Year 3 estimated the question’s diﬃculty as medium diﬃcult (Figure 4). Appl. Sci. 2020, 10, x FOR PEER REVIEW 12 of 21 Figure 4. Estimation of difficulty for the highest-score (Q42) question. As for the question with the lowest score (Q25),  the students’ estimations were quite different. The majority of students in Year 1 found the qu estion moderately difficult (medium), while the majority of students in Years 2 and 3 found its difficulty hard (Figure 5). The same trend can be observed with the points achieved on these two qu estions: the Year 1 students received the highest amount of points, while the Year 2 and Year 3 students received the lowest points. At the same time, the Year 2 and Year 3 students also felt that it was more difficult for them to solve the task, which supports hypothesis H2. Figure 5. Estimation of difficulty for the lowest-score (Q25) question. When estimating the degree of di fficulty of the full scale (the whole test), the majority of students identified it as being of medium difficulty (Table 9). Table 9. Estimating the degree of difficulty for the whole test. n % easy 30 16 medium 147 81 hard 5 3 Total 182 100 Figure 4. Estimation of diﬃculty for the highest-score (Q42) question.\n\n\nPage 16\neach question. The difference between the teachers’ and the students’ evaluation was examined in a one-sample t-test, where the teachers’ evaluation was viewed as an external standard average (Table 11). Table 11. Comparing teacher and student estimation of test difficulty. Figure 11. Comparing estimation diﬃculty of the ﬁve highest-scoring questions deﬁned by the students in Year 3 and the instructors. In the case of the questions with the lowest score (Figure 12), the estimations of the students and the instructor were the same for Questions 25 and 26, as both parties saw their diﬃculty as hard. Question 6 was considered hard by the students, as opposed to easy by the teacher. Question 7 was considered easy by the majority of students and of medium diﬃculty by the teacher. Question 16 was estimated as hard by the students and of medium di ﬃculty by the teacher. In all cases, the Mann–Whitney test performed with these results showed no relevant diﬀerence between the estimation of the students from the third year and the teacher. This result also supports hypothesis H3 for the students of Year 3. Appl. Sci. 2020, 10, x FOR PEER REVIEW 16 of 21 Figure 11. Comparing estimation difficulty of the five  highest-scoring questions defined by the students in Year 3 and the instructors. In the case of the questions with the lowest score (Figure 12), the estimations of the students and\n\n\nPage 7\nSecond year 58 31.9 Third year 46 25.3 Total 182 100.0 Two professors from Subotica Tech, with more than two decades of experience in teaching computer science courses, have designed the test and conducted the data acquisition. 4.5. Comparison of the Performance in Each Year of Study C o m p a r i n g  t h e  t e s t  r e s u l t s  f o r  e a c h  y e a r  o f  s t u d y ,  t h e  s t u d e n t s  f r o m  t h e  f i r s t  y e a r  g a v e  t h e largest number of correct answers, averaging 25.73 out of 44. The students in Year 2 had an average of 21.21 points, while the weakest result came from the third-year students, who only achieved a point average of 17.39 (see Table 2.). 4.3. Estimation of the Question Diﬃculty Students’ subjective estimation of question diﬃculty is very useful feedback. Based on the students’ answers, teachers gain information about which part of the study material the students understand the least. Furthermore, the teachers can rethink and revise their own estimation of the questions’ diﬃculty. For example, a given question thought to be ‘easy’ by the teacher may be perceived as an unsolvable problem by the students themselves. This feedback can be used by teachers when preparing questions for the exam in order to only include questions with approximately the same di ﬃculty. As mentioned above, a subjective estimation of question di ﬃculty can also\n\n\nPage 4\ngames [20], which could support learning eﬃciency and help complement learning diﬃculties as well. 3. Research The authors of this paper conducted a study among all three student cohorts of bachelor Informatics students at Subotica Tech—College of Applied Sciences. The research included 182 student participants who were taking an introductory course called ‘Algorithms and data structures’. The course is composed of lectures, practices, and laboratory practices. The course runs in the spring semester over 15 weeks in the ﬁrst year of study. It consists of one 90 min lecture per week as well as a 45 min practice and another 90 min lab practice. The aim of the course is to introduce the students to the basic concepts of algorithms and data structures in C/C++ programming language. The course covers the basic principles of programming, variables, control statements ( if, while, do-while, breakand continue statements), arrays, functions, pointers, and some basic algorithms (sort and search). Based on historical data, the grade average in this given course is 7.02 (5 is the minimum grade and means fail, and the maximum grade is 10), while the pass rate is about 50%. During their studies, informatics students learn new developing methods and developing environments as well as new programming languages. The novel techniques help ", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Student Performance Analysis:**\n   - Average scores of students across three years of study (1st year: 25.73, 2nd year: 21.21, 3rd year: 17.39).\n   - Evaluation of student responses on a question-by-question basis, highlighting questions with less than 51% correct answers.\n\n2. **Test Design and Difficulty:**\n   - The test consisted of 44 questions, including multiple-choice and coding output tasks.\n   - Students provided subjective difficulty estimates for each question, which were compared with teacher evaluations.\n\n3. **Comparison of Student and Teacher Evaluations:**\n   - The study examined discrepancies between student and teacher perceptions of question difficulty.\n   - Statistical tests (Mann–Whitney and t-tests) were used to analyze differences in difficulty estimations.\n\n4. **Cognitive Load and Learning Outcomes:**\n   - The research explored how incremental skill acquisition in programming (tracing, syntax writing, etc.) impacts student performance and understanding.\n\n**Goals:**\n- To assess the effectiveness of a programming course (Algorithms and Data Structures) for Informatics students.\n- To identify areas", "updated_at": 1765802481.8496552}, "83185162238f": {"context": "Page 1\nJWT (JSON Web Token) – הוא Token שמכיל בתוכו (ב-payload) מידע מזוהה על המשתמש (כגון userId, name). – הוא חתום דיגיטלית בעזרת מחרוזת סודית (JWT_SECRET), כך שאפשר לאמת שלא נשמט או שונה. מבנה ה-JWT Header: מבנה ואלגוריתם החתימה (alg: HS256, typ: JWT) Payload: אובייקט JSON עם הנתונים שלך (userId, name, ועוד אם תרצי) Signature: HMAC-SHA256 של Base64(Header) + “.” + Base64(Payload) + ה-secret למה צריך אותו? אחרי שהמשתמש הוכח (login) הוא מקבל את ה-token ומאחסן אותו בצד הלקוח (למשל ב-localStorage או cookie). בקריאות הבאות ל־API, מצרפים אותו בכותרת Authorization: Bearer <token>. בצד השרת, ב־middleware לאימות (שנלמד ב-2.6), מבצעים jwt.verify(token, JWT_SECRET) כדי: לוודא שהחתימה תקינה (לא זויפה) לחלץ את ה-payload (למשל userId) ולהניח אותו ב־req.user expiresIn: \"1h\" – קובע שהטוקן יהיה תקף לשעה אחת בלבד; אחרי זה המשתמש ייאלץ להתחבר מחדש. באופן כללי – התהליך הוא: Register → hash+salt → שמירה ב־DB Login → compare → sign JWT → החזרת token Protected Routes → middleware jwt.verify → אם תקין, ממשיכים לבקשה כך מתקבלת מערכת Authentication סטנדרטית, מאובטחת ומבוססת טוקנים ללא מצרך ב־session state בצד השרת. דומה למשתמש קיים כדי לקבל תוקן: { \"email\": \"vered@example.com\", \"password\": \"Secret123\" } its time to travel\n", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **JSON Web Token (JWT)**: Definition and structure, including Header, Payload, and Signature.\n2. **Token Generation and Validation**: Process of creating and verifying JWTs.\n3. **Authentication Flow**: Steps involved in user registration, login, and token management.\n4. **Token Expiration**: Mechanism for token validity and renewal.\n\n**Goals:**\n- To provide a secure method for user authentication without relying on server-side session state.\n- To ensure data integrity and authenticity through digital signatures.\n- To facilitate seamless API access using tokens.\n\n**Insights:**\n- JWTs are signed using a secret key, allowing the server to verify the token's authenticity.\n- Tokens are stored client-side (e.g., localStorage or cookies) and sent with API requests for authorization.\n- The authentication process involves hashing passwords, signing tokens, and implementing middleware for token verification.\n- The system is designed to be secure and efficient, with a defined expiration time for tokens to enhance security.", "updated_at": 1765802486.681208}}