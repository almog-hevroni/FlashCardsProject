{"320849bc3626": {"context": "Page 6\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2191 Fig. 4. Difﬁculty-controllable answer-extraction model using BERT. We can ﬁne-tune the answer-extraction model by using a collection of triplets (r, a, b), which can be obtained from the extended SQuAD dataset explained in Section VI-A. This ﬁne- tuning is performed by minimizing cross-entropy loss between the true positions of the start and end of an answer text and the predicted probabilities for these positions. C. Difﬁculty-Controllable Answer-Aware QG Model We use T5 for difﬁculty-controllable answer-aware QG, where T5 is a pretrained transformer model, the details of which are summarized in Table II. We use T5 because it has been widely used before in various text generation tasks [54], [55], [56], [57] including QG tasks [2], [58], [59] and has achieved higher accuracy in QG compared to models, such as BART and GPT-2 [58]. Conventional answer-aware QG models [60] based on pre- trained language models are implemented by designing the model’s input as r1, . . . , [A], a1, . . . , aK, [A], . . . , rI (3) where [A] is a special token representing an answer’s start and end positions within a reading passage. The model’s target, which is a question text, is designed as [G], q1, . . . , qJ, [E] (4) where [G] and [E] are also special tokens representing the beginning of a question text and the end\n\n\nPage 5\nBERT output vector for the ith word in a passage r, we add two dense layers, denoted as f (s)(vi) and f (e)(vi), for each vi, where each dense layer transforms vi into a scalar value . The dense-layer outputs are then transformed through softmax acti- vations as P (s) = softmax(f (s)(v1), . . . , f (s)(vI)) and P (e) = softmax(f (e)(v1), . . . , f (e)(vI)), whose ith elements P (s) i and P (e) i represent the probability values for whether the ith word is at the start and end positions, respectively. Thus, by extracting the word sequence within the start and end positions, which take the maximum probabilities, we can extract an answer text from a given reading passage. We control the difﬁculty of the answer extraction by inputting a difﬁculty value with the reading passage. Speciﬁcally, the input for our model is deﬁned as b, [SEP], r1, r2, r3, . . . , rI (2) where [SEP] is the special token used to separate the difﬁculty value and the reading passage. This conﬁguration allows the modeltoconsiderthedifﬁcultyvaluewhenextractingtheanswer text from the reading passage. Fig. 4 shows an outline of the answer-extraction model. 3No changes were made to original BERT and T5 models in terms of architecture and training parameters.\n\n\nPage 10\ntable, more than 90% of the questions were rated as either ﬂuent or acceptable, and approximately 90% appropriately reﬂected the content of the reading passage. Moreover, in over 60% of cases, question and answer pairs were generated that were answerable, and nearly 90% were partially appropriate when accepting excessive or insufﬁcient results. These ﬁndings were conﬁrmed by the practicality ratings, which also found that nearly 90% of the questions were evaluated positively. 6The crowdworkers were recruited via the crowdsourcing platform Crowd- Works (crowdworks.jp). The only requirement was that they have a TOEIC score of 900 or above. No additional information about the raters’ attributes was collected. A TOEIC score of 900, which corresponds to an effective operational proﬁciency in the English language, can be compared with other international English scoring systems as follows: TOEFL > 79, International English Language Testing System (IELTS) > 6.5, Cambridge Exam ≥First Certiﬁcate in English (FCE), and Common European Framework of Reference for Languages (CEFR) ≥B2. Next, we calculated the Spearman rank-order correlation co- efﬁcient between the human evaluations of difﬁculty and the difﬁculty levels speciﬁed for QG, obtaining a low correlation value of 0.15. To assess the reason for this outcome, we de- termined the agreement ratio between the two raters for each\n\n\nPage 6\nof a question text, respectively. To implement difﬁculty-control for the answer-aware QG model, we concatenate a target difﬁculty value to the aforemen- tioned conventional input form using b, [Q], r1, . . . , [A], a1, . . . , aK, [A], . . . , rI (5) where [Q] is the special token used to separate the difﬁculty value and the given reading passage. Given this input, the model generates a question text based on a reading passage, an answer, and a target difﬁculty value. Fig. 5 presents an outline of our QG model. We can ﬁne-tune the answer-aware QG model by using a dataset consisting of quadruplets (r, q, a, b), explained in Fig. 5. Difﬁculty-controllable answer-aware QG model using T5. Section VI-A. Speciﬁcally, we prepare the input data following (5) format and output data following (4) format, and train T5 by maximizing the log-likelihood for target data. D. Estimating the Learner’s Ability and Adaptively Generating Questions A previous study on adaptive learning [44] reported that posing questions that learners can answer correctly with a 50% probability is pedagogically effective. As discussed in Sec- tion V-A, in the Rasch model, the probability of obtaining a correct answer is 0.5 when the ability and difﬁculty levels are equal. Therefore, if a learner’s ability level is known, using the proposed method to generate a question whose difﬁculty level is equal to or near the\n\n\nPage 9\n2194 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 Fig. 12. Logistic regression curves for the three different difﬁculties shown in Figs. 9–11. Fig. 13. Average word count of answers by difﬁculty level. that specifying higher difﬁculty values makes the generated questions more difﬁcult across a wide range of ability levels. Next, the average word count of the answers for each difﬁculty level is displayed in Fig. 13. The x-axis represents the speciﬁed levels of difﬁculty, while the y-axis represents the average word count of the answers. The ﬁgure shows an increasing trend in the average word count for higher levels of difﬁculty. Since longer answers generally make the question difﬁcult, the proposed method could extract answers while reﬂecting the difﬁculty levels. Table III shows examples of the generated questions and answers for different difﬁculty levels from the same reading pas- sage. It shows that giving a lower difﬁculty value induces short answer text, whereas giving a higher difﬁculty value induces longer and more complex answers. Finally, we investigated how the input difﬁculty affects the types of generated questions. To do so, we examined the pro- portion of leading interrogative words, “What,” “Who,” “When,” “Why,” “Where,” “Which,” and “How” in the generated ques- tions. The results are shown in Fig. 14. The x-axis represents the speciﬁed levels of\n\n\nPage 7\nlarge dataset without difﬁculty considerations can enhance their performance in answer extraction and QG. 7) Fine-tuning was then performed on D(train) b to develop answer-extraction and QG models that consider question difﬁculty. This ﬁne-tuning employed the model parame- ters estimated in step (6) as initial values. 8) To assess the proﬁciency of the developed models in con- trolling the difﬁculty, we generated question–answer pairs with various difﬁculties and evaluated them. Speciﬁcally, we ﬁrst input each reading passage in D(eval) b and each of the 61 difﬁculty values, from −3.0 to 3.0 in increments of 0.1, into the proposed answer-extraction model and gen- erated 61 answers for each reading passage. Then, given each triplet consisting of a reading passage, difﬁculty value, and generated answer, we generated questions using the proposed QG model. The generated sets of questions and answers were subjected to both machine-based and human evaluations. We now summarize the basic statistics of the datasets D(train) b and D(eval) b , which we developed in the aforementioned pro- cedure (5) to train and evaluate our difﬁculty-controllable QG method. First, the number of reading passages in D(train) b and D(eval) b was 1860and207, respectively. Next, theaveragenumber of questions per reading passage in D(train) b and D(eval) b was 5.21 and 4.28. Furthermore, Fig. 7 is a\n\n\nPage 12\nseparation,” in Proc. AAAI Conf. Artif. Intell., 2019, pp. 6602–6609. [43] X. Sun, J. Liu, Y. Lyu, W. He, Y. Ma, and S. Wang, “Answer-focused and position-aware neural question generation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2018, pp. 3930–3939. [44] M. Ueno and Y. Miyazawa, “IRT-based adaptive hints to scaffold learning inprogramming,”IEEETrans.Learn.Technol.,vol.11,no.4,pp. 415–428, Oct.–Dec. 2018.\n\n\nPage 7\nanswer. 3) Utilizing the collected binary response data, we employed theRaschmodel,expressedin(1),toestimatethedifﬁculty level for each question in D(eval) and the ability for each of the QA systems. 4The rationale behind our use of 60 QA systems is based on a previous report [61], which stated that the minimum requirement of respondents for estimating difﬁculty based on the Rasch model is 30. However, a larger number of respondents generally contributes to improving the stability of difﬁculty estimation. Thus, considering the tradeoff between computational cost and estimation stability, this experiment used 60 QA systems. 5[Online]. Available: https://huggingface.co/ Fig. 7. Number of questions corresponding to the difﬁculty values in D(train) b and D(eval) b . 4) We then integrated the estimated question difﬁculty with D(eval) to produce a new dataset, Db. The real-valued difﬁ- culty estimates were rounded to the second decimal place to facilitate processing by the language models BERT and T5. 5) The created dataset, Db, was further divided into two partitions that were 90% and 10%, denoted as D(train) b and D(eval) b , respectively. 6) We ﬁrst ﬁne-tuned both the answer-extraction model and the QG model using the original SQuAD training dataset, D(train), without consideringthedifﬁcultylevels. Although this step is not mandatory, we expect that pretraining these models on a\n\n\nPage 10\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2195 TABLE IV HUMAN EVALUATION 900 or higher.6 The raters were asked to assess the assigned question–answer pairs based on the following evaluation crite- ria. The raters were crowdworkers having a TOEIC score of 900 or higher . 1) Fluency: Evaluate the grammatical correctness and ﬂu- ency of the questions. Ratings were done on a three-point scale: 3. appropriate, 2. acceptable, and 1. inappropriate. 2) Content Relevance: Assess whether the generated ques- tions were relevant to the reading passage. Ratings were done on a two-point scale: 2. appropriate and 1. inappro- priate. 3) Answerability: Evaluate whether the extracted answer was indeed the correct answer for the generated question. Ratings were done on a nominal scale with four categories: a. appropriate, b. insufﬁcient, c. excessive, and d. inappro- priate. 4) Practicality: Assess whether the question and/or answer could become feasible with slight modiﬁcations. Ratings were done on a three-point scale: 3. already feasible, 2. need minor correction, and 1. not feasible. 5) Question Difﬁculty: Evaluate the difﬁculty level of the generated question. Ratings were done on a ﬁve-point scale, ranging from 1 (easiest) to 5 (most difﬁcult). Results for ﬂuency, content relevance, answerability, and practicality are presented in Table IV. From this\n\n\nPage 10\nevaluation criterion. The results showed that the agreement rates for ﬂuency, content relevance, answerability, and practicality were 0.63, 0.85, 0.53, and 0.60, respectively, while that for difﬁcultywas 0.22. Fromtheseresults, weconcludethat it is hard to ensure reliable subjective evaluations for question difﬁculty, while the evaluations for other criteria seem acceptable. Thus, to achieve a more reliable evaluation of question difﬁ- culty, we conducted anot", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Adaptive Question-Answer Generation (QG):** The document discusses a methodology for generating questions and answers that adapt to different difficulty levels using models like BERT and T5.\n2. **Difficulty Control:** The focus is on controlling the difficulty of the generated questions and answers based on learner ability and specified difficulty values.\n3. **Model Architecture:** The architecture of the answer-extraction model and the QG model is outlined, detailing how inputs are structured to incorporate difficulty levels.\n\n**Goals:**\n1. **Fine-tuning Models:** The aim is to fine-tune the answer-extraction and QG models using datasets that include difficulty levels, enhancing their performance in generating appropriate questions and answers.\n2. **Evaluating Effectiveness:** The document aims to evaluate the effectiveness of the generated question-answer pairs in terms of fluency, content relevance, answerability, practicality, and difficulty.\n3. **Adaptive Learning:** The goal is to develop a system that can adaptively generate questions that match the learner's ability, thereby optimizing the learning experience.\n\n**Insights:**\n1. **Model Performance:** The T5 model is noted for its superior performance in Q", "updated_at": 1765699328.369592}, "ca12a935c93b": {"context": "Page 1\nTABLE IV Human Evaluation 900 or higher.6 The raters were asked to assess the assigned question–answer pairs based on the following evaluation crite- ria. The raters were crowdworkers having a TOEIC score of 900 or higher . Fluency: Evaluate the grammatical correctness and flu- ency of the questions. Ratings were done on a three-point scale: 3. appropriate, 2. acceptable, and 1. inappropriate. Content Relevance: Assess whether the generated ques- tions were relevant to the reading passage. Ratings were done on a two-point scale: 2. appropriate and 1. inappro- priate. Answerability: Evaluate whether the extracted answer was indeed the correct answer for the generated question. Ratings were done on a nominal scale with four categories: a. appropriate, b. insufficient, c. excessive, and d. inappro- priate. Practicality: Assess whether the question and/or answer could become feasible with slight modifications. Ratings were done on a three-point scale: 3. already feasible, 2. need minor correction, and 1. not feasible. Question Difficulty: Evaluate the difficulty level of the generated question. Ratings were done on a five-point scale, ranging from 1 (easiest) to 5 (most difficult).\n\n\nPage 1\nThe obtained answers were graded by one of the authors to remove the effects of superficial fluctuation in human answers. Based on the correct and incorrect response data, we calculated the correct rates for each question as an index representing its difficulty. The correlation between correct rates and the specified diffi- culty levels was −0.67, where a test for noncorrelation revealed that the p-value was less than .01, confirming a statistically significant correlation at the 1% level. These results suggest that the proposed method can generate questions in alignment with the difficulty levels perceived by human respondents. Evaluation of the Accuracy of Learners’ Ability Estimation In this section, we evaluate the efficacy of the method of adaptive QG proposed in Section V-B. We simulated the adap- tive QG process for three respondents with different abili- ties. For the respondents, we used three QA models with the minimum and maximum θ, specifically, −3.658 and 2.766, as well as that which was closest to the average, θ = 1.244. Using the three QA models, we examined the following two QG processes and analyzed the trajectory of their estimated abilities. Adaptive Generation (Proposed): Initially, ten questions with random difficulty are generated and administered. Then, 40 questions are administered, following the adap- tive procedures detailed in Section VII-B, while\n\n\nPage 1\nEarly QG methods were based on rule-based or template- based approaches, which generated questions by converting declarative texts into interrogative questions, using handcrafted rules or templates [1], [8], [9], [10], [11]. However, preparing well-designed rules and templates for specific applications is time consuming and labor-intensive [1], [12]. To address this limitation, end-to-end QG methods based on deep neural net- works have been used [1], [13], [14], [15], [16], [17], [18], [19], [20]. Initial neural QG methods were designed as sequence- to-sequence (seq2seq) models based on recurrent neural net- works (RNNs) and attention mechanisms [14], [21], whereas more recent methods have employed pretrained transformer models [2], [5], [13], [15], [22], [23], such as bidirectional encoder representations from transformers (BERT) [24], gen- erative pretrained transformer 2 (GPT-2) [25], bidirectional and autoregressive transformers (BART) [26], and text-to-text trans- fer transformer (T5) [27]. These approaches have successfully generated fluent questions that are relevant to the given reading passages.\n\n\nPage 1\nreal data, we confirmed that the proposed method can generate question–answer pairs with difficulty levels that align with the learners’ abilities while efficiently estimating their abilities.\nIndex Terms—Adaptive learning, adaptive testing, automated question generation (QG) for reading comprehension, deep neural networks, item response theory (IRT), natural language processing. Introduction\nUTOMATIC question generation (QG) for reading com- prehension is the task of creating questions related to given reading passages without human intervention, where read- ing passages are arbitrary natural language texts comprising a sequence of words, a sentence, a paragraph, or an entire document. The field of natural language processing has pro- duced numerous methods for QG [1], which are being utilized\n\n\nPage 1\nA notable application of QG in education is reading tutors [1], [3], [4], [5], which provide reading-comprehension questions for diverse reading materials. Offering questions helps direct learn- ers’ attention to the content and helps them identify misconcep- tions, thereby improving their reading-comprehension skills [3]. To enhance the efficiency of such learning, offering questions with difficulty levels tailored to each learner’s reading ability is beneficial. For this reason, several difficulty-controllable QG methods have recently been proposed.\nDifficulty-controllable QG for reading comprehension is a rel- atively new area of research, and thus, the literature is scant [22], [28]. One method for realizing difficulty-controllable QG is to use an RNN-based seq2seq model in which the hidden states from the encoder are adjusted to accept the difficulty levels categorized as either easy or hard [22]. Another approach is a multihop method [28], which defines the question difficulty according to the number of inference steps required to answer the question and generates questions by controlling the number of these inference steps. However, both methods face the following limitations that make it difficult to generate questions suitable for the learner’s abilities.\n\n\nPage 1\n© 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Fig. 1.  Conventional QG task: answer-aware QG. The relationship between the difficulty of the questions and the learner’s ability is ignored, making it difficult to determine the appropriate difficulty for each learner.\nThe methods are answer-aware, which means a reading passage and an answer text must be input to generate questions, as shown in Fig. 1. For this reason, they can- not generate question–answer pairs. Generating answers along with questions is essential to realize automatic assessment of learners’ responses to generated questions. Furthermore, controlling the difficulty of the generated answers is also crucial because both questions and answers generally affect the overall difficulty.\n\n\nPage 1\nUsing this procedure, we can effectively estimate a learner’s ability while administering questions at appropriate levels of difficulty. Remember that when the ability and difficulty levels are equal, the Rasch model produces a 0.5 probability that a correct answer will be obtained, and Fischer information is maximized.\nFig. 7.  Number of questions corresponding to the difficulty values in D(train) Fig. 6.  Flow of the experimental procedures. Experiments In this section, we demonstrate that our proposed method can generate questions and answers corresponding to the target IRT- based difficulty values. Experimental Procedures We conducted the following experiment using the original SQuAD data, which was originally split into D(train) and D(eval), where the sample size of D(train) was 87 599 and that of D(eval) was 10 570. Fig. 6 illustrates the flow of the following experi- mental procedures. Using the original SQuAD training data D(train), we con- structed 60 different QA systems.4\nSpecifically, 12 QA systems (BERT-base, BERT- large, RoBERTa-base, RoBERTa-large, DeBERTa-base, DeBERTa-large, DeBERTa-v3-base, DeBERTa-v3-large, ALBERT-base-v1, ALBERT-base-v2, ALBERT-large-v2, and DistilBERT-base), which are available on hugging- face,5 were trained with 600, 1200, 1800, 2400, and 3000\n\n\nPage 1\nA representative neural QG method for reading comprehen- sion is an RNN-based seq2seq model [14]. In this method, a reading passage and an answer are fed into an RNN encoder, and the output feature vector is given to an RNN decoder to generate a question text. In addition, Zhou et al. [32] proposed using an RNN-based QG model that can consider both the words’ sequence and their part-of-speech (POS) tags.\nIn recent years, pretrained transformer-based models, which have outperformed RNN-based seq2seq models on many natural language processing tasks (e.g., [25], [27], [33], [34], [35], [36], [37], [38], [39], and [40]), have been used for automated QG tasks (e.g., [1], [13], [14], [41], [42], and [43]). Some examples include a QG method proposed by Chan and Fan [13] that uses BERT and a method proposed by Lee and Lee [23] that uses T5. Difficulty-Controllable Neural QG for Reading Comprehension When utilizing QG methods as a learning aid to foster reading comprehension skills, it is critical to be able to generate ques- tions with arbitrary difficulty levels [44]. Accordingly, several recent studies have proposed difficulty-controllable QG meth- ods [22], [28].\n\n\nPage 1\nAdaptive Question–Answer Generation With Difficulty Control Using Item Response Theory and Pretrained Transformer Models Yuto Tomikawa  , Ayaka Suzuki, and Masaki Uto\n\n\nPage 1\nThe parameters of the IRT model are typically estimated in two stages: item calibration and ability estimation. Item calibra- tion involves estimating the item parameters from the response data. Specifically, marginal maximum likelihood estimation is commonly employed for item calibration [50]. Once the item parameters have been calibrated, the ability estimation phase estimates the examinee’s ability θ, commonly based on the expected a posteriori estimation [51], [52].\nComputerized Adaptive Testing (CAT) exp(θm − bn)  pnm = 1 + exp(θ − b ) (1) As explained earlier, the IRT is often used as a basis for CAT, which adaptively administers items appropriate for each exam- where bn denotes the difficulty of the nth item, and θm denotes the latent ability of the mth examinee. For a detailed explanation of this model, Fig. 3 illustrates item response curves (IRCs) for the Rasch model, which are plots of 1In this context, “latent” indicates that the corresponding factors are not directly observable but lie behind the observed data. 2“Discriminative power” means the degree to which an item can distinguish between examinees with different levels of the underlying ability.\n\n\nPage 1\nWe collected ten subjects, different from those in the previous experiment, and asked them to answer 20 of those 30 questions. The set of questions administered to each subject was different, and each question was administered an equal number of times. Given that the experiment required participants with diverse language abilities, this time we recruited crowdworkers possessing English skills equivalent to a TOEIC score of 600 or higher.\n\n\nPage 1\nAs discussed in the aforementioned sections, our first research objective is to develop a method that generates reading compre- hension questions along with their corresponding answers, based on the given reading passages and specified difficulty levels, as illustrated in Fig. 2. Our second objective is to develop an adaptive QG framework that generates questions with difficulty levels appropriate for learners while efficiently estimating their abilities each time a question is presented.\nThe detailed task definition for the first objective is as follows. Let a given reading passage be a word sequence r = {ri | i ∈ {1,...,I}}, where ri represents the ith word in the passage, and I is the passage text length. Similarly, let a question text q and an answer text a be word sequences q = {qj | j ∈ {1,...,J}} and a = {ak | k ∈ {1,...,K}}, respectively, where qj is the jth word in the question text, ak is the kth word in the answer\ntext,", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Automated Question Generation (QG):** Focuses on creating questions related to reading passages using natural language processing (NLP) techniques.\n2. **Evaluation Criteria for QG:** Discusses various criteria for assessing the quality of generated questions, including fluency, content relevance, answerability, practicality, and question difficulty.\n3. **Adaptive QG Methods:** Explores methods that adjust question difficulty based on learners' abilities, utilizing Item Response Theory (IRT) and deep learning models.\n4. **Neural QG Techniques:** Highlights the transition from rule-based methods to advanced neural network approaches, particularly using transformer models like BERT and T5.\n\n**Goals:**\n1. **Develop Effective QG Frameworks:** Create systems that generate reading comprehension questions and corresponding answers tailored to different difficulty levels.\n2. **Adaptive Learning:** Implement adaptive QG processes that can dynamically adjust the difficulty of questions based on the learner's estimated abilities.\n3. **Enhance Reading Comprehension:** Utilize QG to improve learners' engagement and understanding of reading materials by providing appropriately challenging questions.\n\n**Insights:**\n1. **Statistical Correlation:** The study", "updated_at": 1765699334.835139}, "3af775a53ec1": {"context": "Page 3\nFigure 2: Book2Dial Framework for Generating Dialogues from Textbooks: Our approach uses two models – a Student model and a Teacher model. The Student model plays the role of a student, formulating questions from a limited context (document formatting). In contrast, the Teacher model assumes the role of a teacher, providing answers and guidance by referencing the (sub-)section in the textbook. This framework can be adapted to various instantiations of the two roles with varying formatting information, such as multi-turn QA-QG models (Kim et al., 2022), Dialogue Inpainting (Dai et al., 2022), and a prompting approach utilizing role-playing LLMs. grounded dialogue datasets. 2.2 Educational Dialogue Datasets The development of educational chatbots is highly reliant on high-quality data. Yet such data is hard to obtain. Therefore, previous works such as Math- Dial (Macina et al., 2023a) collect conversational data by pairing real teachers with an LLM that simulates students. Other datasets are commonly created by roleplaying both teacher and student, such as CIMA (Stasaski et al., 2020) or by tran- scribing classroom interactions (Suresh et al., 2022; Demszky and Hill, 2023) or logging online conver- sations (Caines et al., 2020). However, all of these methods are challenging to scale, and using non- experts often leads to data quality issues (Macina et al., 2023a). Thus, in this\n\n\nPage 12\nGangwoo Kim, Sungdong Kim, Kang Min Yoo, and Jaewoo Kang. 2022. Generating information-seeking conversations from unlabeled documents. In Pro- ceedings of the 2022 Conference on Empirical Meth- ods in Natural Language Processing, pages 2362– 2378. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2023. SODA: Million-scale dialogue dis- tillation with social commonsense contextualization. In Proceedings of the 2023 Conference on Empiri- cal Methods in Natural Language Processing, pages 12930–12949, Singapore. Association for Computa- tional Linguistics. Mohammad Amin Kuhail, Nazik Alturki, Salwa Alram- lawi, and Kholood Alhejori. 2023. Interacting with educational chatbots: A systematic review. Educa- tion and Information Technologies, 28(1):973–1018. Ghader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and Salam Al-Emari. 2020. A systematic review of auto- matic question generation for educational purposes. International Journal of Artificial Intelligence in Ed- ucation, 30:121–204. Mark R. Lepper and Maria Woolverton. 2002. Chapter 7 - the wisdom of practice: Lessons learned from the study of highly effective tutors. In Joshua Aron- son, editor, Improving Academic Achievement, Educa- tional Psychology, pages 135–158. Academic Press, San Diego. I Loshchilov and F Hutter.\n\n\nPage 23\nimages such as paintings and photographs. Student: How do monocular cues like linear perspec- tive, interposition, and relative size and closeness of images contribute to our perception of depth in the con- text of the section “5-3-vision”? Teacher: Linear perspective and interposition, as binoc- ular cues, contribute to our perception of depth by pro- viding additional visual information through the use of both eyes. Linear perspective refers to the perception of depth when we see two parallel lines appearing to con- verge, while interposition refers to the partial overlap of objects. These cues, along with binocular disparity and other monocular cues, help us establish a sense of depth and understand the spatial relationships between objects in our three-dimensional world and in 2-D images such as paintings and photographs. Table 17: Example of Dialogue with Insufficient Follow-up Ability: This table illustrates a dialogue sequence where the first QA pair addresses monocular cues, but the following pair shifts focus to binocular cues without maintaining topical continuity. The dialogue then reverts to monocular cues in the third pair, demonstrating an overall lack of follow-up ability in the dialogue. For each teacher’s response, the student also does not give any feedback and just jumps to the next question, which also does not align with natural conversations. 9729\n\n\nPage 5\nreconstruction task treats a conversation as a sequence of utterances {ui}2T i=1, To train it, a ran- domly chosen utterance ui is masked to create a 9711\n\n\nPage 5\nWhile non-answerable questions could also rep- resent the curiosity of a student (Scialom and Sta- iano, 2020), the answerability of questions given the context is generally important for a more useful dialog. We use the QA model1 to judge whether each question is answerable given the textbook con- tent. We refer to this metric as Answerability. This approach is akin to the method employed in (Kim et al., 2022). More details are in Table 5. 3.2.6 Factual Consistency of the Answer Factual Consistency measures whether the answer correctly responds to the student’s question. This criterion is crucial in education because it is impor- tant for students to learn accurate information (Met- zger et al., 2003). Existing metrics like Q2 (Hon- ovich et al., 2021) use a QA model to assess an- swer correctness, while RQUGE (Mohammadshahi et al., 2023) uses a QA model to evaluate the quality of the candidate question. In our scenario, we need to measure whether the answer contains correct information and accurately answers the question. Therefore, we build on the idea of Q2 and introduce a new metric referred as QFactScore: α · sim(QA(qt, S), at) + β · sim(qt, at) (1) It calculates the cosine similarity of embeddings between the predicted and original answers for each QA pair and also evaluates the similarity between the question and the original answer. This metric has been validated for\n\n\nPage 20\nlearning rate of 6.25e- 5 and employ linear learning rate decay without warmup. For model optimization, we utilize check- points from the transformers library (Wolf et al., 2020). The negative log-likelihood of the ground- truth response is minimized using the AdamW opti- mizer, as detailed in (Loshchilov and Hutter, 2019). We assess model performance using the sacrebleu implementation of the BLEU metric, following (Post, 2018). We use one V100 GPU to train the model; the FLAN-T5-LARGE model has 0.8 Bil- lion parameters and takes 7 hours to train. The result in Table 2 is the average of 3 runs. G.1 Examples of Results We demonstrate an example of a comparison be- tween the ground truth, the prediction from the model without fine-tuning, and the prediction from the model with fine-tuning in Table 15. These dia- logues are based on the MCTest split of the CoQA dataset and the predictions are generated based on the same context information as shown in Table 14. From Table 15, we find that in the third answer, the prediction example without fine-tuning is overly verbose, suggesting a poor grasp of the question, ’What was the first thing she did that morning?’ which focuses on the person’s initial action. In con- trast, the model without fine-tuning simply repli- cates the entire context sentence. On the other hand, the prediction with fine-tuning accurately mirrors the ground\n\n\nPage 2\nStudent: What is the color of Mars? Teacher: Mars has moons. Formatting (C) Subsection Title: Planet Key Concepts: Sun, Earth, Mars Learning Objectives: Learn about Planets Summary: The Sun is the center of the solar system, Earth is ... Answer Relevance Informativeness Coherence Factual Consistency Textbook source text (S) The Sun as the center of the solar system. Earth, the third planet from the Sun, with one moon. Mars, known for its red color, having two moons, Phobos and Deimos. Student: How many moons does it have? Teacher: I don't know how many moons Mars has. Student: What is interesting about this passage? Teacher: Sun is the center of solar system. Student: How many moons does Earth have? Teacher: Earth has moons, it has two moons. NA Specificity Answerability Groundedness Student: Mars is red. Teacher: Mars is red. Figure 1: Example of a synthetic teacher-student interaction based on a textbook, along with various criteria for evaluating the quality of the interaction. The criteria include Answer Relevance of the answer to the question, Coherence of the question-answer interaction with respect to the dialogue history, Informativeness of the overall interaction, Groundedness to the textbook, Answerability of the question from the textbook, Factual Consistency of the answer with respect to the question, and Specificity of the question. More details in Section 3.2.\n\n\nPage 6\nSection 5.4. 5.1 Automatic Evaluation In this section, we discuss statistics and metrics for the generated datasets. We present aggregated statistics of the datasets across four textbook do- mains in Tables 1 and 2, also noting comparisons with existing datasets: MathDial (Macina et al., 2023a) – math tutoring conversations between hu- man teachers and student LLMs, fact-based human- human conversations of QuAC (Choi et al., 2018), and NCTE transcripts of math classrooms (Dem- szky and Hill, 2023). Domain-specific results in the dataset are detailed in Tables 10 and 11. To adjust for varying dialogue lengths, we limit the number of turns to T = 12 for each model, as in (Kim et al., 2022). 5.1.1 Statistical Analysis In dialogue, different types of questions empha- size various aspects. We hypothesize that “what” and “which” questions focus on factual information. In contrast, other question types, such as “why” and “how,” tend to reflect more complex inquiries, which are also important in educational contexts. In Table 1, we present the percentages of student questions including words what, which, why, and how4. Furthermore, the average token count for questions and answers across each dataset is also shown. The key findings are as follows: 3https://openstax.org/ 4This ratio excludes ’how much’ and ’how many’ questions because they pertain to factual information. 9712\n\n\nPage 23\nStudent: What are monocular cues and how do they contribute to our perception of depth in the context of the section “5-3-vision”? Teacher: Monocular cues are visual cues that only re- quire one eye and contribute to our perception of depth. In the context of the section “5-3-vision monocular cues such as linear perspective, interposition, and relative size and closeness of images help us perceive depth in 2-D images, such as paintings and photographs. These cues allow us to understand the spatial relationships between objects in a scene and create a sense of depth, despite the visual stimulus being 2-D. Student: How do binocular cues like linear perspective and interposition contribute to our perception of depth in the context of the section “5-3-vision”? Teacher: Binocular cues like linear perspective and in- terposition contribute to our perception of depth in the context of the section “5-3-vision” by providing addi- tional visual information to our brain through the use of both eyes. Linear perspective refers to the perception of depth when we see two parallel lines appearing to con- verge, while interposition refers to the partial overlap of objects. These binocular cues, along with other cues like binocular disparity, help us establish a sense of depth and understand the spatial relationships between objects in a scene, both in our three-dimensional world and in 2-D\n\n\nPage 24\nthe first law of thermodynamics rather than the zeroth? The explanation is that this law was discovered af- ter the first and second laws of thermodynam- ics but is so fundamental that scientists decided it should logically come first. As an example of the zeroth law in action, consider newborn babies in neonatal intensive-care units in hos- pitals. Prematurely born or sick newborns are placed in special incubators. These babies have very little covering while in the incuba- tors, so to an observer, they look as though they may not be warm enough. However, in- side the incubator, t", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Book2Dial Framework**: \n   - Utilizes a Student model for question formulation and a Teacher model for providing answers based on textbook content.\n   - Adaptable to various educational dialogue formats.\n\n2. **Educational Dialogue Datasets**:\n   - Importance of high-quality data for developing educational chatbots.\n   - Challenges in data collection methods, including teacher-student pairing and classroom transcriptions.\n\n3. **Dialogue Quality Metrics**:\n   - Evaluation criteria for dialogues, including Answer Relevance, Coherence, Informativeness, Groundedness, Answerability, Factual Consistency, and Specificity.\n\n4. **Automatic Evaluation**:\n   - Statistical analysis of generated datasets across different educational domains.\n   - Examination of question types and their implications for educational dialogues.\n\n#### Goals:\n- To create a robust framework for generating educational dialogues that enhance learning through interactive question-answer formats.\n- To address the challenges of data quality and scalability in creating educational dialogue datasets.\n- To establish metrics for evaluating the effectiveness and accuracy of educational dialogues.\n\n#### Insights:\n- The Student and Teacher models can be tailored for various educational contexts, improving the adaptability of chatbots.\n- High-quality", "updated_at": 1765699340.884916}}