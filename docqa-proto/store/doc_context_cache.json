{"1bd5029ac4b4": {"context": "Page 4\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2189 for answering as a measure of difﬁculty. A knowledge graph rooted at the answer is ﬁrst constructed from a reading passage, then questions are generated such that the number of inferential steps required for answering, as determined by the knowledge graph, is set to one. The initial question is iteratively reﬁned by increasing the number of inferential steps required for answer- ing, using the knowledge graph as a guide. However, the following issues remain unaddressed in these difﬁculty-controllable QG methods. 1) The relation between the learner’s ability level and the difﬁculty of the questions is not considered, making it impossible to select a difﬁculty level that matches the learner’s ability. 2) While they can generate questions from a given reading passage and answer, they are unable to generate both a question and an answer solely from the reading pas- sage. Controlling the difﬁculty in answer generation has not been investigated, although it is just as important as controlling the difﬁculty of questions since difﬁculty is a property that generally depends on both questions and answers. To resolve these issues, we use the IRT to quantify difﬁculty levels and to automatically generate both questions and answers from the reading passage, based on speciﬁed difﬁculty values. V. ITEM RESPONSE\n\n\nPage 11\n1 60 × 500 60 \u0002 d=1 500 \u0002 k=1 |ˆθdk −θd| (6) where θd is the true ability of the d ∈{1, . . . , 60}th model, and ˆθdk is the estimated ability of the dth model obtained in the k ∈{1, . . . , 500}th experiment. We found that the MAE was 0.707 for the proposed method and 0.744 for random generation. This conﬁrms that even when considering all QA systems, our method enables ability estimation with higher accuracy than the random method. More efﬁcient ability estimation enables the generation of questions with a more appropriate difﬁculty level for each learner, suggesting that the proposed adaptive strategy is an important component for the difﬁculty-controllable QG. IX. CONCLUSION In this study, we proposed a method for automatically gener- ating question-and-answer pairs with speciﬁed difﬁculties based on the IRT, as well as an adaptive QG technique that leverages this method. Through machine-based evaluation experiments using the SQuAD dataset, we demonstrated that the proposed method is capable of generating questions with appropriately adjusted difﬁculty levels. In addition, human-based evaluation experiments supported the result that the generated questions were ﬂuent and contextually relevant. These experiments also conﬁrmed that questions can be generated with difﬁculty levels aligned with human abilities. Moreover, we showed that even in general scenarios where the\n\n\nPage 7\nlarge dataset without difﬁculty considerations can enhance their performance in answer extraction and QG. 7) Fine-tuning was then performed on D(train) b to develop answer-extraction and QG models that consider question difﬁculty. This ﬁne-tuning employed the model parame- ters estimated in step (6) as initial values. 8) To assess the proﬁciency of the developed models in con- trolling the difﬁculty, we generated question–answer pairs with various difﬁculties and evaluated them. Speciﬁcally, we ﬁrst input each reading passage in D(eval) b and each of the 61 difﬁculty values, from −3.0 to 3.0 in increments of 0.1, into the proposed answer-extraction model and gen- erated 61 answers for each reading passage. Then, given each triplet consisting of a reading passage, difﬁculty value, and generated answer, we generated questions using the proposed QG model. The generated sets of questions and answers were subjected to both machine-based and human evaluations. We now summarize the basic statistics of the datasets D(train) b and D(eval) b , which we developed in the aforementioned pro- cedure (5) to train and evaluate our difﬁculty-controllable QG method. First, the number of reading passages in D(train) b and D(eval) b was 1860and207, respectively. Next, theaveragenumber of questions per reading passage in D(train) b and D(eval) b was 5.21 and 4.28. Furthermore, Fig. 7 is a\n\n\nPage 13\n2198 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 [45] M. Uto and M. Ueno, “A generalized many-facet Rasch model and its Bayesian estimation using Hamiltonian Monte Carlo,” Behaviormetrika, vol. 47, pp. 469–496, May 2020. [46] M. Uto and M. Ueno, “Empirical comparison of item response theory mod- els with Rater’s parameters,” Heliyon, vol. 4, no. 5, pp. 1–32 May 2018. [47] M. Uto, “A Bayesian many-facet Rasch model with Markov modeling for rater severity drift,” Behav. Res. Methods, vol. 55, pp. 3910–3928, Oct. 2022. [48] M. Uto, “A multidimensional generalized many-facet Rasch model for rubric-based performance assessment,” Behaviormetrika, vol. 48, pp. 423–457, Jul. 2021. [49] G.Rasch,ProbabilisticModelsforSomeIntelligenceandAttainmentTests. Chicago, IL, USA: Univ. Chicago Press, 1981. [50] S.-H. K. Frank and B. Baker, Item Response Theory: Parameter Estimation Techniques. Boca Raton, FL, USA: CRC Press, 2004. [51] J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications. New York, NY, USA: Springer, 2010. [52] M. Uto, I. Aomi, E. Tsutsumi, and M. Ueno, “Integration of prediction scores from various automated essay scoring models using item response theory,” IEEE Trans. Learn. Technol., vol. 16, no. 6, pp. 983–1000, Dec. 2023. [53] A. Srikanth, A. S. Umasankar, S. Thanu, and S. J. Nirmala, “Extractive text summarization using dynamic clustering and\n\n\nPage 12\nquestion generation,” in Proc. Conf. Web Big Data, 2021, pp. 332–347. [13] Y.-H. Chan and Y.-C. Fan, “A recurrent BERT-based model for question generation,” in Proc. 2nd Workshop Mach. Reading Question Answering, 2019, pp. 154–162. [14] X. Du, J. Shao, and C. Cardie, “Learning to ask: Neural question generation for reading comprehension,” in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, 2017, pp. 1342–1352. [15] A. Ushio, F. Alva-Manchego, and J. Camacho-Collados, “Generative language models for paragraph-level question generation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2022, pp. 670–688. [16] J.Yu,Q.Su,X.Quan,andJ.Yin,“Multi-hopreasoningquestiongeneration and its application,” IEEE Trans. Knowl. Data Eng., vol. 35, no. 1, pp. 725–740, Jan. 2023. [17] Y. Zhao, X. Ni, Y. Ding, and Q. Ke, “Paragraph-level neural question generationwithmaxoutpointerandgatedself-attentionnetworks,”inProc. Conf. Empirical Methods Natural Lang. Process., 2018, pp. 3901–3910. [18] J. Li, Y. Gao, L. Bing, I. King, and M. R. Lyu, “Improving question generation with to the point context,” in Proc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process., 2019, pp. 3216–3226. [19] P. Nema, A. K. Mohankumar, M. M. Khapra, B.V. Srinivasan, and B. Ravindran, “Let’s ask again: Reﬁne network for automatic question gen- eration,” in Proc. Conf.\n\n\nPage 5\nsubstitute actual learner responses with automated QA systems, in the same way that several previous difﬁculty-controllable QG studies have done [12], [22]. 2) Difﬁculty estimation using IRT: Using the collected re- sponse data, we estimate3 the question difﬁculty based on the Rasch model following the item calibration procedure introduced in Section V. 3) Creating a dataset with difﬁculty estimates: We construct a dataset consisting of quadruplets (r, q, a, b) by appending the estimated difﬁculty values b into the triplets (r, q, a) of the SQuAD dataset. Using this dataset, the proposed method trains following two models: 1) a difﬁculty-controllable answer-extraction model; 2) a difﬁculty–controllable answer-aware QG model. B. Difﬁculty-Controllable Answer-Extraction Model We use BERT [24] for difﬁculty-controllable answer extrac- tion. BERT is a pretrained transformer model, the details of which are summarized in Table II. BERT can be adapted for various downstream tasks by ﬁne-tuning it with task-speciﬁc su- pervised datasets and incorporating task-speciﬁc output layers. We employed BERT for the answer-extraction task because of its extensivepriorusageinvarioustext-extractionapplications[53]. To perform answer extraction using BERT, we add output layers that predict the start and end positions of the answer text within a given reading passage. Speciﬁcally, letting vi be the\n\n\nPage 12\nseparation,” in Proc. AAAI Conf. Artif. Intell., 2019, pp. 6602–6609. [43] X. Sun, J. Liu, Y. Lyu, W. He, Y. Ma, and S. Wang, “Answer-focused and position-aware neural question generation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2018, pp. 3930–3939. [44] M. Ueno and Y. Miyazawa, “IRT-based adaptive hints to scaffold learning inprogramming,”IEEETrans.Learn.Technol.,vol.11,no.4,pp. 415–428, Oct.–Dec. 2018.\n\n\nPage 8\nis given by Fig. 9, which shows the correct answer ratio of the 60 QA systems for the generated question–answer pairs given b = 1.0. The x-axis represents the ability θ, the y-axis represents the correct answer ratio, each plot indicates an individual QA system, and the orange line repre- sents the logistic regression curve ﬁtted to the 60 data points by the least-squares method. As described before, the Rasch model gives a probability of 0.5 that a correct answer will be given when θ = b. To emphasize this point, the green dashed line represents θ = 1.0 and a correct answer ratio of 0.5. The ﬁgure shows that the QA systems with an ability level around θ = 1.0 demonstrate a correct answer ratio of approximately 0.5, suggesting that the questions are generated as expected. A similar trend was observed for other difﬁculty levels, as exempliﬁed in Figs. 10 Fig. 9. Average correct answer ratio of the QA systems for generated questions given difﬁculty b = 1.0. Each dot plot represents an individual QA system. Fig. 10. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b = 2.0. Each dot plot represents an individual QA system. Fig. 11. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b = 0.0. Each dot plot represents an individual QA system. and 11, which show the results for b = 2.0 and b = 0.0, respec-\n\n\nPage 3\nneural QG method for reading comprehen- sion is an RNN-based seq2seq model [14]. In this method, a reading passage and an answer are fed into an RNN encoder, and the output feature vector is given to an RNN decoder to generate a question text. In addition, Zhou et al. [32] proposed using an RNN-based QG model that can consider both the words’ sequence and their part-of-speech (POS) tags. In recent years, pretrained transformer-based models, which have outperformed RNN-based seq2seq models on many natural language processing tasks (e.g., [25], [27], [33], [34], [35], [36], [37], [38], [39], and [40]), have been used for automated QG tasks (e.g., [1], [13], [14], [41], [42], and [43]). Some examples include a QG method proposed by Chan and Fan [13] that uses BERT and a method proposed by Lee and Lee [23] that uses T5. B. Difﬁculty-Controllable Neural QG for Reading Comprehension When utilizing QG methods as a learning aid to foster reading comprehension skills, it is critical to be able to generate ques- tions with arbitrary difﬁculty levels [44]. Accordingly, several recent studies have proposed difﬁculty-controllable QG meth- ods [22], [28]. For example, Gao et al. [22] proposed an RNN-based seq2seq model that generates reading comprehension questions for dif- ﬁculty levels categorized as either “easy” or “hard.” They also proposed to construct training data for the\n\n\nPage 10\nevaluation criterion. The results showed that the agreement rates for ﬂuency, content relevance, answerability, and practicality were 0.63, 0.85, 0.53, and 0.60, respectively, while that for difﬁcultywas 0.22. Fromtheseresults, weconcludethat it is hard to ensure reliable subjective evaluations for question difﬁculty,", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Adaptive Question-Answer Generation (QG):** The study focuses on generating questions and answers from reading passages with controlled difficulty levels.\n2. **Item Response Theory (IRT):** Utilized to quantify difficulty levels and align them with learners' abilities.\n3. **Knowledge Graphs:** Employed to construct questions based on inferential steps from reading passages.\n\n**Goals:**\n1. **Address Limitations of Existing QG Methods:** The study aims to resolve issues related to matching question difficulty with learner ability and generating both questions and answers from reading passages.\n2. **Develop a Difficulty-Controllable QG System:** Create a model that can automatically generate question-answer pairs with specified difficulty levels, enhancing personalized learning experiences.\n3. **Evaluate Model Performance:** Conduct both machine-based and human evaluations to assess the fluency, relevance, and appropriateness of generated questions.\n\n**Insights:**\n1. **Higher Accuracy in Ability Estimation:** The proposed method showed better accuracy in estimating learner ability compared to random generation methods, which is crucial for generating appropriately difficult questions.\n2. **Human Alignment:** The generated questions were found to be contextually relevant and fluent,", "updated_at": 1763989173.353781}, "d8b65f65dc38": {"context": "Page 12\nInformation Retrieval, pages 539–548. Manav Rathod, Tony Tu, and Katherine Stasaski. 2022. Educational multi-question generation for reading comprehension. In Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022), pages 216–223. Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversational question answering challenge. Transactions of the Association for Com- putational Linguistics, 7:249–266. Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Association for Com- putational Linguistics. Sherry Ruan, Liwei Jiang, Justin Xu, Bryce Joe-Kun Tham, Zhengneng Qiu, Yeshuang Zhu, Elizabeth L Murnane, Emma Brunskill, and James A Landay. 2019. Quizbot: A dialogue-based adaptive learn- ing system for factual knowledge. In Proceedings 9718\n\n\nPage 22\nGround Truth Prediction without Fine-tuning Prediction with Fine-tuning Q: Who is waking up? A: Sophie Q: How was she feeling today? A: Tired. Q: What was the first thing she did that morning? A: She rolled out of bed. Q: What woke her up that morning? A: Her alarm Q: What did she do after turning the alarm off? A: Stretched. Q: What did she do next? A: Showered. Q: Who is waking up? A: Sophie Q: How was she feeling today? A: Tired. Q: What was the first thing she did that morning? A: She rolled out of bed, turned off her alarm and stretched. Q: What woke her up that morning? A: Her alarm Q: What did she do after turning the alarm off? A: Stretched. Q: What did she do next? A: Showered. Q: Who is waking up? A: Sophie Q: How was she feeling today? A: Tired. Q: What was the first thing she did that morning? A: She rolled out of bed. Q: What woke her up that morning? A: Her alarm Q: What did she do after turning the alarm off? A: Stretched. Q: What did she do next? A: Showered. Table 15: CoQA MCTest split dialogue comparison: The prediction example without fine-tuning provides verbose content in the third answer, indicating that the model does not have a good understanding of the question which asks, ’What was the first thing she did that morning?’, emphasizing the initial action of the person. However, the model without fine-tuning returns the entire sentence from the context.\n\n\nPage 8\nDialogue Inpainting In alignment with the results of human evaluation, we find that the Dialogue Inpainting model tends to generate “general” questions, such as “What is interesting about this passage?” These types of questions, which are not specific to the textbook content, are less desirable in educational dialogue. 5.4 Fine-tuning for Educational Chatbots In this section, we show the effectiveness of these imperfect but automatically created synthetic data based on any textbook domain for the downstream task of fine-tuning educational chatbots. We fine- tune simple chatbot models with our synthesized data and assess their performance on educational conversation task of next utterance generation. Specifically, we use text generation models based on language models to generate teacher re- sponses at given the dialogue history h<t, textbook grounding information S, and the question qt. We compare two scenarios: (1) a model fine-tuned on synthetic textbook data, then further fine-tuned and tested on various educational or information- seeking dialogue datasets; and (2) a model fine- tuned and tested solely on these dialogue datasets without fine-tuning on synthetic textbook data. We use FLAN-T5-LARGE (Chung et al., 2024) as our base language model. For our test sets, we use the MCTest and CNN splits of the CoQA dataset (Reddy et al., 2019), as well as the NCTE dataset (Demszky\n\n\nPage 15\nBERTScore F1 for each dialogue question against the immediately preceding answer as the reference. Ag- gregated scores provide a measure of overall coherence. Informative- -ness 1-Overlap (at, a<t) 1 −|at∩a<t| |at∪a<t| For each answer in a dialogue, the proportion of its intersection with previous answers to their union is computed using word- level tokens. This value is then subtracted from 1. Content Match Density 1 |h1:T | P f∈F(S,h1:T ) |f|2 ,F(S, h1:T ):the set of extractive phrases in dialogue h1:T extracted from textbook content S. Density refer to Extractive Fragment Density (Grusky et al., 2018), as the average length of text spans that are directly ex- tracted from textbook content S and included in the dialogues. Coverage 1 |h1:T | P f∈F(S,h1:T ) |f| Coverage refer to Extractive Fragment Coverage (Grusky et al., 2018), as the percentage of words in a dialogue that originated from the textbook content. Answerability Answerable Valid(QA(qt,S)) We use the “distilbert-base-cased-distilled-squad” QA model to determine if a question is answerable from the textbook content. If it generates an empty string or an invalid answer such as “CANNOTANSWER”, the question is deemed unanswerable. We report the ratio of answerable questions as 1 minus the ratio of unanswerable questions. Factual Consistency QFactScore αsim(QA(qt, S), at) + βsim(qt, at) For each QA pair, it computes\n\n\nPage 13\n1941–1961, Singapore. Association for Computational Linguistics. Katherine Stasaski, Kimberly Kao, and Marti A Hearst. 2020. Cima: A large open access dialogue dataset for tutoring. In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–64. Katherine Stasaski, Manav Rathod, Tony Tu, Yunfang Xiao, and Marti A Hearst. 2021. Automatically gen- erating cause-and-effect questions from passages. In Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 158–170. Abhijit Suresh, Jennifer Jacobs, Margaret Perkoff, James H. Martin, and Tamara Sumner. 2022. Fine- tuning transformers with additional context to clas- sify discursive moves in mathematics classrooms. In Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022), pages 71–81, Seattle, Washington. Associa- tion for Computational Linguistics. Wei Tan, Jionghao Lin, David Lang, Guanliang Chen, Dragan Gaševi´c, Lan Du, and Wray Buntine. 2023. Does informativeness matter? active learning for ed- ucational dialogue act classification. In International Conference on Artificial Intelligence in Education, pages 176–188. Springer. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al.\n\n\nPage 19\nDomain Models Answer Relevance Informativeness Groundedness Coherence Answerability Factual Consistency BF1 (qt, at) QuestEval Uptake 1 - Overlap (at, a<t) Density Coverage BF1 (qt, a<t) BF1 (qt, at−1) Answerable QFactScore Math SimSeek 0.51 0.24 0.64 0.61 9.5 0.71 0.49 0.53 0.74 0.27 Dialogue Inpainting 0.57 0.30 0.84 0.88 19.37 0.88 0.46 0.47 0.52 0.19 Persona (Single Instance) 0.58 0.32 0.97 0.85 2.94 0.62 0.50 0.52 0.87 0.53 Persona (Low Info) 0.62 0.43 0.99 0.54 1.94 0.59 0.51 0.59 0.99 0.80 Persona (Medium Info) 0.62 0.43 0.99 0.55 2.09 0.60 0.51 0.59 1.00 0.81 Persona (High Info) 0.62 0.43 0.99 0.56 2.07 0.60 0.52 0.60 0.99 0.81 Business SimSeek 0.54 0.25 0.82 0.77 13.16 0.88 0.52 0.56 0.89 0.32 Dialogue Inpainting 0.49 0.26 0.81 0.94 26.44 0.92 0.43 0.45 0.88 0.23 Persona (Single Instance) 0.58 0.36 0.99 0.88 4.07 0.82 0.50 0.53 0.95 0.52 Persona (Low Info) 0.62 0.46 0.99 0.61 2.38 0.76 0.52 0.60 0.99 0.73 Persona (Medium Info) 0.62 0.46 0.99 0.61 2.31 0.77 0.53 0.60 1.00 0.73 Persona (High Info) 0.63 0.46 0.99 0.62 2.44 0.77 0.54 0.60 1.00 0.74 Science SimSeek 0.52 0.25 0.81 0.71 11.78 0.83 0.50 0.54 0.89 0.34 Dialogue Inpainting 0.51 0.27 0.82 0.92 20.43 0.90 0.44 0.44 0.72 0.24 Persona (Single Instance) 0.58 0.35 0.98 0.85 4.65 0.79 0.48 0.51 0.94 0.61 Persona (Low Info) 0.59 0.43 0.99 0.57 2.55 0.73 0.51 0.57 0.98 0.79 Persona (Medium Info) 0.59 0.43 0.99 0.57 2.63\n\n\nPage 5\nits alignment with human evaluation, as detailed in Appendix E. The final score is the weighted sum of two similarity scores. More details are in Table 5 and Appendix C.4. 3.2.7 Specificity of the Question Specificity assesses whether the question is specific, rather than general. An example of a generic ques- tion is ‘What is interesting about this passage?’. The Specificity criterion is crucial in education, as teachers should ask specific questions (Yang, 2017). We assess specificity through human evaluation, as there is no existing metric that captures specificity. 4 From Textbooks to Dialogues In this section, we describe different methods used for generating dialogues from educational text- books in Book2Dial, namely: 1https://huggingface.co/distilbert-base-cased-distilled- squad 1. Multi-turn QG-QA models. In this setting, we use fine-tuned QG and QA models inter- acting with each other. 2. Dialogue Inpainting. Dai et al. (2021) uses a span extraction model over the textbook as a teacher model, where the response is copied from the textbook and the question is gener- ated by a QG model acting as the student. 3. Persona-based Generation. This approach uses LLMs like GPT-3.5, and leverages prompting to interactively simulate the stu- dent and the teacher and generate dialogues. We describe these methods next. More imple- mentation details are in Appendix C. 4.1 Multi-turn\n\n\nPage 11\nHonovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. q2: Evaluating factual consistency in knowledge- grounded dialogues via question generation and ques- tion answering. In Proceedings of the 2021 Confer- ence on Empirical Methods in Natural Language Pro- cessing, pages 7856–7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Yerin Hwang, Yongil Kim, Hyunkyung Bae, Hwanhee Lee, Jeesoo Bang, and Kyomin Jung. 2023. Dialo- gizer: Context-aware conversational-QA dataset gen- eration from textual sources. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8806–8828, Singapore. Association for Computational Linguistics. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation. ACM Comput- ing Surveys, 55(12):1–38. 9717\n\n\nPage 20\nCriterion Questions for each QA pair Answer Relevance Question: Is the response directly ad- dressing the posed question? (answer no if it is answering a different question) Informativeness Question: Does the current answer in- troduce new information that was not mentioned in previous answers within the same conversation? Groundedness Question: Does the answer contain spe- cific details or data points mentioned in the contextual background or previous dialogue? Coherence Question 1: Does the current question directly follow up on the immediate pre- vious answer? (Ignore the first QA pair) Factual Consis- tency Question: Does the answer correctly ad- dress the question, considering the con- text provided? (If ’answerability’ is ’no,’ then this criterion should also be ’no.’) Answerability Question: Can the question be answered given the context? Specificity Question: Does this question exhibit generality, such that it could be relevant beyond the immediate context provided? (e.g. What is interesting about this pas- sage?) Table 12: Exact framing of questions asked during the human evaluation: annotators answer each question with yes or no for each QA pair. Participants Pairs Cohen’s Kappa P1 vs. P2 0.71 P1 vs. P3 0.7", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Educational Multi-Question Generation**: Focus on generating questions from educational texts to enhance reading comprehension.\n2. **Conversational Question Answering (CoQA)**: Discusses a framework for evaluating dialogue systems that can answer questions based on conversational context.\n3. **Fine-tuning Techniques**: Explores the effectiveness of fine-tuning language models with synthetic data derived from textbooks for educational chatbots.\n4. **Dialogue Inpainting**: Investigates methods for generating educational dialogues, emphasizing the specificity and relevance of questions generated.\n\n#### Goals:\n- **Improve Reading Comprehension**: Develop systems that can generate meaningful questions to facilitate better understanding of educational material.\n- **Enhance Chatbot Performance**: Fine-tune models to improve their ability to generate contextually relevant and specific responses in educational settings.\n- **Evaluate Dialogue Quality**: Establish metrics for assessing the coherence, informativeness, and factual consistency of dialogue generated by educational chatbots.\n\n#### Insights:\n- **Importance of Specificity**: Questions should be specific rather than general to effectively engage learners and assess their comprehension.\n- **Fine-tuning Benefits**: Models that are fine-tuned on synthetic educational", "updated_at": 1763989179.684442}, "e18fc4c1febe": {"context": "Page 1\nSchool of Informatics and Engineering, University of Electro-Communications, Chofu 182-8585, Japan (e-mail: tomikawa@ai.lab.uec.ac.jp; suzuki_ayaka@ai.lab.uec.ac.jp; uto@ai. lab.uec.ac.jp). Digital Object Identifier 10.1109/TLT.2024.3491801 in various educational applications, including intelligent tutoring systems, writing-assistance tools, and knowledge-evaluation platforms [1], [2], [3], [4], [5], [6], [7]. Early QG methods were based on rule-based or templatebased approaches, which generated questions by converting declarative texts into interrogative questions, using handcrafted rules or templates [1], [8], [9], [10], [11]. However, preparing well-designed rules and templates for specific applications is time consuming and labor-intensive [1], [12]. To address this limitation, end-to-end QG methods based on deep neural networks have been used [1], [13], [14], [15], [16], [17], [18], [19], [20]. Initial neural QG methods were designed as sequenceto-sequence (seq2seq) models based on recurrent neural networks (RNNs) and attention mechanisms [14], [21], whereas more recent methods have employed pretrained transformer models [2], [5], [13], [15], [22], [23], such as bidirectional encoder representations from transformers (BERT) [24], generative pretrained transformer 2 (GPT-2) [25], bidirectional and autoregressive transformers (BART) [26], and text-to-text transfer\n\n\nPage 1\nstandard benchmark dataset, such as SQuAD. In the method, each question in the dataset is answered by two question–answering (QA) systems. Then, the binary difficulty label is estimated for each question. Specifically, the label “easy” is assigned to a question when both QA systems correctly answer it, while the label “hard” is assigned when both incorrectly answer it. Questions for which only one QA system produces a correct answer are excluded from the dataset. An RNN-based seq2seq QG model is then trained using this training dataset, which comprises a reading passage, a question text, a corresponding answer, and a difficulty label. The model is designed to take the difficulty label as part of the input, enabling the generation of questions tailored to specific difficulty levels. Another difficulty-controllable QG method, proposed by Cheng et al. [28], uses the number of inferential steps required TOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL for answering as a measure of difficulty. A knowledge graph rooted at the answer is first constructed from a reading passage, then questions are generated such that the number of inferential steps required for answering, as determined by the knowledge graph, is set to one. The initial question is iteratively refined by increasing the number of inferential steps required for answering, using the knowledge\n\n\nPage 1\nModel We use T5 for difficulty-controllable answer-aware QG, where T5 is a pretrained transformer model, the details of which are summarized in Table II. We use T5 because it has been widely used before in various text generation tasks [54], [55], [56], [57] including QG tasks [2], [58], [59] and has achieved higher accuracy in QG compared to models, such as BART and GPT-2 [58]. Conventional answer-aware QG models [60] based on pretrained language models are implemented by designing the model’s input as r1 , . . . , [A], a1 , . . . , aK , [A], . . . , rI (3) where [A] is a special token representing an answer’s start and end positions within a reading passage. The model’s target, which is a question text, is designed as [G], q1 , . . . , qJ , [E] (4) where [G] and [E] are also special tokens representing the beginning of a question text and the end of a question text, respectively. To implement difficulty-control for the answer-aware QG model, we concatenate a target difficulty value to the aforementioned conventional input form using b, [Q], r1 , . . . , [A], a1 , . . . , aK , [A], . . . , rI (5) where [Q] is the special token used to separate the difficulty value and the given reading passage. Given this input, the model generates a question text based on a reading passage, an answer, and a target difficulty value. Fig. 5 presents an outline of our QG model. We can fine-tune\n\n\nPage 1\nmodel; 2) a difficulty–controllable answer-aware QG model. B. Difficulty-Controllable Answer-Extraction Model We use BERT [24] for difficulty-controllable answer extraction. BERT is a pretrained transformer model, the details of which are summarized in Table II. BERT can be adapted for various downstream tasks by fine-tuning it with task-specific supervised datasets and incorporating task-specific output layers. We employed BERT for the answer-extraction task because of its extensive prior usage in various text-extraction applications [53]. To perform answer extraction using BERT, we add output layers that predict the start and end positions of the answer text within a given reading passage. Specifically, letting v i be the BERT output vector for the ith word in a passage r, we add two dense layers, denoted as f (s) (v i ) and f (e) (v i ), for each v i , where each dense layer transforms v i into a scalar value . The dense-layer outputs are then transformed through softmax activations as P (s) = softmax(f (s) (v 1 ), . . . , f (s) (v I )) and P (e) = (s) softmax(f (e) (v 1 ), . . . , f (e) (v I )), whose ith elements Pi and (e) Pi represent the probability values for whether the ith word is at the start and end positions, respectively. Thus, by extracting the word sequence within the start and end positions, which take the maximum probabilities, we can extract an answer text\n\n\nPage 1\nTECHNOLOGIES, VOL. 17, 2024 TABLE I OVERVIEW OF RELATED WORKS A. Neural QG for Reading Comprehension A representative neural QG method for reading comprehension is an RNN-based seq2seq model [14]. In this method, a reading passage and an answer are fed into an RNN encoder, and the output feature vector is given to an RNN decoder to generate a question text. In addition, Zhou et al. [32] proposed using an RNN-based QG model that can consider both the words’ sequence and their part-of-speech (POS) tags. In recent years, pretrained transformer-based models, which have outperformed RNN-based seq2seq models on many natural language processing tasks (e.g., [25], [27], [33], [34], [35], [36], [37], [38], [39], and [40]), have been used for automated QG tasks (e.g., [1], [13], [14], [41], [42], and [43]). Some examples include a QG method proposed by Chan and Fan [13] that uses BERT and a method proposed by Lee and Lee [23] that uses T5. Fig. 2. Our QG task: generating a question–answer pair while controlling its difficulty. generate a question text q and an answer text a given a reading passage r and a target difficulty value b, where the difficulty value b is assumed to be quantified based on the IRT, as explained in the introduction. Furthermore, the task for the second objective is a cyclic process comprising the following subtasks. 1) Estimating the learner’s ability based on\n\n\nPage 1\ntransformer (T5) [27]. These approaches have successfully generated fluent questions that are relevant to the given reading passages. A notable application of QG in education is reading tutors [1], [3], [4], [5], which provide reading-comprehension questions for diverse reading materials. Offering questions helps direct learners’ attention to the content and helps them identify misconceptions, thereby improving their reading-comprehension skills [3]. To enhance the efficiency of such learning, offering questions with difficulty levels tailored to each learner’s reading ability is beneficial. For this reason, several difficulty-controllable QG methods have recently been proposed. Difficulty-controllable QG for reading comprehension is a relatively new area of research, and thus, the literature is scant [22], [28]. One method for realizing difficulty-controllable QG is to use an RNN-based seq2seq model in which the hidden states from the encoder are adjusted to accept the difficulty levels categorized as either easy or hard [22]. Another approach is a multihop method [28], which defines the question difficulty according to the number of inference steps required to answer the question and generates questions by controlling the number of these inference steps. However, both methods face the following limitations that make it difficult to generate questions suitable for the\n\n\nPage 1\n3314–3323. 2197 [20] X. Ma, Q. Zhu, Y. Zhou, and X. Li, “Improving question generation with sentence-level semantic matching and answer position inferring,” in Proc. AAAI conf. Artif. Intell., 2020, vol. 34, pp. 8464–8471. [21] L. Song, Z. Wang, and W. Hamza, “A unified query-based generative model for question generation and question answering,” 2017, arXiv:1709.01058. [22] Y. Gao, L. Bing, W. Chen, M. Lyu, and I. King, “Difficulty controllable generation of reading comprehension questions,” in Proc. 28th Int. Joint Conf. Artif. Intell., 2019, pp. 4968–4974. [23] S. Lee and M. Lee, “Type-dependent prompt CycleQAG: Cycle consistency for multi-hop question generation,” in Proc. 29th Int. Conf. Comput. Linguistics, 2022, pp. 6301–6314. [24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pretraining of deep bidirectional transformers for language understanding,” in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, 2019, pp. 4171–4186. [25] A. Radford, J. Wu, R. Child, D. Luan, and D. A. I. Sutskever, “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019. [26] M. Lewis and et al., “BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020, pp. 7871–7880. [27] C. Raffel and et al., “Exploring the limits of\n\n\nPage 1\nthe answer-aware QG model by using a dataset consisting of quadruplets (r, q, a, b), explained in D. Estimating the Learner’s Ability and Adaptively Generating Questions A previous study on adaptive learning [44] reported that posing questions that learners can answer correctly with a 50% probability is pedagogically effective. As discussed in Section V-A, in the Rasch model, the probability of obtaining a correct answer is 0.5 when the ability and difficulty levels are equal. Therefore, if a learner’s ability level is known, using the proposed method to generate a question whose difficulty level is equal to or near the learner’s ability would be effective for enhancing reading comprehensive skills. However, in typical educational settings, a learner’s ability is often unknown a priori. To address this, we are proposing an efficient method for estimating a learner’s ability. We utilize the CAT framework, as mentioned in Section V-B, while generating and administering questions at appropriate levels of difficulty. Specifically, we propose the following procedure for ability estimation and QG. 1) Randomly generate and administer a few questions to the learner to obtain initial response data. 2) Utilize the obtained response data along with the Rasch model to estimate and update the learner’s ability level. 3) Generate and administer a question at the estimated ability level\n\n\nPage 1\ngrammatical correctness and fluency of the questions. Ratings were done on a three-point scale: 3. appropriate, 2. acceptable, and 1. inappropriate. 2) Content Relevance: Assess whether the generated questions were relevant to the reading passage. Ratings were done on a two-point scale: 2. appropriate and 1. inappropriate. 3) Answerability: Evaluate whether the extracted answer was indeed the correct answer for the generated question. Ratings were done on a nominal scale with four categories: a. appropriate, b. insufficient, c. excessive, and d. inappropriate. 4) Practicality: Assess whether the question and/or answer could become feasible with slight modifications. Ratings were done on a three-point scale: 3. already feasible, 2. need minor", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Question Generation (QG) Methods**: \n   - Evolution from rule-based/template-based approaches to deep neural network methods.\n   - Use of sequence-to-sequence (seq2seq) models and transformer models (e.g., BERT, GPT-2, T5) for generating questions.\n\n2. **Difficulty Control in QG**:\n   - Techniques for generating questions with controlled difficulty levels, including methods based on learner ability and inferential steps required for answering.\n\n3. **Adaptive Learning**:\n   - The importance of tailoring questions to match the learner's ability to enhance reading comprehension skills.\n   - Use of the Rasch model and Computerized Adaptive Testing (CAT) framework for estimating learner ability.\n\n4. **Evaluation Metrics for QG**:\n   - Criteria for assessing the quality of generated questions, including grammatical correctness, content relevance, answerability, and practicality.\n\n**Goals:**\n- To improve the efficiency and effectiveness of educational applications by generating questions that are relevant and appropriately challenging for learners.\n- To develop methods that adaptively assess and respond to a learner's ability, ensuring that questions are neither too easy nor too difficult.\n\n**", "updated_at": 1763989185.594941}}