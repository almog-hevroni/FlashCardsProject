{"4d0f977beb85": {"context": "Page 10\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2195 TABLE IV HUMAN EVALUATION 900 or higher.6 The raters were asked to assess the assigned question–answer pairs based on the following evaluation crite- ria. The raters were crowdworkers having a TOEIC score of 900 or higher . 1) Fluency: Evaluate the grammatical correctness and ﬂu- ency of the questions. Ratings were done on a three-point scale: 3. appropriate, 2. acceptable, and 1. inappropriate. 2) Content Relevance: Assess whether the generated ques- tions were relevant to the reading passage. Ratings were done on a two-point scale: 2. appropriate and 1. inappro- priate. 3) Answerability: Evaluate whether the extracted answer was indeed the correct answer for the generated question. Ratings were done on a nominal scale with four categories: a. appropriate, b. insufﬁcient, c. excessive, and d. inappro- priate. 4) Practicality: Assess whether the question and/or answer could become feasible with slight modiﬁcations. Ratings were done on a three-point scale: 3. already feasible, 2. need minor correction, and 1. not feasible. 5) Question Difﬁculty: Evaluate the difﬁculty level of the generated question. Ratings were done on a ﬁve-point scale, ranging from 1 (easiest) to 5 (most difﬁcult). Results for ﬂuency, content relevance, answerability, and practicality are presented in Table IV. From this\n\n\nPage 4\nTHEORY (IRT) The IRT [29] is a statistical framework that uses probabilistic models,calledIRTmodels,toestimatetwolatentfactors:1 exam- inee’s ability and item characteristics such as item difﬁculty and discriminative power,2 where the examinee and the item corre- spond to the learner and the question, respectively, in our study. These latent factors are estimated from the response data, which generally consist of the examinee’s binary correct/incorrect re- sponses to the items. The IRT has been widely used in various educationalandpsychologicaltestsbecauseofitsnumerousben- eﬁts, such as offering accurate estimates of examinee’s ability and item characteristics, unifying measurement scales among different tests, and facilitating CAT applications [45], [46], [47], [48]. This study uses the Rasch model [49], the most traditional and well-known IRT model, to quantify question difﬁculty. A. Rasch Model The Rasch model deﬁnes the probability that the mth exami- nee correctly answers the nth item as pnm = exp(θm −bn) 1 + exp(θm −bn) (1) where bn denotes the difﬁculty of the nth item, and θm denotes the latent ability of the mth examinee. For a detailed explanation of this model, Fig. 3 illustrates item response curves (IRCs) for the Rasch model, which are plots of 1In this context, “latent” indicates that the corresponding factors are not directly observable but lie behind the\n\n\nPage 12\nseparation,” in Proc. AAAI Conf. Artif. Intell., 2019, pp. 6602–6609. [43] X. Sun, J. Liu, Y. Lyu, W. He, Y. Ma, and S. Wang, “Answer-focused and position-aware neural question generation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2018, pp. 3930–3939. [44] M. Ueno and Y. Miyazawa, “IRT-based adaptive hints to scaffold learning inprogramming,”IEEETrans.Learn.Technol.,vol.11,no.4,pp. 415–428, Oct.–Dec. 2018.\n\n\nPage 6\nof a question text, respectively. To implement difﬁculty-control for the answer-aware QG model, we concatenate a target difﬁculty value to the aforemen- tioned conventional input form using b, [Q], r1, . . . , [A], a1, . . . , aK, [A], . . . , rI (5) where [Q] is the special token used to separate the difﬁculty value and the given reading passage. Given this input, the model generates a question text based on a reading passage, an answer, and a target difﬁculty value. Fig. 5 presents an outline of our QG model. We can ﬁne-tune the answer-aware QG model by using a dataset consisting of quadruplets (r, q, a, b), explained in Fig. 5. Difﬁculty-controllable answer-aware QG model using T5. Section VI-A. Speciﬁcally, we prepare the input data following (5) format and output data following (4) format, and train T5 by maximizing the log-likelihood for target data. D. Estimating the Learner’s Ability and Adaptively Generating Questions A previous study on adaptive learning [44] reported that posing questions that learners can answer correctly with a 50% probability is pedagogically effective. As discussed in Sec- tion V-A, in the Rasch model, the probability of obtaining a correct answer is 0.5 when the ability and difﬁculty levels are equal. Therefore, if a learner’s ability level is known, using the proposed method to generate a question whose difﬁculty level is equal to or near the\n\n\nPage 9\nWe then divided them into ﬁve sets, each containing 20 question–answer pairs, and assigned two raters to each set. The raters were crowdworkers having Test of English for International Communication (TOEIC) scores of\n\n\nPage 12\n2014, vol. 282, pp. 325–338. [5] M. Rathod, T. Tu, and K. Stasaski, “Educational multi-question generation for reading comprehension,” in Proc. 17th Workshop Innov. Use NLP Build. Educ. Appl., 2022, pp. 216–223. [6] M. Liu and R. A. Calvo, “Using information extraction to generate trigger questions for academic writing support,” in Proc. Int. Conf. Intell. Tutoring Syst., 2012, pp. 358–367. [7] M. Liu, R. A. Calvo, and V. Rus, “G-Asks: An intelligent automatic ques- tiongenerationsystemforacademicwritingsupport,”DialogueDiscourse, vol. 3, no. 2, pp. 101–124, Mar. 2012. [8] J. Mostow and W. Chen, “Generating instruction automatically for the reading strategy of self-questioning,” in Proc. Int. Conf. Artif. Intell. Educ., 2009, pp. 456–472. [9] H. Kunichika, T. Katayama, T. Hirashima, and A. Takeuchi, “Automated question generation methods for intelligent English learning systems and its evaluation,” in Proc. Int. Conf. Consum. Electron., 2004. [10] Y. Huang and L. He, “Automatic generation of short answer questions for reading comprehension assessment,” Natural Lang. Eng., vol. 22, no. 3, pp. 457–489, May 2016. [11] M. Heilman and N. A. Smith, “Good question! Statistical ranking for question generation,” in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics, 2010, pp. 609–617. [12] F. Chen, J. Xie, Y. Cai, T. Wang, and Q. Li, “Difﬁculty-controllable visual\n\n\nPage 11\n1 60 × 500 60 \u0002 d=1 500 \u0002 k=1 |ˆθdk −θd| (6) where θd is the true ability of the d ∈{1, . . . , 60}th model, and ˆθdk is the estimated ability of the dth model obtained in the k ∈{1, . . . , 500}th experiment. We found that the MAE was 0.707 for the proposed method and 0.744 for random generation. This conﬁrms that even when considering all QA systems, our method enables ability estimation with higher accuracy than the random method. More efﬁcient ability estimation enables the generation of questions with a more appropriate difﬁculty level for each learner, suggesting that the proposed adaptive strategy is an important component for the difﬁculty-controllable QG. IX. CONCLUSION In this study, we proposed a method for automatically gener- ating question-and-answer pairs with speciﬁed difﬁculties based on the IRT, as well as an adaptive QG technique that leverages this method. Through machine-based evaluation experiments using the SQuAD dataset, we demonstrated that the proposed method is capable of generating questions with appropriately adjusted difﬁculty levels. In addition, human-based evaluation experiments supported the result that the generated questions were ﬂuent and contextually relevant. These experiments also conﬁrmed that questions can be generated with difﬁculty levels aligned with human abilities. Moreover, we showed that even in general scenarios where the\n\n\nPage 5\ndifﬁculty levels for their ability. Because our difﬁculty-controllable QG method requires a dataset with IRT-based difﬁculty values for model training, we ﬁrst propose a method for constructing it in the next section. A. Creating a Dataset With IRT-Based Question Difﬁculty While several popular datasets have been developed for gen- eral reading comprehension QG tasks [1], the most popular is SQuAD [30], which consists of over 100 000 question–answer pairs from Wikipedia articles posed by crowdworkers. Speciﬁ- cally, SQuAD is a collection of triplets (r, q, a), where each an- swer a is a text fragment from a corresponding reading passage r and each reading passage r corresponds to a paragraph of a Wikipediaarticle.However,toconstructadifﬁculty-controllable QG method, we require a dataset consisting of quadruplets (r, q, a, b). Thus, we ﬁrst propose a method for extending the SQuAD dataset by appending the IRT-based difﬁculty values for each question–answer pair. The details for doing so are as follows. 1) Collecting response data for each question–answer pair: We collect answers from multiple respondents to each question in the SQuAD dataset and grade those answers as correct or incorrect based on exact matching with the corresponding true answers. Ideally, we should gather responses from a population of target learners, but this is highly expensive and time consuming. Thus, we\n\n\nPage 1\nwork is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n\n\nPage 11\n2196 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 Fig. 15. Trajectory in ability estimates for a QA system with ability −3.658 (Minimum θ). Fig. 16. Trajectory in ability estimates for a QA system with ability 1.244 (average θ). Fig. 17. Trajectory in ability estimates for a QA system with ability 2.766 (Maximum θ). 2) Random Generation: In total, 50 questions with random difﬁculty are generated and administered. The trajectories in estimated θ values obtained using these methods are presented in Figs. 15–17. The horizontal axis rep- resents the number of questions administered, and the vertical axis denotes the value of θ. The blue line signiﬁes the true value, whereas the orange and green lines represent the trajectory in estimated abilities due to adaptive and random question posing, respectively. Note that these graphs are constructed by averaging the results from 500 iterations of the experiment. The results show that the proposed adaptive strategy estimated the respon- dents’ true ability more efﬁciently than the random strategy. For further analysis, we conducted a performance evalua- tion targeting all the QA systems. Speciﬁcally, we ran the the aforementioned experiment 500 times for each of the 60 QA models and calculated the mean absolute error (MAE) between the true ability values and the estimated abilities after posing 50 questions, as given by MAE =\n\n\nPage 8\nis given by Fig. 9, which shows the correct answer ratio of the 60 QA systems for the generated question–answer pairs given b = 1.0. The x-axis represents the ability θ, the y-axis represents the correct answer ratio, each plot indicates an individual QA system, and the orange line repre- sents the logistic regression curve ﬁtted to the 60 data points by the least-squares method. As described before, the Rasch model gives a probability of 0.5 that a correct answer will be given when θ = b. To emphasize this point, the green dashed line represents θ = 1.0 and a correct answer ratio of 0.5. The ﬁgure shows that the QA systems with an ability level around θ = 1.0 demonstrate a correct answer ratio of approximately 0.5, suggesting that the questions are generated as expected. A similar trend was observed for other difﬁculty levels, as exempliﬁed in Figs. 10 Fig. 9. Average correct answer ratio of the QA systems for generated questions given difﬁculty b = 1.0. Each dot plot represents an individual QA system. Fig. 10. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b = 2.0. Each dot plot represents an individual QA system. Fig. 11. Average correct answer ratio of the QA systems for generated ques- tions given difﬁculty b = 0.0. Each dot plot represents an individual QA system. and 11, wh", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n**Primary Topics:**\n1. **Adaptive Question-Answer Generation (QG):** The study focuses on generating questions and answers that are tailored to specific difficulty levels using an adaptive model.\n2. **Item Response Theory (IRT):** The research employs IRT, particularly the Rasch model, to estimate learner abilities and question difficulties.\n3. **Evaluation Criteria:** The effectiveness of generated question-answer pairs is assessed based on fluency, content relevance, answerability, practicality, and question difficulty.\n\n**Goals:**\n1. **Develop a Method for Difficulty-Controlled QG:** The primary goal is to create a system that can generate question-answer pairs with specified difficulty levels that align with the learner's abilities.\n2. **Enhance Learning Outcomes:** By generating questions that match the learner's ability, the study aims to improve educational effectiveness and engagement.\n3. **Validate the Model:** The research seeks to demonstrate the model's capability through both machine-based and human evaluations, ensuring that generated questions are relevant and appropriately challenging.\n\n**Insights:**\n1. **Importance of Difficulty Matching:** The study highlights that questions should be generated to match a learner's ability level for optimal learning outcomes, ideally targeting", "updated_at": 1763990684.05024}, "14ce585be375": {"context": "Page 4\nand Wylie, 2014; Ruan et al., 2019). We detail these criteria in the rest of this subsection. 3.2.1 Answer Relevance Answer Relevance measures how directly related the answer is to the question in each QA pair in the dialogue. This criterion is crucial in education, as teachers should have subject-matter knowledge and can adapt responses to the student’s various knowledge levels (Lepper and Woolverton, 2002). We assess the Answer Relevance of individual QA pairs and then combine these assessments to de- termine the dialogue’s overall Answer Relevance. We use BF1(qt, at), QuestEval and Uptake as metrics for Answer Relevance. The BF1 is an ab- breviation for the BERTScore F1 (Zhang* et al., 2020) for semantic alignment between questions and answers using BERT embeddings. QuestE- val (Scialom et al., 2021) generates questions from both the question and answer, then generates an- swers for these questions, comparing them to mea- sure relevance. The Uptake metric (Demszky et al., 2021), specific to the education domain, analyzes teachers’ responses to student utterances, focusing on their dependence and relevance. More details are in Table 5. 3.2.2 Coherence of the Dialogue Coherence measures whether QA pairs in the dia- logue form a logical and smooth whole, rather than independent QA pairs. Coherence is an important aspect of good dialogue (Dziri et al., 2019) and it is important\n\n\nPage 2\ncriteria shown in Figure 1. Book2Dial comprises of three approaches taken from prior work: multi-turn QG-QA (Kim et al., 2022), Dialogue Inpainting (Dai et al., 2022) and prompting LLMs to role-play teacher and stu- dent. We use the formatting information in the text- book, such as titles or key concepts, to initialize student models with imperfect information. In con- trast, the teacher models have perfect information and are expected to generate grounded responses based on the textbook. We fine-tune and prompt various open-source language models to generate teacher-student interactions. We evaluate Book2Dial on the proposed qual- ity criteria and use human evaluations to support our findings. Results reveal that data generated by role-playing LLMs scores highest in most criteria, as shown in Section 5.1.1 and 5.1.2, demonstrat- ing reasonable efficacy in creating educational dia- logues. The generated dialogues effectively incor- porate textbook content but fall short in mimicking the natural scaffolding of educational conversations and exhibit issues like hallucinations and repeti- tion, as discussed in Section 5.3. Despite these limitations, we show that the generated synthetic data may be used to fine-tune educational chatbots with benefits in some educational domains (Sec- tion 5.4), especially when the domain of the evalu- ation dataset matches with the fine-tuning\n\n\nPage 5\nQG-QA models This scenario utilizes separate QG and QA models to interact in a multi-turn scenario. As a represen- tation of this approach from related work, we con- sider the SimSeek-asym model (Kim et al., 2022). The approach consists of two components: 1. A Question Generation (QG) model for gen- erating conversational questions relying solely on prior information (i.e., formatting informa- tion relevant to the topic). This student model generates a question based on the dialogue his- tory and filtered Information C: p(qt|C, h<t). 2. A Conversational Answer Finder (CAF) to comprehend the generated question and provide an acceptable teacher answer to the question from the evidence passage: p(at|S, h<t, qt). 4.2 Dialogue Inpainting Dialogue Inpainting (Dai et al., 2022) is an ap- proach for dialogue generation characterized by its information-symmetric setting. In this framework, both the student and teacher model are provided with the complete textbook text S. The teacher model iterates over each sentence in S and copies it as an answer. The student model is a QG model. We use data from the OR-QuAC (Qu et al., 2020), QReCC (Anantha et al., 2021), and Taskmaster-2 (Byrne et al., 2020) datasets to train the student model. For the student model, a dialogue recon- struction task is employed. At training time rather than distinguishing questions and answers, the dia- logue\n\n\nPage 7\nscores only 0.79, which indicates that approximately 21% of the QA pairs lack Factual Consistency. For edu- cational dialogues, it’s imperative to aim for high Factual Consistency to ensure the reliability of the knowledge imparted. The primary reason for this issue is the hallucination in LLMs, where LLMs respond to questions using fabricated or false infor- mation not grounded in the textbook. This poses a significant challenge and calls for further research 9713\n\n\nPage 5\nreconstruction task treats a conversation as a sequence of utterances {ui}2T i=1, To train it, a ran- domly chosen utterance ui is masked to create a 9711\n\n\nPage 8\nin Answer Relevance, Coherence, Factual Consistency, Answerability, and Specificity, while Dialogue Inpainting generated dialogues excel in Informativeness and Groundedness. into ways to better ground LLMs to text documents in the future. 5.3 Qualitative Human Analysis We inspect dialogues generated by each model and find several limitations compared to natural educa- tional conversations. We report qualitative descrip- tions of these findings: Insufficient follow-up ability of Persona models Even though Persona models outperformed other methods in automatic and human evaluations, the model has several issues. Dialogues generated by Persona models are unlike natural conversations and resemble a series of artificial QA pairs about textbooks. Moreover, the dialogue does not have enough follow-up questions by students and rather broadly touches on the textbook content rather than going into depth about certain topics. We present an example of a dialogue demonstrating the insuffi- cient follow-up ability of the Persona-based model in table 17. Repeating answers in SimSeek and Persona In the SimSeek and Persona datasets, we find that teacher answers often reiterate information from previous interactions. SimSeek often generates questions related to the previous teacher’s answer, while Persona often provides summaries of text- book content in each answer. Insufficient Specificity of\n\n\nPage 17\nreward them with non-monetary gifts for each annotator. As all annotators are satisfied with this payment, we consider this as adequate. To alleviate the burden on participants, we select 3 models from each method category for evaluation. To ensure the consistency of results across different domains, we choose datasets from four textbooks, each covering a different subject area: mathematics, business, science, and social sciences. From each textbook, we randomly select a subsection. For each subsection, we generate one dialogue using a different method, preparing each dialogue sepa- rately for evaluation. We use only the first 12 turns (6 QA pairs) of each dialogue for evaluation, simi- lar to what is described in Section 5.1.2. During the 9723\n\n\nPage 5\nits alignment with human evaluation, as detailed in Appendix E. The final score is the weighted sum of two similarity scores. More details are in Table 5 and Appendix C.4. 3.2.7 Specificity of the Question Specificity assesses whether the question is specific, rather than general. An example of a generic ques- tion is ‘What is interesting about this passage?’. The Specificity criterion is crucial in education, as teachers should ask specific questions (Yang, 2017). We assess specificity through human evaluation, as there is no existing metric that captures specificity. 4 From Textbooks to Dialogues In this section, we describe different methods used for generating dialogues from educational text- books in Book2Dial, namely: 1https://huggingface.co/distilbert-base-cased-distilled- squad 1. Multi-turn QG-QA models. In this setting, we use fine-tuned QG and QA models inter- acting with each other. 2. Dialogue Inpainting. Dai et al. (2021) uses a span extraction model over the textbook as a teacher model, where the response is copied from the textbook and the question is gener- ated by a QG model acting as the student. 3. Persona-based Generation. This approach uses LLMs like GPT-3.5, and leverages prompting to interactively simulate the stu- dent and the teacher and generate dialogues. We describe these methods next. More imple- mentation details are in Appendix C. 4.1 Multi-turn\n\n\nPage 1\nwith student (Chi and Wylie, 2014). This differs from techniques focusing on teaching using scaffolding (Macina et al., 2023a) or Socratic questioning (Shridhar et al., 2022) which leads to deeper and wider human learning. How- ever, the task of generating high-quality synthetic dialogue data is difficult (Dai et al., 2022). This is amplified in education, where the generated in- teractions should cover the teaching material in an informative and coherent way (Agrawal et al., 2012; Crosby, 2000). Therefore, it is important to have quality controls on such data, because students might otherwise receive wrong feedback, which could be detrimental to learning. Thus, in this work, we also formulate some cri- teria that measure the quality of educational dia- logues. For example, it is crucial that the chat- bot does not provide students with incorrect infor- mation and stays grounded in the textbook, ensur- ing factual consistency with the knowledge taught. This is particularly important given that large lan- guage models (LLMs) are prone to ‘hallucinations’ or generating plausible but incorrect or unverified information (Ji et al., 2023). While a simple teacher strategy would be to answer with extracted pas- sages from the textbook, this might hurt the coher- ence of the dialogue which is present in interactive educational situations (Baker et al., 2021). The teacher’s response\n\n\nPage 10\nfocus on other interaction scenarios and combine our approach with Socratic questioning (Shridhar et al., 2022) and scaffolding (Macina et al., 2023b; Sonkar et al., 2023) to achieve significantly im- proved applicability to educational use cases. Achieving the highest scores in all metrics is not the overall goal leading to the most effective human learning. Considering Informativeness, while a dialogue rich in information suggests a po- tential for a greater extent of learning by a student, there exists a trade-off, as excessive information can increase the student’s cognitive load and be- come overwhelming (Mayer and Moreno, 2003). Therefore, finding the optimal amount of informa- tion that the dialogue should contain needs careful consideration in future work. Similarly, for other metrics, educational practitioners could ideally set the target metrics and their combination for achiev- ing better fine-tuning performance steered towards educational use cases. Aspects of evaluation framework: Although we try to include various aspects of the evaluation in this work, it is not feasible to focus on all impor- tant educational aspects. We specifically focus on one setting, where students ask curious questions and the teacher provides answers. Therefore, com- paring our datasets with the MathDial, QuAC, and NCTE datasets does not fully explain our datasets’ quality, as MathDial,\n\n\nPage 13\n1941–1961, Singapore. Association for Computational Linguistics. Katherine Stasaski, Kimberly Kao, and Marti A Hearst. 2020. Cima: A large open access dialogue dataset for tutoring. In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–64. Katherine Stasaski, Manav Rathod, Tony Tu, Yunfang Xiao, and Marti A Hearst. 2021. Automatically gen- erating cause-and-effect questions from passages. In Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 158–170. Abhijit Suresh, Jennifer Jacobs, Margaret Perkoff, James H. Martin, and Tamara Sumner. 2022. Fine- tuning transformers with additional context to clas- sify discursive m", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Answer Relevance**: Evaluates how closely answers relate to questions in educational dialogues, using metrics like BF1, QuestEval, and Uptake.\n2. **Coherence of Dialogue**: Assesses whether QA pairs form a logical and smooth conversation rather than isolated exchanges.\n3. **Factual Consistency**: Ensures that generated answers are accurate and grounded in the provided educational material, addressing the issue of hallucinations in language models.\n4. **Specificity of Questions**: Measures how specific questions are, emphasizing the importance of targeted inquiries in educational contexts.\n5. **Dialogue Generation Methods**: Discusses various approaches for generating educational dialogues, including multi-turn QG-QA models, Dialogue Inpainting, and persona-based generation.\n\n#### Goals:\n- To create high-quality synthetic dialogue data for educational purposes that is informative, coherent, and factually consistent.\n- To evaluate the effectiveness of different dialogue generation methods in producing realistic and educationally valuable interactions.\n- To identify and address limitations in current models, such as issues with coherence, follow-up questions, and specificity.\n\n#### Insights:\n- The generated dialogues often fail to mimic natural educational conversations, leading", "updated_at": 1763990690.313998}, "ec4876c149d4": {"context": "Page 1\ngenerated questions for each difficulty level. The average correct answer ratio of the 60 QA systems, which were those trained in procedure (1) of Section VII-A, for each difficulty level is illustrated in Fig. 8. The x-axis represents the specified difficulty levels, and the y-axis represents the average correct answer ratio. The figure indicates that as the specified level of difficulty increases, the average correct answer ratio of the QA systems for the generated questions tends to decrease. This suggests that the proposed method for generating questions successfully reflects the specified levels of difficulty. Further detailed analysis is given by Fig. 9, which shows the correct answer ratio of the 60 QA systems for the generated question–answer pairs given b = 1.0. The x-axis represents the ability θ, the y-axis represents the correct answer ratio, each plot indicates an individual QA system, and the orange line represents the logistic regression curve fitted to the 60 data points by the least-squares method. As described before, the Rasch model gives a probability of 0.5 that a correct answer will be given when θ = b. To emphasize this point, the green dashed line represents θ = 1.0 and a correct answer ratio of 0.5. The figure shows that the QA systems with an ability level around θ = 1.0 demonstrate a correct answer ratio of approximately 0.5, suggesting that the\n\n\nPage 1\nhuman evaluations. (train) We now summarize the basic statistics of the datasets Db (eval) and Db , which we developed in the aforementioned procedure (5) to train and evaluate our difficulty-controllable QG (train) and method. First, the number of reading passages in Db (eval) Db was 1860 and 207, respectively. Next, the average number (train) (eval) and Db was 5.21 of questions per reading passage in Db and 4.28. Furthermore, Fig. 7 is a histogram of the difficulty TOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL Fig. 8. Average correct answer ratio for each difficulty level. 2193 Fig. 9. Average correct answer ratio of the QA systems for generated questions given difficulty b = 1.0. Each dot plot represents an individual QA system. values in each dataset. From these results, we can confirm that the difficulty distributions are similar between the two datasets, (train) (eval) and Db indicating that Db was randomly divided into Db without bias. B. Automatic Evaluation of Difficulty-Controllable QG Quality We first evaluated the generated questions and answers in the aforementioned experimental procedure (8) from the following three perspectives: 1) the average correct answer ratio of QA systems for each difficulty level; 2) the average word count of the answers for each difficulty level; 3) the frequency of leading interrogative words in the\n\n\nPage 1\ntheir responses to previously administered questions. 2) Selecting an appropriate difficulty level for the estimated ability and generating questions using a difficultycontrollable QG method by specifying that difficulty. For this cyclic task, we propose an efficient strategy grounded in the IRT and CAT. IV. TECHNICAL DETAILS AND LIMITATIONS OF RELATED WORKS In this study, we employ deep neural networks as the foundational technology for enabling automated QG for reading comprehension tasks (e.g., [13], [14], and [32]). In this section, we provide an overview of conventional neural methods for generating reading comprehension questions and conventional difficulty-controllable QG methods. Table I summarizes the characteristics and limitations of related works introduced as follows. B. Difficulty-Controllable Neural QG for Reading Comprehension When utilizing QG methods as a learning aid to foster reading comprehension skills, it is critical to be able to generate questions with arbitrary difficulty levels [44]. Accordingly, several recent studies have proposed difficulty-controllable QG methods [22], [28]. For example, Gao et al. [22] proposed an RNN-based seq2seq model that generates reading comprehension questions for difficulty levels categorized as either “easy” or “hard.” They also proposed to construct training data for the difficulty-controllable QG task based on a\n\n\nPage 1\nSchool of Informatics and Engineering, University of Electro-Communications, Chofu 182-8585, Japan (e-mail: tomikawa@ai.lab.uec.ac.jp; suzuki_ayaka@ai.lab.uec.ac.jp; uto@ai. lab.uec.ac.jp). Digital Object Identifier 10.1109/TLT.2024.3491801 in various educational applications, including intelligent tutoring systems, writing-assistance tools, and knowledge-evaluation platforms [1], [2], [3], [4], [5], [6], [7]. Early QG methods were based on rule-based or templatebased approaches, which generated questions by converting declarative texts into interrogative questions, using handcrafted rules or templates [1], [8], [9], [10], [11]. However, preparing well-designed rules and templates for specific applications is time consuming and labor-intensive [1], [12]. To address this limitation, end-to-end QG methods based on deep neural networks have been used [1], [13], [14], [15], [16], [17], [18], [19], [20]. Initial neural QG methods were designed as sequenceto-sequence (seq2seq) models based on recurrent neural networks (RNNs) and attention mechanisms [14], [21], whereas more recent methods have employed pretrained transformer models [2], [5], [13], [15], [22], [23], such as bidirectional encoder representations from transformers (BERT) [24], generative pretrained transformer 2 (GPT-2) [25], bidirectional and autoregressive transformers (BART) [26], and text-to-text transfer\n\n\nPage 1\npsychological measurement, Bayesian statistics, machine learning, and natural language processing. Dr. Uto was the recipient of the Best Paper RunnerUp Award at the 2020 International Conference on Artificial Intelligence in Education.\n\n\nPage 1\nbased on the expected a posteriori estimation [51], [52]. B. Computerized Adaptive Testing (CAT) As explained earlier, the IRT is often used as a basis for CAT, which adaptively administers items appropriate for each examinee while sequentially estimating the examinee’s ability from their response history. Specifically, CAT based on the Rasch model begins by initializing the ability of a target examinee, then it generally selects an item with a difficulty level that maximizes Fisher information [31]. The ability estimate is then updated with a correct or incorrect response to the offered item. By repeating these procedures, CAT offers optimal items while efficiently estimating an examinee’s ability. 2190 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 TABLE II CONFIGURATION OF BERT AND T5 USED AS THE BASIS MODELS IN THIS STUDY3 The reason for using the Fisher information for item selection is that the variance of the maximum likelihood estimate of ability converges to the inverse of Fisher information, meaning that an item with the maximum Fisher information is the most effective for accurately estimating the examinee’s ability. In the Rasch model, Fisher information is maximized when the item difficulty parameter b equals the ability value θ. Repeating the ability estimation and the Fisher-information-based item selection enables us to achieve a highly accurate\n\n\nPage 1\nitems. The IRT has been widely used in various educational and psychological tests because of its numerous benefits, such as offering accurate estimates of examinee’s ability and item characteristics, unifying measurement scales among different tests, and facilitating CAT applications [45], [46], [47], [48]. This study uses the Rasch model [49], the most traditional and well-known IRT model, to quantify question difficulty. A. Rasch Model The Rasch model defines the probability that the mth examinee correctly answers the nth item as pnm = exp(θm − bn ) 1 + exp(θm − bn ) (1) where bn denotes the difficulty of the nth item, and θm denotes the latent ability of the mth examinee. For a detailed explanation of this model, Fig. 3 illustrates item response curves (IRCs) for the Rasch model, which are plots of 1 In this context, “latent” indicates that the corresponding factors are not directly observable but lie behind the observed data. 2 “Discriminative power” means the degree to which an item can distinguish between examinees with different levels of the underlying ability. Fig. 3. 2189 IRCs for a Rasch model with different item difficulty values. the probability p. In this figure, the horizontal axis represents the ability θ; the vertical axis represents the probability p; and the three solid curves represent the IRCs for three items with different difficulty levels, b = −1.0,\n\n\nPage 1\nability estimate with fewer items administered [31]. VI. PROPOSED METHOD Based on conventional QG methods and IRT approach, our difficulty-controllable QG method is carried out by performing the following two tasks in sequence. 1) Difficulty-controllable answer extraction, which extracts an answer text from a given reading passage while considering a target IRT-based difficulty value. 2) Difficulty-controllable answer-aware QG, which generates a question given a reading passage, an answer text, and a target IRT-based difficulty value. Furthermore, within the framework of CAT, we propose a methodology for sequentially estimating a learner’s ability and adaptively generating questions with appropriate difficulty levels for their ability. Because our difficulty-controllable QG method requires a dataset with IRT-based difficulty values for model training, we first propose a method for constructing it in the next section. A. Creating a Dataset With IRT-Based Question Difficulty While several popular datasets have been developed for general reading comprehension QG tasks [1], the most popular is SQuAD [30], which consists of over 100 000 question–answer pairs from Wikipedia articles posed by crowdworkers. Specifically, SQuAD is a collection of triplets (r, q, a), where each answer a is a text fragment from a corresponding reading passage r and each reading passage r corresponds to a\n\n\nPage 1\nlearner’s abilities. © 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ TOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2187 framework of computerized adaptive testing (CAT) [31], a wellknown test administration method. This method repeats a cycle of sequentially presenting questions of a difficulty level suited to a learner’s ability and estimating their ability from their responses. We expect this method to enable efficient ability estimation while tuning the difficulty levels of provided questions. To our knowledge, this is the first QG method that enables difficulty control aimed at generating question–answer pairs quantified with IRT-based difficulty. II. RESEARCH QUESTIONS AND CONTRIBUTIONS Fig. 1. Conventional QG task: answer-aware QG. 1) The relationship between the difficulty of the questions and the learner’s ability is ignored, making it difficult to determine the appropriate difficulty for each learner. 2) The methods are answer-aware, which means a reading passage and an answer text must be input to generate questions, as shown in Fig. 1. For this reason, they cannot generate question–answer pairs. Generating answers along with questions is essential to realize automatic assessment of learners’ responses to generated\n\n\nPage 1\nstatistically significant correlation at the 1% level. These results suggest that the proposed method can generate questions in alignment with the difficulty levels perceived by human respondents. VIII. EVALUATION OF THE ACCURACY OF LEARNERS’ ABILITY ESTIMATION In this section, we evaluate the efficacy of the method of adaptive QG proposed in Section V-B. We simulated the adaptive QG process for three respondents with different abilities. For the respondents, we used three QA models with the minimum and maximum θ, s", "summary": "### Summary of Primary Topics, Goals, and Insights\n\n#### Primary Topics:\n1. **Difficulty-Controllable Question Generation (QG)**: The document discusses a method for generating questions with varying difficulty levels, utilizing deep neural networks and the Item Response Theory (IRT).\n2. **Evaluation of QA Systems**: It presents statistical analyses of QA systems' performance based on generated questions, highlighting the correlation between question difficulty and correct answer ratios.\n3. **Computerized Adaptive Testing (CAT)**: The integration of CAT principles into the QG process, allowing for adaptive question selection based on learner ability.\n\n#### Goals:\n1. **To Develop a QG Method**: Create a method that generates questions tailored to the learner's ability, facilitating personalized learning experiences.\n2. **To Validate the Method**: Assess the effectiveness of the proposed QG method through empirical evaluations, ensuring that generated questions align with specified difficulty levels.\n3. **To Enhance Learning Outcomes**: Improve reading comprehension and assessment through adaptive question generation that responds to individual learner needs.\n\n#### Insights:\n1. **Inverse Relationship Between Difficulty and Correct Answers**: As question difficulty increases, the average correct answer ratio decreases, indicating that the method effectively generates questions that match intended difficulty levels.\n2", "updated_at": 1763990695.302119}}