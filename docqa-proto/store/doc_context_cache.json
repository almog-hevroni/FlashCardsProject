{"63c15d67fc21": {"context": "Page 12\nAssoc. Comput. Linguistics, 2020, pp. 7871–7880. [27] C. Raffel and et al., “Exploring the limits of transfer learning with a uniﬁed text-to-text transformer,” J. Mach. Learn. Res., vol. 21, no. 140, pp. 1–67, Jan. 2020. [28] Y. Cheng et al., “Guiding the growth: Difﬁculty-controllable question generation through step-by-step rewriting,” in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics/11th Int. Joint Conf. Natural Lang. Process., 2021, pp. 5968–5978. [29] F. M. Lord, Applications of Item Response Theory to Practical Testing Problems. Evanston, IL, USA: Routledge, 1980. [30] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000 questions for machine comprehension of text,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2016, pp. 2383–2392. [31] W. J. van der Linden and C. A. Glas, Elements of Adaptive Testing. New York, NY, USA: Springer, 2010. [32] Q. Zhou, N. Yang, F. Wei, C. Tan, H. Bao, and M. Zhou, “Neural question generation from text: A preliminary study,” in Proc. Nat. CCF Conf. Natural Lang. Process. Chin. Comput., 2018, pp. 662–671. [33] A. Vaswani et al., “Attention is all you need,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2017. [34] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, “Xlnet: Generalized autoregressive pretraining for language understand- ing,” in Proc. Neural Inf. Process. Syst., 2019. [35]\n\n\nPage 6\nlearner’s ability would be effective for enhancing reading comprehensive skills. However, in typical educational settings, a learner’s ability is often unknown a priori. To address this, we are proposing an efﬁcient method for estimating a learner’s ability. We utilize the CAT framework, as mentioned in Section V-B, while generating and administering questions at appropriate levels of difﬁculty. Speciﬁcally, we propose the following procedure for ability estimation and QG. 1) Randomly generate and administer a few questions to the learner to obtain initial response data. 2) Utilize the obtained response data along with the Rasch model to estimate and update the learner’s ability level. 3) Generate and administer a question at the estimated ability level using the proposed QG method, thereby obtaining new response data. 4) Repeat steps (2) and (3). Using this procedure, we can effectively estimate a learner’s ability while administering questions at appropriate levels of difﬁculty. Remember that when the ability and difﬁculty levels are equal, the Rasch model produces a 0.5 probability that a correct answer will be obtained, and Fischer information is maximized.\n\n\nPage 2\ngenerating question–answer pairs quantiﬁed with IRT-based difﬁculty. II. RESEARCH QUESTIONS AND CONTRIBUTIONS The research questions of this study are summarized as follows. 1) RQ1: Is it possible to generate question–answer pairs by specifying arbitrary IRT-based difﬁculty values? 2) RQ2: Can the adaptive QG strategy, based on the CAT framework, enhance the selection of more suitable difﬁ- culty levels for each learner through its efﬁcient ability estimation? The proposed method has the potential to make the following contributions, which would be beneﬁcial in various educational applications. 1) It enhances learning efﬁciency in intelligent tutoring sys- tems by adaptively providing questions and answers at difﬁculty levels appropriate for each learner. 2) The CAT-inspired adaptive QG framework enables more efﬁcient measurement of learners’ abilities compared with question presentations that are not difﬁculty-aware. 3) It can assist in the expansion of item banks, a task often crucial for managing standardized tests, although this aspect is not the primary focus of this study. III. RESEARCH OBJECTIVE AND TASK DEFINITION As discussed in the aforementioned sections, our ﬁrst research objective is to develop a method that generates reading compre- hensionquestionsalongwiththeircorrespondinganswers,based on the given reading passages and speciﬁed difﬁculty levels, as illustrated\n\n\nPage 2\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2187 Fig. 1. Conventional QG task: answer-aware QG. 1) The relationship between the difﬁculty of the questions and the learner’s ability is ignored, making it difﬁcult to determine the appropriate difﬁculty for each learner. 2) The methods are answer-aware, which means a reading passage and an answer text must be input to generate questions, as shown in Fig. 1. For this reason, they can- not generate question–answer pairs. Generating answers along with questions is essential to realize automatic assessment of learners’ responses to generated questions. Furthermore, controlling the difﬁculty of the generated answers is also crucial because both questions and answers generally affect the overall difﬁculty. To address these issues, we introduce a novel method for gen- erating question–answer pairs while considering the difﬁculty associated with the learners’ ability. A unique feature of our method is the use of item response theory (IRT) [29] to quantify the question difﬁculty. The IRT is based on statistical models that deﬁne the relationship between question difﬁculty and learner ability, thereby facilitating the selection of an appropriate difﬁculty level for each learner. For this reason, our method is designed to generate question–answer pairs while controlling IRT-based difﬁculty.\n\n\nPage 1\n2186 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 Adaptive Question–Answer Generation With Difﬁculty Control Using Item Response Theory and Pretrained Transformer Models Yuto Tomikawa , Ayaka Suzuki, and Masaki Uto Abstract—The automatic generation of reading comprehension questions, referred to as question generation (QG), is attracting attention in the ﬁeld of education. To achieve efﬁcient educational applications of QG methods, it is desirable to generate questions with difﬁculty levels that are appropriate for each learner’s reading ability. Therefore, in recent years, several difﬁculty-controllable QG methods have been proposed. However, conventional meth- ods generate only questions and cannot produce question–answer pairs. Furthermore, such methods ignore the relationship between question difﬁculty and learner ability, making it challenging to as- certain the appropriate difﬁculty levels for each learner. To address these issues, in this article, we propose a method for generating question–answerpairsbasedondifﬁculty,deﬁnedusingastatistical model known as item response theory. The proposed difﬁculty- controllable generation is achieved by extending two pretrained transformer models: bidirectional encoder representations from transformers and text-to-text transfer transformer. In addition, because learners’ abilities are generally not knowable in advance,\n\n\nPage 13\n2198 IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, VOL. 17, 2024 [45] M. Uto and M. Ueno, “A generalized many-facet Rasch model and its Bayesian estimation using Hamiltonian Monte Carlo,” Behaviormetrika, vol. 47, pp. 469–496, May 2020. [46] M. Uto and M. Ueno, “Empirical comparison of item response theory mod- els with Rater’s parameters,” Heliyon, vol. 4, no. 5, pp. 1–32 May 2018. [47] M. Uto, “A Bayesian many-facet Rasch model with Markov modeling for rater severity drift,” Behav. Res. Methods, vol. 55, pp. 3910–3928, Oct. 2022. [48] M. Uto, “A multidimensional generalized many-facet Rasch model for rubric-based performance assessment,” Behaviormetrika, vol. 48, pp. 423–457, Jul. 2021. [49] G.Rasch,ProbabilisticModelsforSomeIntelligenceandAttainmentTests. Chicago, IL, USA: Univ. Chicago Press, 1981. [50] S.-H. K. Frank and B. Baker, Item Response Theory: Parameter Estimation Techniques. Boca Raton, FL, USA: CRC Press, 2004. [51] J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications. New York, NY, USA: Springer, 2010. [52] M. Uto, I. Aomi, E. Tsutsumi, and M. Ueno, “Integration of prediction scores from various automated essay scoring models using item response theory,” IEEE Trans. Learn. Technol., vol. 16, no. 6, pp. 983–1000, Dec. 2023. [53] A. Srikanth, A. S. Umasankar, S. Thanu, and S. J. Nirmala, “Extractive text summarization using dynamic clustering and\n\n\nPage 7\nanswer. 3) Utilizing the collected binary response data, we employed theRaschmodel,expressedin(1),toestimatethedifﬁculty level for each question in D(eval) and the ability for each of the QA systems. 4The rationale behind our use of 60 QA systems is based on a previous report [61], which stated that the minimum requirement of respondents for estimating difﬁculty based on the Rasch model is 30. However, a larger number of respondents generally contributes to improving the stability of difﬁculty estimation. Thus, considering the tradeoff between computational cost and estimation stability, this experiment used 60 QA systems. 5[Online]. Available: https://huggingface.co/ Fig. 7. Number of questions corresponding to the difﬁculty values in D(train) b and D(eval) b . 4) We then integrated the estimated question difﬁculty with D(eval) to produce a new dataset, Db. The real-valued difﬁ- culty estimates were rounded to the second decimal place to facilitate processing by the language models BERT and T5. 5) The created dataset, Db, was further divided into two partitions that were 90% and 10%, denoted as D(train) b and D(eval) b , respectively. 6) We ﬁrst ﬁne-tuned both the answer-extraction model and the QG model using the original SQuAD training dataset, D(train), without consideringthedifﬁcultylevels. Although this step is not mandatory, we expect that pretraining these models on a\n\n\nPage 8\nTOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2193 Fig. 8. Average correct answer ratio for each difﬁculty level. values in each dataset. From these results, we can conﬁrm that the difﬁculty distributions are similar between the two datasets, indicating that Db was randomly divided into D(train) b and D(eval) b without bias. B. Automatic Evaluation of Difﬁculty-Controllable QG Quality We ﬁrst evaluated the generated questions and answers in the aforementioned experimental procedure (8) from the following three perspectives: 1) the average correct answer ratio of QA systems for each difﬁculty level; 2) the average word count of the answers for each difﬁculty level; 3) the frequency of leading interrogative words in the gener- ated questions for each difﬁculty level. The average correct answer ratio of the 60 QA systems, which were those trained in procedure (1) of Section VII-A, for each difﬁculty level is illustrated in Fig. 8. The x-axis represents the speciﬁed difﬁculty levels, and the y-axis represents the average correct answer ratio. The ﬁgure indicates that as the speciﬁed level of difﬁculty increases, the average correct answer ratio of the QA systems for the generated questions tends to decrease. This suggests that the proposed method for generating questions successfully reﬂects the speciﬁed levels of difﬁculty. Further detailed analysis\n\n\nPage 8\nis given by Fig. 9, which shows the correct answer ratio of the 60 QA systems for the generated question–answer pairs given b = 1.0. The x-axis represents the ability θ, the y-axis represents the correct answer ratio, each plot indicates an individual QA system, and the orange line repre- sents the logistic regression curve ﬁtted to the 60 data points by the least-squares method. As described before, the Rasch model gives a probability of 0.5 that a correct answer will be given when θ = b. To emphasize this point, the green dashed line represents θ = 1.0 and a correct answer ratio of 0.5. The ﬁgure shows that the QA systems with an ability level around θ = 1.0 demonstrate a correct answer ratio of approximately 0.5, suggesting that the questions are generated as expected. A similar trend was observed for other difﬁculty levels, as exempliﬁed in Figs. 10 Fig. 9. Average correct answer ratio of the QA systems for generated questions given difﬁculty b ", "summary": "The document discusses the development of an adaptive question-answer generation (QG) system that controls difficulty using Item Response Theory (IRT) and pretrained transformer models. The primary topics and goals include:\n\n1. **Adaptive QG System**: The study proposes a method for generating question-answer pairs with difficulty levels tailored to individual learners' abilities. This is achieved by extending transformer models like BERT and T5.\n\n2. **Difficulty Control**: The system uses IRT to quantify question difficulty, ensuring that questions are appropriate for each learner's ability. This addresses the limitations of conventional QG methods that do not consider the relationship between question difficulty and learner ability.\n\n3. **Research Questions**: The study explores whether it is possible to generate question-answer pairs with specified IRT-based difficulty values and whether an adaptive QG strategy can enhance the selection of suitable difficulty levels for learners.\n\n4. **Educational Applications**: The method aims to improve learning efficiency in intelligent tutoring systems by adaptively providing questions at appropriate difficulty levels. It also facilitates the efficient measurement of learners' abilities and can assist in expanding item banks for standardized tests.\n\n5. **Methodology**: The proposed system estimates a learner's ability using the CAT framework and the Rasch model. It", "updated_at": 1763988498.074223}, "ba440750cdf4": {"context": "Page 9\ndata, may be effective for fine-tuning chatbots if they align with the target domain. Upon a more qualitative human examination of the generated results, we find that the fine-tuned models on synthetic textbook data have a better understanding of the input context and generate more correct answers than the corresponding non- fine-tuned models. Some example generations are shown in Appendix G.1. 6 Conclusion This work introduces a new task of generating conversational question-answering dialogues from textbooks to help fine-tune educational chatbots in various domains where high-quality dialogue data are scarce. We detail and compare various approaches and settings to simulate student-teacher interactions and create such data. We evaluate the generated dialogues, focusing on some measures of their quality, such as Answer Relevance, Informa- tiveness, Coherence, and Factual Consistency. Our results indicate that the approach with LLMs role- CoQA (MCTest) CoQA (CNN) NCTE Math 26.10 (+3.96) 13.95 (+0.82) 8.79 (+0.39) Business 18.91 (-3.23) 13.29 (+0.16) 8.99 (+0.59) Science 22.36 (+0.22) 14.96 (+1.83) 8.73 (+0.33) Social 26.30 (+4.16) 15.11 (+1.98) 8.37 (-0.03) All 23.05 (+0.91) 14.31 (+1.18) 8.41 (+0.01) UltraChat 25.47 (+3.33) 14.89 (+1.76) 8.74 (+0.34) 0-Shot 3.96 (-18.18) 3.67 (-9.46) 1.08 (-7.32) Table 4: Downstream Task Results. We use dialogues generated from one textbook\n\n\nPage 9\nWe fine-tune the base model on four textbook- based synthetic datasets, each from a different sub- ject: math, business, science, and social science. The datasets and training details are shown in Ap- pendix G. The results are shown in Table 4. We report the BLEU score for the scenario where we fine-tune the base model on our textbook-generated dialogue dataset, with the difference between this fine-tuned version and the version without fine- tuning shown in brackets. We use two baselines: Zero-shot (0-Shot), which directly uses the FLAN- T5-LARGE model without any fine-tuning, and UltraChat, which is fine-tuned on the synthetic but non-educational UltraChat dataset (Ding et al., 2023) and tested on different target datasets. We find that the model that is first fine-tuned on the social science textbook data achieves the highest score when tested on the MCTest and CNN splits of the CoQA dataset, with improvements of 4.16 and 1.99. Meanwhile, the model fine-tuned on the business textbook data achieves the high- est score when tested on the NCTE dataset. The model fine-tuned on the math textbook data also shows improvements. As the social science text- book dataset contains the fewest math expressions, it improves the most in non-math domains but per- forms the worst in the math domain. We conclude that synthetic datasets created from textbooks, as well as Ultrachat synthetic\n\n\nPage 17\nMethods Student’s Model Teacher’s Model Inpute to Student Input to Teacher SimSeek T5 Longformer Title + Summary Contents+Formatting Dialog Inpainting FLAN-T5 COPY Contents + Format- ting Persona (Low Info) GPT-3.5 GPT-3.5 Title Persona (Medium Info) Title + Summary Persona (High Info) Formatting Persona (Single Instance) Contents + Format- ting Table 7: Summary of methods used in this work showing details of different types of student models and teacher models of each generation method and the detail of corresponding input. of 0.38 with Factual Consistency in human evalua- tion, both with p-values below 0.005. We interpret this as indicative of a moderate correlation, sug- gesting that this metric can approximate factual consistency to a certain extent. When comparing the correlation results with existing methods, in- cluding the use of GPT-3.5 scores derived from prompts, QuestEval, and QrelScore, the findings indicate that QFactScore’s correlation score sur- passes others. However, Factual Consistency is a nuanced criterion that necessitates an assessment of whether the answer accurately addresses the question within the given context. Existing metrics struggle with this task, highlighting the need for more comprehensive evaluations in the future. Correlation P-Value 1 - Overlap(at,a<t) vs Informativeness 0.81 0.002 1 - BF1(at,a<t) vs Informativeness 0.69 0.01 QFactScore vs\n\n\nPage 13\n2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 con- ference on empirical methods in natural language processing: system demonstrations, pages 38–45. Hao Yang. 2017. A research on the effective questioning strategies in class. Science Journal of education, 5(4):158–163. Lichao Zhang, Abel Gonzalez-Garcia, Joost Van De Weijer, Martin Danelljan, and Fahad Shahbaz Khan. 2018. Synthetic data generation for end-to- end thermal infrared tracking. IEEE Transactions on Image Processing, 28(4):1837–1850. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with bert. In International Conference on Learning Representations. 9719\n\n\nPage 18\nEvaluators rate each question-answer (QA) pair within a dialogue based on eight criteria. The overall evaluation score for a dialogue is determined by averaging the scores of all its QA pairs. The specific evaluation criterion and corresponding questions are detailed in Table 12. Participants respond to each question with “yes” or “no”. The “yes” is coded as a score of 1, while the “no” is coded as a score of 0. We provide the specific question the participants will be asked during human evaluation as shown in Table 12. The task is straightforward, we provide QA pairs for evaluation in an Excel file and the annotators just read the QA pair and give a score based on their judgement of each question. The overall Fleiss’ kappa score for the annota- tions of the four annotators is 0.744. We further show the Cohen’s Kappa score between each par- ticipant in Table 13, which proves that each pair of participants has a substantial agreement. F.2 Disclaimer for Annotators Thank you for participating in our evaluation pro- cess. Please read the following important points before you begin: • Voluntary Participation: Your participation is completely voluntary. You have the free- dom to withdraw from the task at any time without any consequences. • Confidentiality: All data you will be work- ing with is anonymized and does not contain any personal information. Your responses and scores\n\n\nPage 22\nMeanwhile, the prediction with fine-tuning shows an exact match with the ground truth dialogue, demonstrating that the model can understand the concept of the first thing and appropriately truncate the sentence from the context. Domain Generation Method Dialogues Dialogic Pairs Bigram Entropy Avg. words per utterance Math Persona (High Info) 142 852 6.08 48.95 Dialog Inpainting 142 1444 4.07 11.05 Business Persona (High Info) 123 738 6.61 59.01 Dialog Inpainting 123 3575 4.46 14.39 Science Persona (High Info) 228 1368 6.22 48.03 Dialog Inpainting 228 5898 4.56 13.99 Social Persona (High Info) 396 2376 6.2 51.04 Dialog Inpainting 396 7503 4.34 11.69 Total 1778 23754 5.3175 19.48875 Table 16: Detailed overview of the synthetic dataset: the Persona model generated dialogues are more verbose and diverse than dialogues generated by the dialogue inpainting model. 9728\n\n\nPage 5\nWhile non-answerable questions could also rep- resent the curiosity of a student (Scialom and Sta- iano, 2020), the answerability of questions given the context is generally important for a more useful dialog. We use the QA model1 to judge whether each question is answerable given the textbook con- tent. We refer to this metric as Answerability. This approach is akin to the method employed in (Kim et al., 2022). More details are in Table 5. 3.2.6 Factual Consistency of the Answer Factual Consistency measures whether the answer correctly responds to the student’s question. This criterion is crucial in education because it is impor- tant for students to learn accurate information (Met- zger et al., 2003). Existing metrics like Q2 (Hon- ovich et al., 2021) use a QA model to assess an- swer correctness, while RQUGE (Mohammadshahi et al., 2023) uses a QA model to evaluate the quality of the candidate question. In our scenario, we need to measure whether the answer contains correct information and accurately answers the question. Therefore, we build on the idea of Q2 and introduce a new metric referred as QFactScore: α · sim(QA(qt, S), at) + β · sim(qt, at) (1) It calculates the cosine similarity of embeddings between the predicted and original answers for each QA pair and also evaluates the similarity between the question and the original answer. This metric has been validated for\n\n\nPage 1\nwith student (Chi and Wylie, 2014). This differs from techniques focusing on teaching using scaffolding (Macina et al., 2023a) or Socratic questioning (Shridhar et al., 2022) which leads to deeper and wider human learning. How- ever, the task of generating high-quality synthetic dialogue data is difficult (Dai et al., 2022). This is amplified in education, where the generated in- teractions should cover the teaching material in an informative and coherent way (Agrawal et al., 2012; Crosby, 2000). Therefore, it is important to have quality controls on such data, because students might otherwise receive wrong feedback, which could be detrimental to learning. Thus, in this work, we also formulate some cri- teria that measure the quality of educational dia- logues. For example, it is crucial that the chat- bot does not provide students with incorrect infor- mation and stays grounded in the textbook, ensur- ing factual consistency with the knowledge taught. This is particularly important given that large lan- guage models (LLMs) are prone to ‘hallucinations’ or generating plausible but incorrect or unverified information (Ji et al., 2023). While a simple teacher strategy would be to answer with extracted pas- sages from the textbook, this might hurt the coher- ence of the dialogue which is present in interactive educational situations (Baker et al., 2021). The teacher’s response\n\n\nPage 20\nCriterion Questions for each QA pair Answer Relevance Question: Is the response directly ad- dressing the posed question? (answer no if it is answering a different question) Informativeness Question: Does the current answer in- troduce new information that was not mentioned in previous answers within the same conversation? Groundedness Question: Does the answer contain spe- cific details or data points mentioned in the contextual background or previous dialogue? Coherence Question 1: Does the current question directly follow up on the immediate pre- vious answer? (Ignore the first QA pair) Factual Consis- tency Question: Does the answer correctly ad- dress the question, considering the con- text provided? (If ’answerability’ is ’no,’ then this criterion should also be ’no.’) Answerability Question: Can the question be answered given the context? Specificity Question: Does this question exhibit generality, such that it could be relevant beyond the immediate context provided? (e.g. What is interesting about this pas- sage?) Table 12: Exact framing of questions asked during the human evaluation: annotators answer each question with yes or no for each QA pair. Participants Pairs Cohen’s Kappa P1 vs. P2 0.71 P1 vs. P3 0.71 P1 vs. P4 0.90 P2 vs. P3 0.73 P2 vs. P4 0.71 P3 vs. P4 0.71 Table 13: The Cohen’s Kappa score between each pair of participants: indicates reasonable alignment\n\n\nPage 8\nAnswer Relevance Informativeness Groundedness Coherence Answerability Factual Consistency BF1 (qt, at) QuestEval Uptake 1 - Overlap (at, a<t) Density Coverage BF1 (qt, a<t) BF1 (qt, at−1) Answerable QFactScore SimSeek 0.53 0.25 0.78 0.71 11.66 0.82 0.51 0.55 0.84 0.32 Dialogue Inpainting 0.52 0.28 0.84 0.91 22.62 0.90 0.45 0.46 0.75 0.24 Persona (Sing. Inst.) 0.58 0.35 0.98 0.86 3.94 0.75 0.49 0.52 0.92 0.54 Persona (Low Info) 0.61 0.44 0.99 0.59 2.39 0.70 0.52 0.59 0.98 0.75 Persona (Med. Info) ", "summary": "The document primarily discusses the development and evaluation of synthetic dialogue datasets generated from textbooks to improve educational chatbots. The key topics and goals include:\n\n1. **Synthetic Dialogue Generation**: The work introduces a task of creating conversational question-answering dialogues from textbooks to fine-tune educational chatbots, especially in domains lacking high-quality dialogue data.\n\n2. **Evaluation Metrics**: The generated dialogues are evaluated based on criteria such as Answer Relevance, Informativeness, Coherence, and Factual Consistency. A new metric, QFactScore, is introduced to assess factual consistency, highlighting the importance of accurate information in educational contexts.\n\n3. **Fine-Tuning and Results**: The study fine-tunes models on synthetic datasets from different subjects (math, business, science, social science) and compares their performance against baselines like Zero-shot and UltraChat. Fine-tuning on social science data showed the highest improvement in certain datasets, while business data excelled in others.\n\n4. **Evaluation Process**: Human evaluators rate QA pairs based on specific criteria, with substantial agreement indicated by Cohen’s Kappa scores. The evaluation process ensures that dialogues maintain educational quality and factual accuracy.\n\n5. **Challenges and Insights**: The document acknowledges the challenges in", "updated_at": 1763988503.2661152}, "0616a81f5d17": {"context": "Page 1\nlearner’s abilities. © 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ TOMIKAWA et al.: ADAPTIVE QUESTION–ANSWER GENERATION WITH DIFFICULTY CONTROL 2187 framework of computerized adaptive testing (CAT) [31], a wellknown test administration method. This method repeats a cycle of sequentially presenting questions of a difficulty level suited to a learner’s ability and estimating their ability from their responses. We expect this method to enable efficient ability estimation while tuning the difficulty levels of provided questions. To our knowledge, this is the first QG method that enables difficulty control aimed at generating question–answer pairs quantified with IRT-based difficulty. II. RESEARCH QUESTIONS AND CONTRIBUTIONS Fig. 1. Conventional QG task: answer-aware QG. 1) The relationship between the difficulty of the questions and the learner’s ability is ignored, making it difficult to determine the appropriate difficulty for each learner. 2) The methods are answer-aware, which means a reading passage and an answer text must be input to generate questions, as shown in Fig. 1. For this reason, they cannot generate question–answer pairs. Generating answers along with questions is essential to realize automatic assessment of learners’ responses to generated\n\n\nPage 1\nspecified levels of difficulty, while the y-axis represents the average word count of the answers. The figure shows an increasing trend in the average word count for higher levels of difficulty. Since longer answers generally make the question difficult, the proposed method could extract answers while reflecting the difficulty levels. Table III shows examples of the generated questions and answers for different difficulty levels from the same reading passage. It shows that giving a lower difficulty value induces short answer text, whereas giving a higher difficulty value induces longer and more complex answers. Finally, we investigated how the input difficulty affects the types of generated questions. To do so, we examined the proportion of leading interrogative words, “What,” “Who,” “When,” “Why,” “Where,” “Which,” and “How” in the generated questions. The results are shown in Fig. 14. The x-axis represents the specified levels of difficulty, and the y-axis represents the proportion of each interrogative word appearing as the leading word in the generated questions, categorized by their level of difficulty. The figure shows that questions beginning with “When,” “Which,” and “Who,” which are generally answerable by a single element, are more likely to be generated at lower difficulty levels. Conversely, questions starting with “Why” and “How,” which ask for reasons or\n\n\nPage 1\nitems. The IRT has been widely used in various educational and psychological tests because of its numerous benefits, such as offering accurate estimates of examinee’s ability and item characteristics, unifying measurement scales among different tests, and facilitating CAT applications [45], [46], [47], [48]. This study uses the Rasch model [49], the most traditional and well-known IRT model, to quantify question difficulty. A. Rasch Model The Rasch model defines the probability that the mth examinee correctly answers the nth item as pnm = exp(θm − bn ) 1 + exp(θm − bn ) (1) where bn denotes the difficulty of the nth item, and θm denotes the latent ability of the mth examinee. For a detailed explanation of this model, Fig. 3 illustrates item response curves (IRCs) for the Rasch model, which are plots of 1 In this context, “latent” indicates that the corresponding factors are not directly observable but lie behind the observed data. 2 “Discriminative power” means the degree to which an item can distinguish between examinees with different levels of the underlying ability. Fig. 3. 2189 IRCs for a Rasch model with different item difficulty values. the probability p. In this figure, the horizontal axis represents the ability θ; the vertical axis represents the probability p; and the three solid curves represent the IRCs for three items with different difficulty levels, b = −1.0,\n\n\nPage 1\nmodel; 2) a difficulty–controllable answer-aware QG model. B. Difficulty-Controllable Answer-Extraction Model We use BERT [24] for difficulty-controllable answer extraction. BERT is a pretrained transformer model, the details of which are summarized in Table II. BERT can be adapted for various downstream tasks by fine-tuning it with task-specific supervised datasets and incorporating task-specific output layers. We employed BERT for the answer-extraction task because of its extensive prior usage in various text-extraction applications [53]. To perform answer extraction using BERT, we add output layers that predict the start and end positions of the answer text within a given reading passage. Specifically, letting v i be the BERT output vector for the ith word in a passage r, we add two dense layers, denoted as f (s) (v i ) and f (e) (v i ), for each v i , where each dense layer transforms v i into a scalar value . The dense-layer outputs are then transformed through softmax activations as P (s) = softmax(f (s) (v 1 ), . . . , f (s) (v I )) and P (e) = (s) softmax(f (e) (v 1 ), . . . , f (e) (v I )), whose ith elements Pi and (e) Pi represent the probability values for whether the ith word is at the start and end positions, respectively. Thus, by extracting the word sequence within the start and end positions, which take the maximum probabilities, we can extract an answer text\n\n\nPage 1\nparagraph of a Wikipedia article. However, to construct a difficulty-controllable QG method, we require a dataset consisting of quadruplets (r, q, a, b). Thus, we first propose a method for extending the SQuAD dataset by appending the IRT-based difficulty values for each question–answer pair. The details for doing so are as follows. 1) Collecting response data for each question–answer pair: We collect answers from multiple respondents to each question in the SQuAD dataset and grade those answers as correct or incorrect based on exact matching with the corresponding true answers. Ideally, we should gather responses from a population of target learners, but this is highly expensive and time consuming. Thus, we substitute actual learner responses with automated QA systems, in the same way that several previous difficulty-controllable QG studies have done [12], [22]. 2) Difficulty estimation using IRT: Using the collected response data, we estimate3 the question difficulty based on the Rasch model following the item calibration procedure introduced in Section V. 3) Creating a dataset with difficulty estimates: We construct a dataset consisting of quadruplets (r, q, a, b) by appending the estimated difficulty values b into the triplets (r, q, a) of the SQuAD dataset. Using this dataset, the proposed method trains following two models: 1) a difficulty-controllable answer-extraction\n\n\nPage 1\ninsufficient results. These findings were confirmed by the practicality ratings, which also found that nearly 90% of the questions were evaluated positively. 6 The crowdworkers were recruited via the crowdsourcing platform CrowdWorks (crowdworks.jp). The only requirement was that they have a TOEIC score of 900 or above. No additional information about the raters’ attributes was collected. A TOEIC score of 900, which corresponds to an effective operational proficiency in the English language, can be compared with other international English scoring systems as follows: TOEFL > 79, International English Language Testing System (IELTS) > 6.5, Cambridge Exam ≥ First Certificate in English (FCE), and Common European Framework of Reference for Languages (CEFR) ≥ B2. 2195 Next, we calculated the Spearman rank-order correlation coefficient between the human evaluations of difficulty and the difficulty levels specified for QG, obtaining a low correlation value of 0.15. To assess the reason for this outcome, we determined the agreement ratio between the two raters for each evaluation criterion. The results showed that the agreement rates for fluency, content relevance, answerability, and practicality were 0.63, 0.85, 0.53, and 0.60, respectively, while that for difficulty was 0.22. From these results, we conclude that it is hard to ensure reliable subjective evaluations for question\n\n\nPage 1\ngraph as a guide. However, the following issues remain unaddressed in these difficulty-controllable QG methods. 1) The relation between the learner’s ability level and the difficulty of the questions is not considered, making it impossible to select a difficulty level that matches the learner’s ability. 2) While they can generate questions from a given reading passage and answer, they are unable to generate both a question and an answer solely from the reading passage. Controlling the difficulty in answer generation has not been investigated, although it is just as important as controlling the difficulty of questions since difficulty is a property that generally depends on both questions and answers. To resolve these issues, we use the IRT to quantify difficulty levels and to automatically generate both questions and answers from the reading passage, based on specified difficulty values. V. ITEM RESPONSE THEORY (IRT) The IRT [29] is a statistical framework that uses probabilistic models, called IRT models, to estimate two latent factors:1 examinee’s ability and item characteristics such as item difficulty and discriminative power,2 where the examinee and the item correspond to the learner and the question, respectively, in our study. These latent factors are estimated from the response data, which generally consist of the examinee’s binary correct/incorrect responses to the\n\n\nPage 1\nprocessing by the language models BERT and T5. 5) The created dataset, Db , was further divided into two (train) partitions that were 90% and 10%, denoted as Db and (eval) Db , respectively. 6) We first fine-tuned both the answer-extraction model and the QG model using the original SQuAD training dataset, D(train) , without considering the difficulty levels. Although this step is not mandatory, we expect that pretraining these models on a large dataset without difficulty considerations can enhance their performance in answer extraction and QG. (train) 7) Fine-tuning was then performed on Db to develop answer-extraction and QG models that consider question difficulty. This fine-tuning employed the model parameters estimated in step (6) as initial values. 8) To assess the proficiency of the developed models in controlling the difficulty, we generated question–answer pairs with various difficulties and evaluated them. Specifically, (eval) and each of we first input each reading passage in Db the 61 difficulty values, from −3.0 to 3.0 in increments of 0.1, into the proposed answer-extraction model and generated 61 answers for each reading passage. Then, given each triplet consisting of a reading passage, difficulty value, and generated answer, we generated questions using the proposed QG model. The generated sets of questions and answers were subjected to both machine-based and\n\n\nPage 1\nSchool of Informatics and Engineering, University of Electro-Communications, Chofu 182-8585, Japan (e-mail: tomikawa@ai.lab.uec.ac.jp; suzuki_ayaka@ai.lab.uec.ac.jp; uto@ai. lab.uec.ac.jp). Digital Object Identifier 10.1109/TLT.2024.3491801 in various educational applications, including intelligent tutoring systems, writing-assistance tools, and knowledge-evaluation platforms [1], [2], [3], [4], [5], [6], [7]. Early QG methods were based on rule-based or templatebased approaches, which generated questions by converting declarative texts into interrogative questions, using handcrafted rules or templates [1], [8], [9], [10], [11]. However, preparing well-designed rules and templates for specific applications is time consuming and labor-intensive [", "summary": "The document focuses on the development of an adaptive question-answer generation (QG) system with difficulty control, leveraging computerized adaptive testing (CAT) and item response theory (IRT). Key topics include:\n\n1. **Adaptive Question-Answer Generation**: The system aims to generate question-answer pairs with controlled difficulty levels, tailored to a learner's ability. This approach contrasts with traditional QG methods that do not consider the relationship between question difficulty and learner ability.\n\n2. **Item Response Theory (IRT)**: The study utilizes the Rasch model, a well-known IRT model, to quantify question difficulty. IRT helps in estimating latent factors such as examinee ability and item characteristics, which are crucial for adaptive testing.\n\n3. **BERT for Answer Extraction**: The system employs BERT, a pretrained transformer model, for extracting answers from reading passages. This involves fine-tuning BERT to predict the start and end positions of answer texts, facilitating difficulty-controlled answer generation.\n\n4. **Dataset Construction**: The study extends the SQuAD dataset by appending IRT-based difficulty values to each question-answer pair. This involves collecting response data, estimating question difficulty using IRT, and creating a dataset of quadruplets (reading passage, question,", "updated_at": 1763988511.804208}}